{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhik24/Chatbot/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YH18QK0y1DP",
        "outputId": "d127ea60-f463-4676-f94b-2ae2d6da00a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=7c8fe007ff36dbafba740aaca97923f29e4cbd0c03890945005abb4b939d75f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/fb/7b/e06204a0ceefa45443930b9a250cb5ebe31def0e4e8245a465\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()  # For Stemming\n",
        "!pip install tflearn\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_bYFcRvKeHa",
        "outputId": "c3678123-52bb-4e51-9958-ed2e611b7c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you?', 'Is anyone there?', 'Hello', 'Good day', \"What's up\", 'how are ya', 'heyy', 'whatsup', '??? ??? ??'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'see you', 'bye bye', 'See you later', 'Goodbye', 'I am Leaving', 'Bye', 'Have a Good day', 'talk to you later', 'ttyl', 'i got to go', 'gtg'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!', 'Come back soon'], 'context_set': ''}, {'tag': 'creator', 'patterns': ['what is the name of your developers', 'what is the name of your creators', 'what is the name of the developers', 'what is the name of the creators', 'who created you', 'your developers', 'your creators', 'who are your developers', 'developers', 'you are made by', 'you are made by whom', 'who created you', 'who create you', 'creators', 'who made you', 'who designed you'], 'responses': ['College Students'], 'context_set': ''}, {'tag': 'name', 'patterns': ['name', 'your name', 'do you have a name', 'what are you called', 'what is your name', 'what should I call you', 'whats your name?', 'what are you', 'who are you', 'who is this', 'what am i chatting to', 'who am i taking to', 'what are you'], 'responses': ['You can call me Luna.', \"I'm Luna\"], 'context_set': ''}, {'tag': 'hours', 'patterns': ['timing of college', 'what is college timing', 'working days', 'when are you guys open', 'what are your hours', 'hours of operation', 'when is the college open', 'college timing', 'what about college timing', 'is college open on saturday', 'tell something about college timing', 'what is the college  hours', 'when should i come to college', 'when should i attend college', 'what is my college time', 'college timing', 'timing college'], 'responses': ['College is open 8am-5pm Monday-Friday!'], 'context_set': ''}, {'tag': 'number', 'patterns': ['more info', 'contact info', 'how to contact college', 'college telephone number', 'college number', 'What is your contact no', 'Contact number?', 'how to call you', 'College phone no?', 'how can i contact you', 'Can i get your phone number', 'how can i call you', 'phone number', 'phone no', 'call'], 'responses': ['You can email at: info@iiitranchi.ac.in'], 'context_set': ''}, {'tag': 'course', 'patterns': ['list of courses', 'list of courses offered', 'list of courses offered in', 'what are the courses offered in your college?', 'courses?', 'courses offered', 'courses offered in (your univrsity(UNI) name)', 'courses you offer', 'branches?', 'courses available at UNI?', 'branches available at your college?', 'what are the courses in UNI?', 'what are branches in UNI?', 'what are courses in UNI?', 'branches available in UNI?', 'can you tell me the courses available in UNI?', 'can you tell me the branches available in UNI?', 'computer engineering?', 'computer', 'Computer engineering?', 'it', 'IT', 'Information Technology', 'AI/Ml', 'Mechanical engineering', 'Chemical engineering', 'Civil engineering'], 'responses': ['For course details visit https://iiitranchi.ac.in/courses.aspx'], 'context_set': ''}, {'tag': 'fees', 'patterns': ['information about fee', 'information on fee', 'tell me the fee', 'college fee', 'fee per semester', 'what is the fee of each semester', 'what is the fees of each year', 'what is fee', 'what is the fees', 'how much is the fees', 'fees for first year', 'fees', 'about the fees', 'tell me something about the fees', 'What is the fees of hostel', 'how much is the fees', 'hostel fees', 'fees for AC room', 'fees for non-AC room', 'fees for Ac room for girls', 'fees for non-Ac room for girls', 'fees for Ac room for boys', 'fees for non-Ac room for boys'], 'responses': ['For Fee details visit https://iiitranchi.ac.in/fees.aspx'], 'context_set': ''}, {'tag': 'location', 'patterns': ['where is the college located', 'college is located at', 'where is college', 'where is college located', 'address of college', 'how to reach college', 'college location', 'college address', 'wheres the college', 'how can I reach college', 'whats is the college address', 'what is the address of college', 'address', 'location'], 'responses': ['https://iiitranchi.ac.in/reach_us.aspx'], 'context_set': ''}, {'tag': 'hostel', 'patterns': ['hostel facility', 'hostel servive', 'hostel location', 'hostel address', 'hostel facilities', 'hostel fees', 'Does college provide hostel', 'Is there any hostel', 'Where is hostel', 'do you have hostel', 'do you guys have hostel', 'hostel', 'hostel capacity', 'what is the hostel fee', 'how to get in hostel', 'what is the hostel address', 'how far is hostel from college', 'hostel college distance', 'where is the hostel', 'how big is the hostel', 'distance between college and hostel', 'distance between hostel and college'], 'responses': ['For hostel details visit https://iiitranchi.ac.in/facilities.aspx'], 'context_set': ''}, {'tag': 'event', 'patterns': ['events organised', 'list of events', 'list of events organised in college', 'list of events conducted in college', 'What events are conducted in college', 'Are there any event held at college', 'Events?', 'functions', 'what are the events', 'tell me about events', 'what about events'], 'responses': ['For event details visit https://iiitranchi.ac.in'], 'context_set': ''}, {'tag': 'document', 'patterns': ['document to bring', 'documents needed for admision', 'documents needed at the time of admission', 'documents needed during admission', 'documents required for admision', 'documents required at the time of admission', 'documents required during admission', 'What document are required for admission', 'Which document to bring for admission', 'documents', 'what documents do i need', 'what documents do I need for admission', 'documents needed'], 'responses': ['To know more about documents required visit https://iiitranchi.ac.in'], 'context_set': ''}, {'tag': 'floors', 'patterns': ['size of campus', 'building size', 'How many floors does college have', 'floors in college', 'floors in college', \"how tall is UNI's College of Engineering college building\", 'floors'], 'responses': ['My College has total 5 floors '], 'context_set': ''}, {'tag': 'syllabus', 'patterns': ['Syllabus for IT', 'what is the Information Technology syllabus', 'syllabus', 'timetable', 'what is IT syllabus', 'syllabus', 'What is next lecture'], 'responses': ['Timetable is provided directly to the students through their respective Class Representatives.'], 'context_set': ''}, {'tag': 'library', 'patterns': ['is there any library', 'library facility', 'library facilities', 'do you have library', 'does the college have library facility', 'college library', 'where can i get books', 'book facility', 'Where is library', 'Library', 'Library information', 'Library books information', 'Tell me about library', 'how many libraries'], 'responses': ['There is one huge and spacious library. Timings are 8am to 6pm.'], 'context_set': ''}, {'tag': 'infrastructure', 'patterns': ['how is college infrastructure', 'infrastructure', 'college infrastructure'], 'responses': ['Our University has Excellent Infrastructure. Campus is clean. Good IT Labs With Good Speed of Internet connection'], 'context_set': ''}, {'tag': 'canteen', 'patterns': ['food facilities', 'canteen facilities', 'canteen facility', 'is there any canteen', 'Is there a cafetaria in college', 'Does college have canteen', 'Where is canteen', 'where is cafetaria', 'canteen', 'Food', 'Cafetaria'], 'responses': ['Our university has canteen with variety of food available'], 'context_set': ''}, {'tag': 'menu', 'patterns': ['food menu', 'food in canteen', 'Whats there on menu', 'what is available in college canteen', 'what foods can we get in college canteen', 'food variety', 'What is there to eat?'], 'responses': ['Please visit college canteen/mess to view the menu.'], 'context_set': ''}, {'tag': 'placement', 'patterns': ['What is college placement', 'Which companies visit in college', 'What is average package', 'companies visit', 'package', 'About placement', 'placement', 'recruitment', 'companies'], 'responses': ['To know about placement visit https://iiitranchi.ac.in/placement_records.aspx'], 'context_set': ''}, {'tag': 'ithod', 'patterns': ['Who is HOD', 'Where is HOD', 'it hod', 'name of it hod'], 'responses': ['All engineering departments have only one hod XYZ who available on (Place name)'], 'context_set': ''}, {'tag': 'computerhod', 'patterns': ['Who is computer HOD', 'Where is computer HOD', 'computer hod', 'name of computer hod'], 'responses': ['All engineering departments have only one hod XYZ who available on (PLACE NAME)'], 'context_set': ''}, {'tag': 'ecehod', 'patterns': ['Who is ece HOD', 'Where is  ece HOD', 'ece hod', 'name of ece hod'], 'responses': ['Different branch wise hod are different. So be more clear with your department'], 'context_set': ''}, {'tag': 'director', 'patterns': ['what is the name of director', 'what is the director name', 'director name', 'Who is college director', \"Where is director's office\", 'director', 'name of director'], 'responses': ['Prof. Vishnu Priye is the Director of IIIT Ranchi and if you need any help then call your branch hod first. That is more appropriate'], 'context_set': ''}, {'tag': 'sem', 'patterns': ['exam dates', 'exam schedule', 'When is semester exam', 'Semester exam timetable', 'sem', 'semester', 'exam', 'when is exam', 'exam timetable', 'exam dates', 'when is semester'], 'responses': ['Here is the Academic Calendar https://iiitranchi.ac.in/AcademicCalendar.aspx'], 'context_set': ''}, {'tag': 'admission', 'patterns': ['what is the process of admission', 'what is the admission process', 'How to take admission in your college', 'What is the process for admission', 'admission', 'admission process'], 'responses': ['Admissions are taken on the basis of JEE rank.'], 'context_set': ''}, {'tag': 'scholarship', 'patterns': ['scholarship', 'Is scholarship available', 'scholarship engineering', 'scholarship it', 'scholarship ce', 'scholarship mechanical', 'scholarship civil', 'scholarship chemical', 'scholarship for AI/ML', 'available scholarships', 'scholarship for computer engineering', 'scholarship for IT engineering', 'scholarship for mechanical engineering', 'scholarship for civil engineering', 'scholarship for chemical engineering', 'list of scholarship', 'comps scholarship', 'IT scholarship', 'mechanical scholarship', 'civil scholarship', 'chemical scholarship', 'automobile scholarship', 'first year scholarship', 'second year scholarship', 'third year scholarship', 'fourth year scholarship'], 'responses': ['Many government scholarships are supported by our university. For details and updates contact scholarship coordinators.'], 'context_set': ''}, {'tag': 'facilities', 'patterns': ['What facilities college provide', 'College facility', 'What are college facilities', 'facilities', 'facilities provided'], 'responses': [\"Our university's Engineering department provides fully AC Lab with internet connection, smart classroom, Auditorium, library, canteen\"], 'context_set': ''}, {'tag': 'college intake', 'patterns': ['max number of students', 'number of seats per branch', 'number of seats in each branch', 'maximum number of seats', 'maximum students intake', 'What is college intake', 'how many stundent are taken in each branch', 'seat allotment', 'seats'], 'responses': ['For IT, Computer and extc 60 per branch and seat may be differ for different department.'], 'context_set': ''}, {'tag': 'uniform', 'patterns': ['college dress code', 'college dresscode', 'what is the uniform', 'can we wear casuals', 'Does college have an uniform', 'Is there any uniform', 'uniform', 'what about uniform', 'do we have to wear uniform'], 'responses': ['There is no uniform, you can wear casuals.'], 'context_set': ''}, {'tag': 'committee', 'patterns': ['what are the different committe in college', 'different committee in college', 'Are there any committee in college', 'Give me committee details', 'committee', 'how many committee are there in college'], 'responses': ['For the various committe in college contact this number: ADD NUMBER'], 'context_set': ''}, {'tag': 'random', 'patterns': ['I love you', 'Will you marry me', 'Do you love me'], 'responses': ['I am not program for this, please ask appropriate query'], 'context_set': ''}, {'tag': 'swear', 'patterns': ['fuck', 'bitch', 'shut up', 'hell', 'stupid', 'idiot', 'dumb ass', 'asshole', 'fucker'], 'responses': ['please use appropriate language', 'Maintaining decency would be appreciated'], 'context_set': ''}, {'tag': 'vacation', 'patterns': ['holidays', 'when will semester starts', 'when will semester end', 'when is the holidays', 'list of holidays', 'Holiday in these year', 'holiday list', 'about vacations', 'about holidays', 'When is vacation', 'When is holidays', 'how long will be the vacation'], 'responses': ['Academic calender is given to you by your class-soordinators after you join your respective classes'], 'context_set': ''}, {'tag': 'sports', 'patterns': ['sports and games', 'give sports details', 'sports infrastructure', 'sports facilities', 'information about sports', 'Sports activities', 'please provide sports and games information'], 'responses': ['Our university encourages all-round development of students and hence provides sports facilities in the campus. For more details visit<a target=\"_blank\" href=/\"(LINK IF HAVE)\">here</a>'], 'context_set': ''}, {'tag': 'salutaion', 'patterns': ['okk', 'okie', 'nice work', 'well done', 'good job', 'thanks for the help', 'Thank You', 'its ok', 'Thanks', 'Good work', 'k', 'ok', 'okay'], 'responses': ['I am glad I helped you', 'welcome, anything else i can assist you with?'], 'context_set': ''}, {'tag': 'task', 'patterns': ['what can you do', 'what are the thing you can do', 'things you can do', 'what can u do for me', 'how u can help me', 'why i should use you'], 'responses': ['I can answer to low-intermediate questions regarding college', 'You can ask me questions regarding college, and i will try to answer them'], 'context_set': ''}, {'tag': 'ragging', 'patterns': ['ragging', 'is ragging practice active in college', 'does college have any antiragging facility', 'is there any ragging cases', 'is ragging done here', 'ragging against', 'antiragging facility', 'ragging juniors', 'ragging history', 'ragging incidents'], 'responses': ['We are Proud to tell you that our college provides ragging free environment, and we have strict rules against ragging'], 'context_set': ''}, {'tag': 'hod', 'patterns': ['hod', 'hod name', 'who is the hod'], 'responses': ['HODs differ for each branch, please be more specific like: (HOD it)'], 'context_set': ''}]}\n"
          ]
        }
      ],
      "source": [
        "# Loading the intents file\n",
        "\n",
        "with open(\"intents.json\") as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGuPOtbmLIjF"
      },
      "outputs": [],
      "source": [
        "# Trying to load the processed data from a 'data.pickle' file. If the file does not exist, we process the data\n",
        "# and save it in the file for future use.\n",
        "\n",
        "try:\n",
        "  with open(\"data.pickle\", \"rb\") as f:\n",
        "    words, labels, training, output = pickle.load(f)\n",
        "except:\n",
        "  # If 'data.pickle' file does not exist, we process the data from 'intents.json' file\n",
        "  words=[]\n",
        "  labels = []\n",
        "  docs_x = []\n",
        "  docs_y = []\n",
        "  for intent in data[\"intents\"]:\n",
        "    # Iterate over patterns in each intent and tokenize them\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      wrds = nltk.word_tokenize(pattern)\n",
        "      words.extend(wrds)\n",
        "      docs_x.append(wrds)\n",
        "      docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    # Add unique tag to the labels list\n",
        "    if intent[\"tag\"] not in labels:\n",
        "      labels.append(intent[\"tag\"])\n",
        "\n",
        "  # Stemming and cleaning the words\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "  words = sorted(list(set(words)))\n",
        "\n",
        "  # Sorting labels\n",
        "  labels = sorted(labels)\n",
        "\n",
        "  # Initialize training and output lists\n",
        "  training = []\n",
        "  output = []\n",
        "\n",
        "  # Create an empty output list for each label\n",
        "  out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "  # Create a bag of words for each pattern\n",
        "  for x, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "    # Stemming and cleaning the words in each pattern\n",
        "    wrds = [stemmer.stem(w) for w in doc]\n",
        "    for w in words:\n",
        "      # If the word is in the pattern, add 1 to the bag. Otherwise, add 0.\n",
        "      if w in wrds:\n",
        "        bag.append(1)\n",
        "      else:\n",
        "        bag.append(0)\n",
        "    # Create an output row for each pattern\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    # Add the bag and output row to the training and output lists\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "  # Convert training and output lists to numpy arrays\n",
        "  training = numpy.array(training)\n",
        "  output = numpy.array(output)\n",
        "\n",
        "  # Save words, labels, training, and output in a 'data.pickle' file for future use\n",
        "  with open(\"data.pickle\", \"wb\") as f:\n",
        "    pickle.dump((words, labels, training, output), f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb_P1kBXYOBa",
        "outputId": "88ebf81a-62d1-437a-f3b5-88dc7e0018b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: (405, 243)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Output: (405, 38)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"Training:\", training.shape)\n",
        "print(training[0])\n",
        "print(\"Output:\", output.shape)\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vczb4KsTLVkX",
        "outputId": "698631fb-6182-4bb8-cf0d-f329a176908f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "# Reset the default TensorFlow graph\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "# Define the input layer of the neural network with shape [batch_size, num_features]\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "\n",
        "# Add two fully connected hidden layers with 8 nodes each\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "\n",
        "# Add the output layer with a softmax activation function to get probabilities for each class\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "\n",
        "# Define the regression layer of the neural network\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "# Define a deep neural network with the specified layers\n",
        "model = tflearn.DNN(net)\n",
        "\n",
        "\n",
        "# Train the neural network on the training data\n",
        "\n",
        "#model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "#model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AZkJnZJCno-",
        "outputId": "fd131414-39ea-48c7-8249-ef666a06fec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9995 -- iter: 136/405\n",
            "Training Step: 46836  | time: 0.097s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9996 -- iter: 144/405\n",
            "Training Step: 46837  | time: 0.101s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9996 -- iter: 152/405\n",
            "Training Step: 46838  | time: 0.106s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9997 -- iter: 160/405\n",
            "Training Step: 46839  | time: 0.109s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9872 -- iter: 168/405\n",
            "Training Step: 46840  | time: 0.114s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9885 -- iter: 176/405\n",
            "Training Step: 46841  | time: 0.121s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9896 -- iter: 184/405\n",
            "Training Step: 46842  | time: 0.126s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9907 -- iter: 192/405\n",
            "Training Step: 46843  | time: 0.131s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9916 -- iter: 200/405\n",
            "Training Step: 46844  | time: 0.136s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9924 -- iter: 208/405\n",
            "Training Step: 46845  | time: 0.141s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9932 -- iter: 216/405\n",
            "Training Step: 46846  | time: 0.146s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9939 -- iter: 224/405\n",
            "Training Step: 46847  | time: 0.152s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9945 -- iter: 232/405\n",
            "Training Step: 46848  | time: 0.158s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9825 -- iter: 240/405\n",
            "Training Step: 46849  | time: 0.164s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9843 -- iter: 248/405\n",
            "Training Step: 46850  | time: 0.169s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9859 -- iter: 256/405\n",
            "Training Step: 46851  | time: 0.174s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9873 -- iter: 264/405\n",
            "Training Step: 46852  | time: 0.179s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9885 -- iter: 272/405\n",
            "Training Step: 46853  | time: 0.184s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9897 -- iter: 280/405\n",
            "Training Step: 46854  | time: 0.190s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9907 -- iter: 288/405\n",
            "Training Step: 46855  | time: 0.199s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9916 -- iter: 296/405\n",
            "Training Step: 46856  | time: 0.205s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9925 -- iter: 304/405\n",
            "Training Step: 46857  | time: 0.210s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9932 -- iter: 312/405\n",
            "Training Step: 46858  | time: 0.215s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9939 -- iter: 320/405\n",
            "Training Step: 46859  | time: 0.222s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9945 -- iter: 328/405\n",
            "Training Step: 46860  | time: 0.228s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9951 -- iter: 336/405\n",
            "Training Step: 46861  | time: 0.233s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9956 -- iter: 344/405\n",
            "Training Step: 46862  | time: 0.238s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9960 -- iter: 352/405\n",
            "Training Step: 46863  | time: 0.245s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9964 -- iter: 360/405\n",
            "Training Step: 46864  | time: 0.251s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9968 -- iter: 368/405\n",
            "Training Step: 46865  | time: 0.256s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9971 -- iter: 376/405\n",
            "Training Step: 46866  | time: 0.263s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9974 -- iter: 384/405\n",
            "Training Step: 46867  | time: 0.271s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9976 -- iter: 392/405\n",
            "Training Step: 46868  | time: 0.278s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9979 -- iter: 400/405\n",
            "Training Step: 46869  | time: 0.285s\n",
            "| Adam | epoch: 919 | loss: 0.00000 - acc: 0.9981 -- iter: 405/405\n",
            "--\n",
            "Training Step: 46870  | time: 0.007s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9983 -- iter: 008/405\n",
            "Training Step: 46871  | time: 0.012s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9985 -- iter: 016/405\n",
            "Training Step: 46872  | time: 0.019s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9986 -- iter: 024/405\n",
            "Training Step: 46873  | time: 0.026s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9987 -- iter: 032/405\n",
            "Training Step: 46874  | time: 0.032s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9989 -- iter: 040/405\n",
            "Training Step: 46875  | time: 0.039s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9990 -- iter: 048/405\n",
            "Training Step: 46876  | time: 0.044s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9991 -- iter: 056/405\n",
            "Training Step: 46877  | time: 0.048s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9992 -- iter: 064/405\n",
            "Training Step: 46878  | time: 0.054s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9993 -- iter: 072/405\n",
            "Training Step: 46879  | time: 0.061s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9993 -- iter: 080/405\n",
            "Training Step: 46880  | time: 0.068s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9994 -- iter: 088/405\n",
            "Training Step: 46881  | time: 0.077s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9995 -- iter: 096/405\n",
            "Training Step: 46882  | time: 0.081s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9870 -- iter: 104/405\n",
            "Training Step: 46883  | time: 0.086s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9883 -- iter: 112/405\n",
            "Training Step: 46884  | time: 0.094s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9895 -- iter: 120/405\n",
            "Training Step: 46885  | time: 0.101s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9905 -- iter: 128/405\n",
            "Training Step: 46886  | time: 0.105s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9915 -- iter: 136/405\n",
            "Training Step: 46887  | time: 0.110s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9923 -- iter: 144/405\n",
            "Training Step: 46888  | time: 0.116s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9931 -- iter: 152/405\n",
            "Training Step: 46889  | time: 0.121s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9938 -- iter: 160/405\n",
            "Training Step: 46890  | time: 0.127s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9944 -- iter: 168/405\n",
            "Training Step: 46891  | time: 0.132s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9950 -- iter: 176/405\n",
            "Training Step: 46892  | time: 0.138s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9955 -- iter: 184/405\n",
            "Training Step: 46893  | time: 0.142s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9959 -- iter: 192/405\n",
            "Training Step: 46894  | time: 0.147s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9963 -- iter: 200/405\n",
            "Training Step: 46895  | time: 0.156s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9967 -- iter: 208/405\n",
            "Training Step: 46896  | time: 0.162s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9970 -- iter: 216/405\n",
            "Training Step: 46897  | time: 0.167s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9973 -- iter: 224/405\n",
            "Training Step: 46898  | time: 0.172s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9976 -- iter: 232/405\n",
            "Training Step: 46899  | time: 0.178s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9978 -- iter: 240/405\n",
            "Training Step: 46900  | time: 0.182s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9981 -- iter: 248/405\n",
            "Training Step: 46901  | time: 0.189s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9982 -- iter: 256/405\n",
            "Training Step: 46902  | time: 0.194s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9984 -- iter: 264/405\n",
            "Training Step: 46903  | time: 0.199s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9986 -- iter: 272/405\n",
            "Training Step: 46904  | time: 0.205s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9987 -- iter: 280/405\n",
            "Training Step: 46905  | time: 0.211s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9988 -- iter: 288/405\n",
            "Training Step: 46906  | time: 0.216s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9990 -- iter: 296/405\n",
            "Training Step: 46907  | time: 0.222s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9991 -- iter: 304/405\n",
            "Training Step: 46908  | time: 0.229s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9992 -- iter: 312/405\n",
            "Training Step: 46909  | time: 0.235s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9992 -- iter: 320/405\n",
            "Training Step: 46910  | time: 0.241s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9993 -- iter: 328/405\n",
            "Training Step: 46911  | time: 0.247s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9994 -- iter: 336/405\n",
            "Training Step: 46912  | time: 0.253s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9994 -- iter: 344/405\n",
            "Training Step: 46913  | time: 0.258s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9995 -- iter: 352/405\n",
            "Training Step: 46914  | time: 0.264s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9996 -- iter: 360/405\n",
            "Training Step: 46915  | time: 0.269s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9996 -- iter: 368/405\n",
            "Training Step: 46916  | time: 0.273s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9996 -- iter: 376/405\n",
            "Training Step: 46917  | time: 0.280s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9997 -- iter: 384/405\n",
            "Training Step: 46918  | time: 0.288s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9997 -- iter: 392/405\n",
            "Training Step: 46919  | time: 0.293s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9997 -- iter: 400/405\n",
            "Training Step: 46920  | time: 0.301s\n",
            "| Adam | epoch: 920 | loss: 0.00000 - acc: 0.9998 -- iter: 405/405\n",
            "--\n",
            "Training Step: 46921  | time: 0.006s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9998 -- iter: 008/405\n",
            "Training Step: 46922  | time: 0.011s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9873 -- iter: 016/405\n",
            "Training Step: 46923  | time: 0.017s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9886 -- iter: 024/405\n",
            "Training Step: 46924  | time: 0.022s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9897 -- iter: 032/405\n",
            "Training Step: 46925  | time: 0.028s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9907 -- iter: 040/405\n",
            "Training Step: 46926  | time: 0.034s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9917 -- iter: 048/405\n",
            "Training Step: 46927  | time: 0.041s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9925 -- iter: 056/405\n",
            "Training Step: 46928  | time: 0.048s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9933 -- iter: 064/405\n",
            "Training Step: 46929  | time: 0.054s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9939 -- iter: 072/405\n",
            "Training Step: 46930  | time: 0.060s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9945 -- iter: 080/405\n",
            "Training Step: 46931  | time: 0.065s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9951 -- iter: 088/405\n",
            "Training Step: 46932  | time: 0.069s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9956 -- iter: 096/405\n",
            "Training Step: 46933  | time: 0.074s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9960 -- iter: 104/405\n",
            "Training Step: 46934  | time: 0.079s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9964 -- iter: 112/405\n",
            "Training Step: 46935  | time: 0.085s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9968 -- iter: 120/405\n",
            "Training Step: 46936  | time: 0.090s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9971 -- iter: 128/405\n",
            "Training Step: 46937  | time: 0.096s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9974 -- iter: 136/405\n",
            "Training Step: 46938  | time: 0.103s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9976 -- iter: 144/405\n",
            "Training Step: 46939  | time: 0.110s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9979 -- iter: 152/405\n",
            "Training Step: 46940  | time: 0.117s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9981 -- iter: 160/405\n",
            "Training Step: 46941  | time: 0.123s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9983 -- iter: 168/405\n",
            "Training Step: 46942  | time: 0.129s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9985 -- iter: 176/405\n",
            "Training Step: 46943  | time: 0.134s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9986 -- iter: 184/405\n",
            "Training Step: 46944  | time: 0.139s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9988 -- iter: 192/405\n",
            "Training Step: 46945  | time: 0.144s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9989 -- iter: 200/405\n",
            "Training Step: 46946  | time: 0.148s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9990 -- iter: 208/405\n",
            "Training Step: 46947  | time: 0.152s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9991 -- iter: 216/405\n",
            "Training Step: 46948  | time: 0.159s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9992 -- iter: 224/405\n",
            "Training Step: 46949  | time: 0.166s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9993 -- iter: 232/405\n",
            "Training Step: 46950  | time: 0.173s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9993 -- iter: 240/405\n",
            "Training Step: 46951  | time: 0.180s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9994 -- iter: 248/405\n",
            "Training Step: 46952  | time: 0.186s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9870 -- iter: 256/405\n",
            "Training Step: 46953  | time: 0.190s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9883 -- iter: 264/405\n",
            "Training Step: 46954  | time: 0.196s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9894 -- iter: 272/405\n",
            "Training Step: 46955  | time: 0.200s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9905 -- iter: 280/405\n",
            "Training Step: 46956  | time: 0.205s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9914 -- iter: 288/405\n",
            "Training Step: 46957  | time: 0.212s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9923 -- iter: 296/405\n",
            "Training Step: 46958  | time: 0.218s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9931 -- iter: 304/405\n",
            "Training Step: 46959  | time: 0.223s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9938 -- iter: 312/405\n",
            "Training Step: 46960  | time: 0.228s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9944 -- iter: 320/405\n",
            "Training Step: 46961  | time: 0.235s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9949 -- iter: 328/405\n",
            "Training Step: 46962  | time: 0.240s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9955 -- iter: 336/405\n",
            "Training Step: 46963  | time: 0.246s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9959 -- iter: 344/405\n",
            "Training Step: 46964  | time: 0.251s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9963 -- iter: 352/405\n",
            "Training Step: 46965  | time: 0.254s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9967 -- iter: 360/405\n",
            "Training Step: 46966  | time: 0.260s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9970 -- iter: 368/405\n",
            "Training Step: 46967  | time: 0.265s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9973 -- iter: 376/405\n",
            "Training Step: 46968  | time: 0.270s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9976 -- iter: 384/405\n",
            "Training Step: 46969  | time: 0.274s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9978 -- iter: 392/405\n",
            "Training Step: 46970  | time: 0.279s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9980 -- iter: 400/405\n",
            "Training Step: 46971  | time: 0.284s\n",
            "| Adam | epoch: 921 | loss: 0.00000 - acc: 0.9982 -- iter: 405/405\n",
            "--\n",
            "Training Step: 46972  | time: 0.006s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9984 -- iter: 008/405\n",
            "Training Step: 46973  | time: 0.014s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9986 -- iter: 016/405\n",
            "Training Step: 46974  | time: 0.019s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9987 -- iter: 024/405\n",
            "Training Step: 46975  | time: 0.024s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9988 -- iter: 032/405\n",
            "Training Step: 46976  | time: 0.030s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9990 -- iter: 040/405\n",
            "Training Step: 46977  | time: 0.037s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9991 -- iter: 048/405\n",
            "Training Step: 46978  | time: 0.043s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9992 -- iter: 056/405\n",
            "Training Step: 46979  | time: 0.049s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9992 -- iter: 064/405\n",
            "Training Step: 46980  | time: 0.055s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9993 -- iter: 072/405\n",
            "Training Step: 46981  | time: 0.060s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9994 -- iter: 080/405\n",
            "Training Step: 46982  | time: 0.064s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9994 -- iter: 088/405\n",
            "Training Step: 46983  | time: 0.070s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9995 -- iter: 096/405\n",
            "Training Step: 46984  | time: 0.076s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9996 -- iter: 104/405\n",
            "Training Step: 46985  | time: 0.082s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9996 -- iter: 112/405\n",
            "Training Step: 46986  | time: 0.088s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9996 -- iter: 120/405\n",
            "Training Step: 46987  | time: 0.093s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9997 -- iter: 128/405\n",
            "Training Step: 46988  | time: 0.100s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9997 -- iter: 136/405\n",
            "Training Step: 46989  | time: 0.105s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9997 -- iter: 144/405\n",
            "Training Step: 46990  | time: 0.110s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9998 -- iter: 152/405\n",
            "Training Step: 46991  | time: 0.116s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9998 -- iter: 160/405\n",
            "Training Step: 46992  | time: 0.121s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9998 -- iter: 168/405\n",
            "Training Step: 46993  | time: 0.125s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9998 -- iter: 176/405\n",
            "Training Step: 46994  | time: 0.131s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9998 -- iter: 184/405\n",
            "Training Step: 46995  | time: 0.136s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 192/405\n",
            "Training Step: 46996  | time: 0.143s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 200/405\n",
            "Training Step: 46997  | time: 0.148s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 208/405\n",
            "Training Step: 46998  | time: 0.152s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 216/405\n",
            "Training Step: 46999  | time: 0.158s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 224/405\n",
            "Training Step: 47000  | time: 0.163s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 232/405\n",
            "Training Step: 47001  | time: 0.168s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 240/405\n",
            "Training Step: 47002  | time: 0.173s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 248/405\n",
            "Training Step: 47003  | time: 0.179s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 256/405\n",
            "Training Step: 47004  | time: 0.185s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9999 -- iter: 264/405\n",
            "Training Step: 47005  | time: 0.189s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 1.0000 -- iter: 272/405\n",
            "Training Step: 47006  | time: 0.196s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9875 -- iter: 280/405\n",
            "Training Step: 47007  | time: 0.201s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9887 -- iter: 288/405\n",
            "Training Step: 47008  | time: 0.207s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9898 -- iter: 296/405\n",
            "Training Step: 47009  | time: 0.213s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9909 -- iter: 304/405\n",
            "Training Step: 47010  | time: 0.216s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9918 -- iter: 312/405\n",
            "Training Step: 47011  | time: 0.221s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9801 -- iter: 320/405\n",
            "Training Step: 47012  | time: 0.227s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9821 -- iter: 328/405\n",
            "Training Step: 47013  | time: 0.233s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9839 -- iter: 336/405\n",
            "Training Step: 47014  | time: 0.238s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9855 -- iter: 344/405\n",
            "Training Step: 47015  | time: 0.243s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9869 -- iter: 352/405\n",
            "Training Step: 47016  | time: 0.249s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9882 -- iter: 360/405\n",
            "Training Step: 47017  | time: 0.254s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9894 -- iter: 368/405\n",
            "Training Step: 47018  | time: 0.259s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9905 -- iter: 376/405\n",
            "Training Step: 47019  | time: 0.264s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9914 -- iter: 384/405\n",
            "Training Step: 47020  | time: 0.268s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9923 -- iter: 392/405\n",
            "Training Step: 47021  | time: 0.275s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9931 -- iter: 400/405\n",
            "Training Step: 47022  | time: 0.280s\n",
            "| Adam | epoch: 922 | loss: 0.00000 - acc: 0.9938 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47023  | time: 0.004s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9944 -- iter: 008/405\n",
            "Training Step: 47024  | time: 0.010s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9949 -- iter: 016/405\n",
            "Training Step: 47025  | time: 0.014s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9954 -- iter: 024/405\n",
            "Training Step: 47026  | time: 0.022s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9959 -- iter: 032/405\n",
            "Training Step: 47027  | time: 0.027s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9963 -- iter: 040/405\n",
            "Training Step: 47028  | time: 0.039s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9967 -- iter: 048/405\n",
            "Training Step: 47029  | time: 0.044s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9970 -- iter: 056/405\n",
            "Training Step: 47030  | time: 0.049s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9973 -- iter: 064/405\n",
            "Training Step: 47031  | time: 0.055s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9976 -- iter: 072/405\n",
            "Training Step: 47032  | time: 0.060s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9978 -- iter: 080/405\n",
            "Training Step: 47033  | time: 0.065s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9980 -- iter: 088/405\n",
            "Training Step: 47034  | time: 0.069s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9982 -- iter: 096/405\n",
            "Training Step: 47035  | time: 0.074s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9984 -- iter: 104/405\n",
            "Training Step: 47036  | time: 0.079s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9986 -- iter: 112/405\n",
            "Training Step: 47037  | time: 0.084s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9987 -- iter: 120/405\n",
            "Training Step: 47038  | time: 0.089s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9988 -- iter: 128/405\n",
            "Training Step: 47039  | time: 0.094s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9990 -- iter: 136/405\n",
            "Training Step: 47040  | time: 0.098s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9991 -- iter: 144/405\n",
            "Training Step: 47041  | time: 0.103s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9992 -- iter: 152/405\n",
            "Training Step: 47042  | time: 0.108s\n",
            "| Adam | epoch: 923 | loss: 0.00000 - acc: 0.9992 -- iter: 160/405\n",
            "Training Step: 47154  | time: 0.174s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 240/405\n",
            "Training Step: 47155  | time: 0.179s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 248/405\n",
            "Training Step: 47156  | time: 0.184s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 256/405\n",
            "Training Step: 47157  | time: 0.189s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 264/405\n",
            "Training Step: 47158  | time: 0.195s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 272/405\n",
            "Training Step: 47159  | time: 0.200s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 280/405\n",
            "Training Step: 47160  | time: 0.206s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 288/405\n",
            "Training Step: 47161  | time: 0.212s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 0.9999 -- iter: 296/405\n",
            "Training Step: 47162  | time: 0.217s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 304/405\n",
            "Training Step: 47163  | time: 0.222s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 312/405\n",
            "Training Step: 47164  | time: 0.226s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 320/405\n",
            "Training Step: 47165  | time: 0.231s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 328/405\n",
            "Training Step: 47166  | time: 0.237s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 336/405\n",
            "Training Step: 47167  | time: 0.242s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 344/405\n",
            "Training Step: 47168  | time: 0.248s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 352/405\n",
            "Training Step: 47169  | time: 0.253s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 360/405\n",
            "Training Step: 47170  | time: 0.259s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 368/405\n",
            "Training Step: 47171  | time: 0.264s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 376/405\n",
            "Training Step: 47172  | time: 0.270s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 384/405\n",
            "Training Step: 47173  | time: 0.277s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 392/405\n",
            "Training Step: 47174  | time: 0.281s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 400/405\n",
            "Training Step: 47175  | time: 0.285s\n",
            "| Adam | epoch: 925 | loss: 0.00000 - acc: 1.0000 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47176  | time: 0.004s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 1.0000 -- iter: 008/405\n",
            "Training Step: 47177  | time: 0.009s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 1.0000 -- iter: 016/405\n",
            "Training Step: 47178  | time: 0.014s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 1.0000 -- iter: 024/405\n",
            "Training Step: 47179  | time: 0.019s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 1.0000 -- iter: 032/405\n",
            "Training Step: 47180  | time: 0.024s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 1.0000 -- iter: 040/405\n",
            "Training Step: 47181  | time: 0.032s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9875 -- iter: 048/405\n",
            "Training Step: 47182  | time: 0.038s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9887 -- iter: 056/405\n",
            "Training Step: 47183  | time: 0.046s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9899 -- iter: 064/405\n",
            "Training Step: 47184  | time: 0.054s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9034 -- iter: 072/405\n",
            "Training Step: 47185  | time: 0.059s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9130 -- iter: 080/405\n",
            "Training Step: 47186  | time: 0.064s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9217 -- iter: 088/405\n",
            "Training Step: 47187  | time: 0.070s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9296 -- iter: 096/405\n",
            "Training Step: 47188  | time: 0.076s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9366 -- iter: 104/405\n",
            "Training Step: 47189  | time: 0.082s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9429 -- iter: 112/405\n",
            "Training Step: 47190  | time: 0.087s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9487 -- iter: 120/405\n",
            "Training Step: 47191  | time: 0.094s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9538 -- iter: 128/405\n",
            "Training Step: 47192  | time: 0.101s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9584 -- iter: 136/405\n",
            "Training Step: 47193  | time: 0.107s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9626 -- iter: 144/405\n",
            "Training Step: 47194  | time: 0.114s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9663 -- iter: 152/405\n",
            "Training Step: 47195  | time: 0.119s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9697 -- iter: 160/405\n",
            "Training Step: 47196  | time: 0.124s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9727 -- iter: 168/405\n",
            "Training Step: 47197  | time: 0.129s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9754 -- iter: 176/405\n",
            "Training Step: 47198  | time: 0.134s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9779 -- iter: 184/405\n",
            "Training Step: 47199  | time: 0.139s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9801 -- iter: 192/405\n",
            "Training Step: 47200  | time: 0.144s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9821 -- iter: 200/405\n",
            "Training Step: 47201  | time: 0.153s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9839 -- iter: 208/405\n",
            "Training Step: 47202  | time: 0.158s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9855 -- iter: 216/405\n",
            "Training Step: 47203  | time: 0.164s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9869 -- iter: 224/405\n",
            "Training Step: 47204  | time: 0.170s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9883 -- iter: 232/405\n",
            "Training Step: 47205  | time: 0.175s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9894 -- iter: 240/405\n",
            "Training Step: 47206  | time: 0.182s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9905 -- iter: 248/405\n",
            "Training Step: 47207  | time: 0.186s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9914 -- iter: 256/405\n",
            "Training Step: 47208  | time: 0.191s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9923 -- iter: 264/405\n",
            "Training Step: 47209  | time: 0.196s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9931 -- iter: 272/405\n",
            "Training Step: 47210  | time: 0.203s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9938 -- iter: 280/405\n",
            "Training Step: 47211  | time: 0.208s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9944 -- iter: 288/405\n",
            "Training Step: 47212  | time: 0.214s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9949 -- iter: 296/405\n",
            "Training Step: 47213  | time: 0.219s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9954 -- iter: 304/405\n",
            "Training Step: 47214  | time: 0.223s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9959 -- iter: 312/405\n",
            "Training Step: 47215  | time: 0.229s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9963 -- iter: 320/405\n",
            "Training Step: 47216  | time: 0.234s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9967 -- iter: 328/405\n",
            "Training Step: 47217  | time: 0.240s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9970 -- iter: 336/405\n",
            "Training Step: 47218  | time: 0.246s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9973 -- iter: 344/405\n",
            "Training Step: 47219  | time: 0.252s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9976 -- iter: 352/405\n",
            "Training Step: 47220  | time: 0.258s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9978 -- iter: 360/405\n",
            "Training Step: 47221  | time: 0.263s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9980 -- iter: 368/405\n",
            "Training Step: 47222  | time: 0.268s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9982 -- iter: 376/405\n",
            "Training Step: 47223  | time: 0.275s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9984 -- iter: 384/405\n",
            "Training Step: 47224  | time: 0.281s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9986 -- iter: 392/405\n",
            "Training Step: 47225  | time: 0.287s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9987 -- iter: 400/405\n",
            "Training Step: 47226  | time: 0.294s\n",
            "| Adam | epoch: 926 | loss: 0.00000 - acc: 0.9988 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47227  | time: 0.005s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9990 -- iter: 008/405\n",
            "Training Step: 47228  | time: 0.014s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9866 -- iter: 016/405\n",
            "Training Step: 47229  | time: 0.019s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9879 -- iter: 024/405\n",
            "Training Step: 47230  | time: 0.026s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9891 -- iter: 032/405\n",
            "Training Step: 47231  | time: 0.033s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9902 -- iter: 040/405\n",
            "Training Step: 47232  | time: 0.039s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9912 -- iter: 048/405\n",
            "Training Step: 47233  | time: 0.048s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9921 -- iter: 056/405\n",
            "Training Step: 47234  | time: 0.058s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9929 -- iter: 064/405\n",
            "Training Step: 47235  | time: 0.067s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9936 -- iter: 072/405\n",
            "Training Step: 47236  | time: 0.072s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.8942 -- iter: 080/405\n",
            "Training Step: 47237  | time: 0.077s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9048 -- iter: 088/405\n",
            "Training Step: 47238  | time: 0.083s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9143 -- iter: 096/405\n",
            "Training Step: 47239  | time: 0.092s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9229 -- iter: 104/405\n",
            "Training Step: 47240  | time: 0.099s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9306 -- iter: 112/405\n",
            "Training Step: 47241  | time: 0.103s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9375 -- iter: 120/405\n",
            "Training Step: 47242  | time: 0.108s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9438 -- iter: 128/405\n",
            "Training Step: 47243  | time: 0.114s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9494 -- iter: 136/405\n",
            "Training Step: 47244  | time: 0.120s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9545 -- iter: 144/405\n",
            "Training Step: 47245  | time: 0.125s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9590 -- iter: 152/405\n",
            "Training Step: 47246  | time: 0.130s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9631 -- iter: 160/405\n",
            "Training Step: 47247  | time: 0.134s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9668 -- iter: 168/405\n",
            "Training Step: 47248  | time: 0.139s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9701 -- iter: 176/405\n",
            "Training Step: 47249  | time: 0.144s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9731 -- iter: 184/405\n",
            "Training Step: 47250  | time: 0.150s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9758 -- iter: 192/405\n",
            "Training Step: 47251  | time: 0.156s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9782 -- iter: 200/405\n",
            "Training Step: 47252  | time: 0.165s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9804 -- iter: 208/405\n",
            "Training Step: 47253  | time: 0.170s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9824 -- iter: 216/405\n",
            "Training Step: 47254  | time: 0.175s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9841 -- iter: 224/405\n",
            "Training Step: 47255  | time: 0.183s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9857 -- iter: 232/405\n",
            "Training Step: 47256  | time: 0.188s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9871 -- iter: 240/405\n",
            "Training Step: 47257  | time: 0.193s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9759 -- iter: 248/405\n",
            "Training Step: 47258  | time: 0.201s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9658 -- iter: 256/405\n",
            "Training Step: 47259  | time: 0.209s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9692 -- iter: 264/405\n",
            "Training Step: 47260  | time: 0.217s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9723 -- iter: 272/405\n",
            "Training Step: 47261  | time: 0.222s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9751 -- iter: 280/405\n",
            "Training Step: 47262  | time: 0.227s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9776 -- iter: 288/405\n",
            "Training Step: 47263  | time: 0.234s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9798 -- iter: 296/405\n",
            "Training Step: 47264  | time: 0.238s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9818 -- iter: 304/405\n",
            "Training Step: 47265  | time: 0.245s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9837 -- iter: 312/405\n",
            "Training Step: 47266  | time: 0.251s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9853 -- iter: 320/405\n",
            "Training Step: 47267  | time: 0.260s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9868 -- iter: 328/405\n",
            "Training Step: 47268  | time: 0.265s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9881 -- iter: 336/405\n",
            "Training Step: 47269  | time: 0.271s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9893 -- iter: 344/405\n",
            "Training Step: 47270  | time: 0.278s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9904 -- iter: 352/405\n",
            "Training Step: 47271  | time: 0.284s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9913 -- iter: 360/405\n",
            "Training Step: 47272  | time: 0.291s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9922 -- iter: 368/405\n",
            "Training Step: 47273  | time: 0.298s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9930 -- iter: 376/405\n",
            "Training Step: 47274  | time: 0.304s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9937 -- iter: 384/405\n",
            "Training Step: 47275  | time: 0.309s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9943 -- iter: 392/405\n",
            "Training Step: 47276  | time: 0.316s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9949 -- iter: 400/405\n",
            "Training Step: 47277  | time: 0.323s\n",
            "| Adam | epoch: 927 | loss: 0.00000 - acc: 0.9954 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47278  | time: 0.005s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9958 -- iter: 008/405\n",
            "Training Step: 47279  | time: 0.010s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9963 -- iter: 016/405\n",
            "Training Step: 47280  | time: 0.014s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9966 -- iter: 024/405\n",
            "Training Step: 47281  | time: 0.019s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9970 -- iter: 032/405\n",
            "Training Step: 47282  | time: 0.024s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9973 -- iter: 040/405\n",
            "Training Step: 47283  | time: 0.029s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9975 -- iter: 048/405\n",
            "Training Step: 47284  | time: 0.034s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9978 -- iter: 056/405\n",
            "Training Step: 47285  | time: 0.039s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9980 -- iter: 064/405\n",
            "Training Step: 47286  | time: 0.046s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9982 -- iter: 072/405\n",
            "Training Step: 47287  | time: 0.050s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9984 -- iter: 080/405\n",
            "Training Step: 47288  | time: 0.055s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.8986 -- iter: 088/405\n",
            "Training Step: 47289  | time: 0.061s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9087 -- iter: 096/405\n",
            "Training Step: 47290  | time: 0.066s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9178 -- iter: 104/405\n",
            "Training Step: 47291  | time: 0.074s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9260 -- iter: 112/405\n",
            "Training Step: 47292  | time: 0.080s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9334 -- iter: 120/405\n",
            "Training Step: 47293  | time: 0.085s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9401 -- iter: 128/405\n",
            "Training Step: 47294  | time: 0.090s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9461 -- iter: 136/405\n",
            "Training Step: 47295  | time: 0.095s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9515 -- iter: 144/405\n",
            "Training Step: 47296  | time: 0.100s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9563 -- iter: 152/405\n",
            "Training Step: 47297  | time: 0.106s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9607 -- iter: 160/405\n",
            "Training Step: 47298  | time: 0.111s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9646 -- iter: 168/405\n",
            "Training Step: 47299  | time: 0.117s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9682 -- iter: 176/405\n",
            "Training Step: 47300  | time: 0.124s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9713 -- iter: 184/405\n",
            "Training Step: 47301  | time: 0.129s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9617 -- iter: 192/405\n",
            "Training Step: 47302  | time: 0.134s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9655 -- iter: 200/405\n",
            "Training Step: 47303  | time: 0.143s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9690 -- iter: 208/405\n",
            "Training Step: 47304  | time: 0.150s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9721 -- iter: 216/405\n",
            "Training Step: 47305  | time: 0.156s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9749 -- iter: 224/405\n",
            "Training Step: 47306  | time: 0.163s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9774 -- iter: 232/405\n",
            "Training Step: 47307  | time: 0.169s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9797 -- iter: 240/405\n",
            "Training Step: 47308  | time: 0.178s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9817 -- iter: 248/405\n",
            "Training Step: 47309  | time: 0.184s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9835 -- iter: 256/405\n",
            "Training Step: 47310  | time: 0.190s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9852 -- iter: 264/405\n",
            "Training Step: 47311  | time: 0.195s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9867 -- iter: 272/405\n",
            "Training Step: 47312  | time: 0.201s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9880 -- iter: 280/405\n",
            "Training Step: 47313  | time: 0.206s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9892 -- iter: 288/405\n",
            "Training Step: 47314  | time: 0.213s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9903 -- iter: 296/405\n",
            "Training Step: 47315  | time: 0.218s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9912 -- iter: 304/405\n",
            "Training Step: 47316  | time: 0.226s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9796 -- iter: 312/405\n",
            "Training Step: 47317  | time: 0.234s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9817 -- iter: 320/405\n",
            "Training Step: 47318  | time: 0.240s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9710 -- iter: 328/405\n",
            "Training Step: 47319  | time: 0.245s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9739 -- iter: 336/405\n",
            "Training Step: 47320  | time: 0.252s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9765 -- iter: 344/405\n",
            "Training Step: 47321  | time: 0.257s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9789 -- iter: 352/405\n",
            "Training Step: 47322  | time: 0.266s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9810 -- iter: 360/405\n",
            "Training Step: 47323  | time: 0.271s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9829 -- iter: 368/405\n",
            "Training Step: 47324  | time: 0.279s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9846 -- iter: 376/405\n",
            "Training Step: 47325  | time: 0.285s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9861 -- iter: 384/405\n",
            "Training Step: 47326  | time: 0.292s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9875 -- iter: 392/405\n",
            "Training Step: 47327  | time: 0.299s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9888 -- iter: 400/405\n",
            "Training Step: 47328  | time: 0.304s\n",
            "| Adam | epoch: 928 | loss: 0.00000 - acc: 0.9899 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47329  | time: 0.008s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9909 -- iter: 008/405\n",
            "Training Step: 47330  | time: 0.014s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9918 -- iter: 016/405\n",
            "Training Step: 47331  | time: 0.021s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9926 -- iter: 024/405\n",
            "Training Step: 47332  | time: 0.025s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9934 -- iter: 032/405\n",
            "Training Step: 47333  | time: 0.031s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9940 -- iter: 040/405\n",
            "Training Step: 47334  | time: 0.037s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9946 -- iter: 048/405\n",
            "Training Step: 47335  | time: 0.042s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9952 -- iter: 056/405\n",
            "Training Step: 47336  | time: 0.047s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9956 -- iter: 064/405\n",
            "Training Step: 47337  | time: 0.054s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9961 -- iter: 072/405\n",
            "Training Step: 47338  | time: 0.060s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9965 -- iter: 080/405\n",
            "Training Step: 47339  | time: 0.065s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9968 -- iter: 088/405\n",
            "Training Step: 47340  | time: 0.070s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.8971 -- iter: 096/405\n",
            "Training Step: 47341  | time: 0.075s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9074 -- iter: 104/405\n",
            "Training Step: 47342  | time: 0.084s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9167 -- iter: 112/405\n",
            "Training Step: 47343  | time: 0.089s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9250 -- iter: 120/405\n",
            "Training Step: 47344  | time: 0.095s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9325 -- iter: 128/405\n",
            "Training Step: 47345  | time: 0.105s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9393 -- iter: 136/405\n",
            "Training Step: 47346  | time: 0.115s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9453 -- iter: 144/405\n",
            "Training Step: 47347  | time: 0.126s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9508 -- iter: 152/405\n",
            "Training Step: 47348  | time: 0.131s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9557 -- iter: 160/405\n",
            "Training Step: 47349  | time: 0.138s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9602 -- iter: 168/405\n",
            "Training Step: 47350  | time: 0.145s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9641 -- iter: 176/405\n",
            "Training Step: 47351  | time: 0.151s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9677 -- iter: 184/405\n",
            "Training Step: 47352  | time: 0.159s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9710 -- iter: 192/405\n",
            "Training Step: 47353  | time: 0.165s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9739 -- iter: 200/405\n",
            "Training Step: 47354  | time: 0.171s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9765 -- iter: 208/405\n",
            "Training Step: 47355  | time: 0.176s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9788 -- iter: 216/405\n",
            "Training Step: 47356  | time: 0.184s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9809 -- iter: 224/405\n",
            "Training Step: 47357  | time: 0.191s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9828 -- iter: 232/405\n",
            "Training Step: 47358  | time: 0.199s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9846 -- iter: 240/405\n",
            "Training Step: 47359  | time: 0.204s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9861 -- iter: 248/405\n",
            "Training Step: 47360  | time: 0.209s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9875 -- iter: 256/405\n",
            "Training Step: 47361  | time: 0.214s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9887 -- iter: 264/405\n",
            "Training Step: 47362  | time: 0.219s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9899 -- iter: 272/405\n",
            "Training Step: 47363  | time: 0.224s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9784 -- iter: 280/405\n",
            "Training Step: 47364  | time: 0.229s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9805 -- iter: 288/405\n",
            "Training Step: 47365  | time: 0.236s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9825 -- iter: 296/405\n",
            "Training Step: 47366  | time: 0.241s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9842 -- iter: 304/405\n",
            "Training Step: 47367  | time: 0.246s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9858 -- iter: 312/405\n",
            "Training Step: 47368  | time: 0.252s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9872 -- iter: 320/405\n",
            "Training Step: 47369  | time: 0.257s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9885 -- iter: 328/405\n",
            "Training Step: 47370  | time: 0.263s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9897 -- iter: 336/405\n",
            "Training Step: 47371  | time: 0.267s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9907 -- iter: 344/405\n",
            "Training Step: 47372  | time: 0.270s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9916 -- iter: 352/405\n",
            "Training Step: 47373  | time: 0.274s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9925 -- iter: 360/405\n",
            "Training Step: 47374  | time: 0.279s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9932 -- iter: 368/405\n",
            "Training Step: 47375  | time: 0.286s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9939 -- iter: 376/405\n",
            "Training Step: 47376  | time: 0.291s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9945 -- iter: 384/405\n",
            "Training Step: 47377  | time: 0.296s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9951 -- iter: 392/405\n",
            "Training Step: 47378  | time: 0.301s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9955 -- iter: 400/405\n",
            "Training Step: 47379  | time: 0.306s\n",
            "| Adam | epoch: 929 | loss: 0.00000 - acc: 0.9960 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47380  | time: 0.008s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9964 -- iter: 008/405\n",
            "Training Step: 47381  | time: 0.013s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9968 -- iter: 016/405\n",
            "Training Step: 47382  | time: 0.016s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9971 -- iter: 024/405\n",
            "Training Step: 47383  | time: 0.021s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9974 -- iter: 032/405\n",
            "Training Step: 47384  | time: 0.026s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9976 -- iter: 040/405\n",
            "Training Step: 47385  | time: 0.030s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9979 -- iter: 048/405\n",
            "Training Step: 47386  | time: 0.035s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9981 -- iter: 056/405\n",
            "Training Step: 47387  | time: 0.043s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9983 -- iter: 064/405\n",
            "Training Step: 47388  | time: 0.046s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9984 -- iter: 072/405\n",
            "Training Step: 47389  | time: 0.051s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9986 -- iter: 080/405\n",
            "Training Step: 47390  | time: 0.055s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9987 -- iter: 088/405\n",
            "Training Step: 47391  | time: 0.060s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9989 -- iter: 096/405\n",
            "Training Step: 47392  | time: 0.064s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9990 -- iter: 104/405\n",
            "Training Step: 47393  | time: 0.071s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9991 -- iter: 112/405\n",
            "Training Step: 47394  | time: 0.076s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9992 -- iter: 120/405\n",
            "Training Step: 47395  | time: 0.081s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9993 -- iter: 128/405\n",
            "Training Step: 47396  | time: 0.087s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9993 -- iter: 136/405\n",
            "Training Step: 47397  | time: 0.091s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9994 -- iter: 144/405\n",
            "Training Step: 47398  | time: 0.096s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9995 -- iter: 152/405\n",
            "Training Step: 47399  | time: 0.101s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9995 -- iter: 160/405\n",
            "Training Step: 47400  | time: 0.107s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9996 -- iter: 168/405\n",
            "Training Step: 47401  | time: 0.112s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9996 -- iter: 176/405\n",
            "Training Step: 47402  | time: 0.116s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9996 -- iter: 184/405\n",
            "Training Step: 47403  | time: 0.126s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9997 -- iter: 192/405\n",
            "Training Step: 47404  | time: 0.135s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9997 -- iter: 200/405\n",
            "Training Step: 47405  | time: 0.140s\n",
            "| Adam | epoch: 930 | loss: 0.00000 - acc: 0.9997 -- iter: 208/405\n",
            "Training Step: 47848  | time: 0.063s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9898 -- iter: 080/405\n",
            "Training Step: 47849  | time: 0.070s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9908 -- iter: 088/405\n",
            "Training Step: 47850  | time: 0.076s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9917 -- iter: 096/405\n",
            "Training Step: 47851  | time: 0.083s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9926 -- iter: 104/405\n",
            "Training Step: 47852  | time: 0.088s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9933 -- iter: 112/405\n",
            "Training Step: 47853  | time: 0.097s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9940 -- iter: 120/405\n",
            "Training Step: 47854  | time: 0.103s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9946 -- iter: 128/405\n",
            "Training Step: 47855  | time: 0.108s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9826 -- iter: 136/405\n",
            "Training Step: 47856  | time: 0.116s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9844 -- iter: 144/405\n",
            "Training Step: 47857  | time: 0.123s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9859 -- iter: 152/405\n",
            "Training Step: 47858  | time: 0.131s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9873 -- iter: 160/405\n",
            "Training Step: 47859  | time: 0.137s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9886 -- iter: 168/405\n",
            "Training Step: 47860  | time: 0.140s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9897 -- iter: 176/405\n",
            "Training Step: 47861  | time: 0.145s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9908 -- iter: 184/405\n",
            "Training Step: 47862  | time: 0.151s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9917 -- iter: 192/405\n",
            "Training Step: 47863  | time: 0.157s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9925 -- iter: 200/405\n",
            "Training Step: 47864  | time: 0.162s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9933 -- iter: 208/405\n",
            "Training Step: 47865  | time: 0.167s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9939 -- iter: 216/405\n",
            "Training Step: 47866  | time: 0.172s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9945 -- iter: 224/405\n",
            "Training Step: 47867  | time: 0.177s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9951 -- iter: 232/405\n",
            "Training Step: 47868  | time: 0.186s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9956 -- iter: 240/405\n",
            "Training Step: 47869  | time: 0.193s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9960 -- iter: 248/405\n",
            "Training Step: 47870  | time: 0.201s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9964 -- iter: 256/405\n",
            "Training Step: 47871  | time: 0.206s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9843 -- iter: 264/405\n",
            "Training Step: 47872  | time: 0.213s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9859 -- iter: 272/405\n",
            "Training Step: 47873  | time: 0.219s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9873 -- iter: 280/405\n",
            "Training Step: 47874  | time: 0.225s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9885 -- iter: 288/405\n",
            "Training Step: 47875  | time: 0.230s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9897 -- iter: 296/405\n",
            "Training Step: 47876  | time: 0.234s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9782 -- iter: 304/405\n",
            "Training Step: 47877  | time: 0.240s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9804 -- iter: 312/405\n",
            "Training Step: 47878  | time: 0.244s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9824 -- iter: 320/405\n",
            "Training Step: 47879  | time: 0.249s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9841 -- iter: 328/405\n",
            "Training Step: 47880  | time: 0.253s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9857 -- iter: 336/405\n",
            "Training Step: 47881  | time: 0.259s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9871 -- iter: 344/405\n",
            "Training Step: 47882  | time: 0.264s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9884 -- iter: 352/405\n",
            "Training Step: 47883  | time: 0.268s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9896 -- iter: 360/405\n",
            "Training Step: 47884  | time: 0.274s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9906 -- iter: 368/405\n",
            "Training Step: 47885  | time: 0.279s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9916 -- iter: 376/405\n",
            "Training Step: 47886  | time: 0.284s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9924 -- iter: 384/405\n",
            "Training Step: 47887  | time: 0.289s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9932 -- iter: 392/405\n",
            "Training Step: 47888  | time: 0.297s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9938 -- iter: 400/405\n",
            "Training Step: 47889  | time: 0.302s\n",
            "| Adam | epoch: 939 | loss: 0.00000 - acc: 0.9945 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47890  | time: 0.006s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9950 -- iter: 008/405\n",
            "Training Step: 47891  | time: 0.010s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9955 -- iter: 016/405\n",
            "Training Step: 47892  | time: 0.015s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9960 -- iter: 024/405\n",
            "Training Step: 47893  | time: 0.020s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9964 -- iter: 032/405\n",
            "Training Step: 47894  | time: 0.024s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9967 -- iter: 040/405\n",
            "Training Step: 47895  | time: 0.028s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9971 -- iter: 048/405\n",
            "Training Step: 47896  | time: 0.032s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9974 -- iter: 056/405\n",
            "Training Step: 47897  | time: 0.038s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9976 -- iter: 064/405\n",
            "Training Step: 47898  | time: 0.043s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9979 -- iter: 072/405\n",
            "Training Step: 47899  | time: 0.048s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9981 -- iter: 080/405\n",
            "Training Step: 47900  | time: 0.053s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9983 -- iter: 088/405\n",
            "Training Step: 47901  | time: 0.059s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9984 -- iter: 096/405\n",
            "Training Step: 47902  | time: 0.065s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9986 -- iter: 104/405\n",
            "Training Step: 47903  | time: 0.070s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9987 -- iter: 112/405\n",
            "Training Step: 47904  | time: 0.074s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9989 -- iter: 120/405\n",
            "Training Step: 47905  | time: 0.079s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9990 -- iter: 128/405\n",
            "Training Step: 47906  | time: 0.084s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9991 -- iter: 136/405\n",
            "Training Step: 47907  | time: 0.088s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9992 -- iter: 144/405\n",
            "Training Step: 47908  | time: 0.094s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9993 -- iter: 152/405\n",
            "Training Step: 47909  | time: 0.098s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9993 -- iter: 160/405\n",
            "Training Step: 47910  | time: 0.104s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9994 -- iter: 168/405\n",
            "Training Step: 47911  | time: 0.110s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9995 -- iter: 176/405\n",
            "Training Step: 47912  | time: 0.116s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9995 -- iter: 184/405\n",
            "Training Step: 47913  | time: 0.124s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9996 -- iter: 192/405\n",
            "Training Step: 47914  | time: 0.129s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9996 -- iter: 200/405\n",
            "Training Step: 47915  | time: 0.136s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9996 -- iter: 208/405\n",
            "Training Step: 47916  | time: 0.141s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9997 -- iter: 216/405\n",
            "Training Step: 47917  | time: 0.148s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9997 -- iter: 224/405\n",
            "Training Step: 47918  | time: 0.156s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9997 -- iter: 232/405\n",
            "Training Step: 47919  | time: 0.162s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9998 -- iter: 240/405\n",
            "Training Step: 47920  | time: 0.169s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9998 -- iter: 248/405\n",
            "Training Step: 47921  | time: 0.173s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9998 -- iter: 256/405\n",
            "Training Step: 47922  | time: 0.178s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9998 -- iter: 264/405\n",
            "Training Step: 47923  | time: 0.183s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9998 -- iter: 272/405\n",
            "Training Step: 47924  | time: 0.190s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 280/405\n",
            "Training Step: 47925  | time: 0.197s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 288/405\n",
            "Training Step: 47926  | time: 0.202s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 296/405\n",
            "Training Step: 47927  | time: 0.207s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 304/405\n",
            "Training Step: 47928  | time: 0.212s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 312/405\n",
            "Training Step: 47929  | time: 0.220s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 320/405\n",
            "Training Step: 47930  | time: 0.227s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 328/405\n",
            "Training Step: 47931  | time: 0.234s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 336/405\n",
            "Training Step: 47932  | time: 0.239s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 344/405\n",
            "Training Step: 47933  | time: 0.245s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 0.9999 -- iter: 352/405\n",
            "Training Step: 47934  | time: 0.251s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 360/405\n",
            "Training Step: 47935  | time: 0.259s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 368/405\n",
            "Training Step: 47936  | time: 0.264s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 376/405\n",
            "Training Step: 47937  | time: 0.269s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 384/405\n",
            "Training Step: 47938  | time: 0.274s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 392/405\n",
            "Training Step: 47939  | time: 0.278s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 400/405\n",
            "Training Step: 47940  | time: 0.286s\n",
            "| Adam | epoch: 940 | loss: 0.00000 - acc: 1.0000 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47941  | time: 0.006s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 1.0000 -- iter: 008/405\n",
            "Training Step: 47942  | time: 0.013s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 1.0000 -- iter: 016/405\n",
            "Training Step: 47943  | time: 0.019s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 1.0000 -- iter: 024/405\n",
            "Training Step: 47944  | time: 0.025s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9800 -- iter: 032/405\n",
            "Training Step: 47945  | time: 0.030s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9620 -- iter: 040/405\n",
            "Training Step: 47946  | time: 0.035s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9658 -- iter: 048/405\n",
            "Training Step: 47947  | time: 0.043s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9692 -- iter: 056/405\n",
            "Training Step: 47948  | time: 0.050s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9723 -- iter: 064/405\n",
            "Training Step: 47949  | time: 0.056s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9751 -- iter: 072/405\n",
            "Training Step: 47950  | time: 0.062s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9776 -- iter: 080/405\n",
            "Training Step: 47951  | time: 0.068s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9798 -- iter: 088/405\n",
            "Training Step: 47952  | time: 0.075s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9818 -- iter: 096/405\n",
            "Training Step: 47953  | time: 0.081s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9836 -- iter: 104/405\n",
            "Training Step: 47954  | time: 0.090s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9853 -- iter: 112/405\n",
            "Training Step: 47955  | time: 0.095s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9867 -- iter: 120/405\n",
            "Training Step: 47956  | time: 0.101s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9881 -- iter: 128/405\n",
            "Training Step: 47957  | time: 0.108s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9893 -- iter: 136/405\n",
            "Training Step: 47958  | time: 0.115s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9903 -- iter: 144/405\n",
            "Training Step: 47959  | time: 0.121s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9913 -- iter: 152/405\n",
            "Training Step: 47960  | time: 0.124s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9922 -- iter: 160/405\n",
            "Training Step: 47961  | time: 0.131s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9930 -- iter: 168/405\n",
            "Training Step: 47962  | time: 0.137s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9937 -- iter: 176/405\n",
            "Training Step: 47963  | time: 0.141s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9943 -- iter: 184/405\n",
            "Training Step: 47964  | time: 0.146s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9949 -- iter: 192/405\n",
            "Training Step: 47965  | time: 0.151s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9954 -- iter: 200/405\n",
            "Training Step: 47966  | time: 0.157s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9958 -- iter: 208/405\n",
            "Training Step: 47967  | time: 0.162s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9963 -- iter: 216/405\n",
            "Training Step: 47968  | time: 0.167s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9966 -- iter: 224/405\n",
            "Training Step: 47969  | time: 0.173s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9970 -- iter: 232/405\n",
            "Training Step: 47970  | time: 0.178s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9973 -- iter: 240/405\n",
            "Training Step: 47971  | time: 0.183s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9975 -- iter: 248/405\n",
            "Training Step: 47972  | time: 0.190s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9978 -- iter: 256/405\n",
            "Training Step: 47973  | time: 0.195s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9980 -- iter: 264/405\n",
            "Training Step: 47974  | time: 0.199s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9982 -- iter: 272/405\n",
            "Training Step: 47975  | time: 0.204s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9984 -- iter: 280/405\n",
            "Training Step: 47976  | time: 0.210s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9985 -- iter: 288/405\n",
            "Training Step: 47977  | time: 0.215s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9987 -- iter: 296/405\n",
            "Training Step: 47978  | time: 0.220s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9988 -- iter: 304/405\n",
            "Training Step: 47979  | time: 0.227s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9989 -- iter: 312/405\n",
            "Training Step: 47980  | time: 0.233s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9990 -- iter: 320/405\n",
            "Training Step: 47981  | time: 0.241s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9991 -- iter: 328/405\n",
            "Training Step: 47982  | time: 0.248s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9992 -- iter: 336/405\n",
            "Training Step: 47983  | time: 0.257s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9993 -- iter: 344/405\n",
            "Training Step: 47984  | time: 0.263s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9994 -- iter: 352/405\n",
            "Training Step: 47985  | time: 0.268s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9994 -- iter: 360/405\n",
            "Training Step: 47986  | time: 0.272s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9995 -- iter: 368/405\n",
            "Training Step: 47987  | time: 0.277s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9995 -- iter: 376/405\n",
            "Training Step: 47988  | time: 0.282s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9996 -- iter: 384/405\n",
            "Training Step: 47989  | time: 0.287s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9996 -- iter: 392/405\n",
            "Training Step: 47990  | time: 0.293s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9997 -- iter: 400/405\n",
            "Training Step: 47991  | time: 0.298s\n",
            "| Adam | epoch: 941 | loss: 0.00000 - acc: 0.9997 -- iter: 405/405\n",
            "--\n",
            "Training Step: 47992  | time: 0.004s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9997 -- iter: 008/405\n",
            "Training Step: 47993  | time: 0.010s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9998 -- iter: 016/405\n",
            "Training Step: 47994  | time: 0.017s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9998 -- iter: 024/405\n",
            "Training Step: 47995  | time: 0.022s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9998 -- iter: 032/405\n",
            "Training Step: 47996  | time: 0.028s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9998 -- iter: 040/405\n",
            "Training Step: 47997  | time: 0.033s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9998 -- iter: 048/405\n",
            "Training Step: 47998  | time: 0.037s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9999 -- iter: 056/405\n",
            "Training Step: 47999  | time: 0.042s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9999 -- iter: 064/405\n",
            "Training Step: 48000  | time: 0.048s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9999 -- iter: 072/405\n",
            "Training Step: 48001  | time: 0.054s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9999 -- iter: 080/405\n",
            "Training Step: 48002  | time: 0.060s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9999 -- iter: 088/405\n",
            "Training Step: 48003  | time: 0.066s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9999 -- iter: 096/405\n",
            "Training Step: 48004  | time: 0.072s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9874 -- iter: 104/405\n",
            "Training Step: 48005  | time: 0.079s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9887 -- iter: 112/405\n",
            "Training Step: 48006  | time: 0.084s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9898 -- iter: 120/405\n",
            "Training Step: 48007  | time: 0.090s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9908 -- iter: 128/405\n",
            "Training Step: 48008  | time: 0.096s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9917 -- iter: 136/405\n",
            "Training Step: 48009  | time: 0.101s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9926 -- iter: 144/405\n",
            "Training Step: 48010  | time: 0.108s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9933 -- iter: 152/405\n",
            "Training Step: 48011  | time: 0.113s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9940 -- iter: 160/405\n",
            "Training Step: 48012  | time: 0.119s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9946 -- iter: 168/405\n",
            "Training Step: 48013  | time: 0.125s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9951 -- iter: 176/405\n",
            "Training Step: 48014  | time: 0.130s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9956 -- iter: 184/405\n",
            "Training Step: 48015  | time: 0.136s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9961 -- iter: 192/405\n",
            "Training Step: 48016  | time: 0.141s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9964 -- iter: 200/405\n",
            "Training Step: 48017  | time: 0.146s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9968 -- iter: 208/405\n",
            "Training Step: 48018  | time: 0.151s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9846 -- iter: 216/405\n",
            "Training Step: 48019  | time: 0.157s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9862 -- iter: 224/405\n",
            "Training Step: 48020  | time: 0.161s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9875 -- iter: 232/405\n",
            "Training Step: 48021  | time: 0.165s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9888 -- iter: 240/405\n",
            "Training Step: 48022  | time: 0.172s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9899 -- iter: 248/405\n",
            "Training Step: 48023  | time: 0.178s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9909 -- iter: 256/405\n",
            "Training Step: 48024  | time: 0.183s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9918 -- iter: 264/405\n",
            "Training Step: 48025  | time: 0.188s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9926 -- iter: 272/405\n",
            "Training Step: 48026  | time: 0.193s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9934 -- iter: 280/405\n",
            "Training Step: 48027  | time: 0.197s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9940 -- iter: 288/405\n",
            "Training Step: 48028  | time: 0.203s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9946 -- iter: 296/405\n",
            "Training Step: 48029  | time: 0.209s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9952 -- iter: 304/405\n",
            "Training Step: 48030  | time: 0.214s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9957 -- iter: 312/405\n",
            "Training Step: 48031  | time: 0.218s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9961 -- iter: 320/405\n",
            "Training Step: 48032  | time: 0.223s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9965 -- iter: 328/405\n",
            "Training Step: 48033  | time: 0.228s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9968 -- iter: 336/405\n",
            "Training Step: 48034  | time: 0.232s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9972 -- iter: 344/405\n",
            "Training Step: 48035  | time: 0.237s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9974 -- iter: 352/405\n",
            "Training Step: 48036  | time: 0.246s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9977 -- iter: 360/405\n",
            "Training Step: 48037  | time: 0.251s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9979 -- iter: 368/405\n",
            "Training Step: 48038  | time: 0.256s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9981 -- iter: 376/405\n",
            "Training Step: 48039  | time: 0.262s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9983 -- iter: 384/405\n",
            "Training Step: 48040  | time: 0.266s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9985 -- iter: 392/405\n",
            "Training Step: 48041  | time: 0.271s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9986 -- iter: 400/405\n",
            "Training Step: 48042  | time: 0.277s\n",
            "| Adam | epoch: 942 | loss: 0.00000 - acc: 0.9863 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48043  | time: 0.004s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9876 -- iter: 008/405\n",
            "Training Step: 48044  | time: 0.010s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9889 -- iter: 016/405\n",
            "Training Step: 48045  | time: 0.015s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9900 -- iter: 024/405\n",
            "Training Step: 48046  | time: 0.020s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9910 -- iter: 032/405\n",
            "Training Step: 48047  | time: 0.025s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9919 -- iter: 040/405\n",
            "Training Step: 48048  | time: 0.029s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9927 -- iter: 048/405\n",
            "Training Step: 48049  | time: 0.034s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9934 -- iter: 056/405\n",
            "Training Step: 48050  | time: 0.039s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9941 -- iter: 064/405\n",
            "Training Step: 48051  | time: 0.044s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9947 -- iter: 072/405\n",
            "Training Step: 48052  | time: 0.048s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9952 -- iter: 080/405\n",
            "Training Step: 48053  | time: 0.054s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9957 -- iter: 088/405\n",
            "Training Step: 48054  | time: 0.059s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9961 -- iter: 096/405\n",
            "Training Step: 48055  | time: 0.063s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9965 -- iter: 104/405\n",
            "Training Step: 48056  | time: 0.069s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9969 -- iter: 112/405\n",
            "Training Step: 48057  | time: 0.074s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9972 -- iter: 120/405\n",
            "Training Step: 48058  | time: 0.078s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9975 -- iter: 128/405\n",
            "Training Step: 48059  | time: 0.083s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9977 -- iter: 136/405\n",
            "Training Step: 48060  | time: 0.088s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9979 -- iter: 144/405\n",
            "Training Step: 48061  | time: 0.093s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9981 -- iter: 152/405\n",
            "Training Step: 48062  | time: 0.098s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9983 -- iter: 160/405\n",
            "Training Step: 48063  | time: 0.103s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9985 -- iter: 168/405\n",
            "Training Step: 48064  | time: 0.109s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9986 -- iter: 176/405\n",
            "Training Step: 48065  | time: 0.116s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9988 -- iter: 184/405\n",
            "Training Step: 48066  | time: 0.120s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9989 -- iter: 192/405\n",
            "Training Step: 48067  | time: 0.125s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9990 -- iter: 200/405\n",
            "Training Step: 48068  | time: 0.130s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9991 -- iter: 208/405\n",
            "Training Step: 48069  | time: 0.135s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9867 -- iter: 216/405\n",
            "Training Step: 48070  | time: 0.143s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9880 -- iter: 224/405\n",
            "Training Step: 48071  | time: 0.148s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9892 -- iter: 232/405\n",
            "Training Step: 48072  | time: 0.154s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9903 -- iter: 240/405\n",
            "Training Step: 48073  | time: 0.159s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9913 -- iter: 248/405\n",
            "Training Step: 48074  | time: 0.164s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9921 -- iter: 256/405\n",
            "Training Step: 48075  | time: 0.169s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9929 -- iter: 264/405\n",
            "Training Step: 48076  | time: 0.173s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9936 -- iter: 272/405\n",
            "Training Step: 48077  | time: 0.178s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9943 -- iter: 280/405\n",
            "Training Step: 48078  | time: 0.183s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9948 -- iter: 288/405\n",
            "Training Step: 48079  | time: 0.188s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9954 -- iter: 296/405\n",
            "Training Step: 48080  | time: 0.195s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9958 -- iter: 304/405\n",
            "Training Step: 48081  | time: 0.200s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9837 -- iter: 312/405\n",
            "Training Step: 48082  | time: 0.205s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9854 -- iter: 320/405\n",
            "Training Step: 48083  | time: 0.211s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9868 -- iter: 328/405\n",
            "Training Step: 48084  | time: 0.216s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9881 -- iter: 336/405\n",
            "Training Step: 48085  | time: 0.220s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9893 -- iter: 344/405\n",
            "Training Step: 48086  | time: 0.225s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9904 -- iter: 352/405\n",
            "Training Step: 48087  | time: 0.230s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9914 -- iter: 360/405\n",
            "Training Step: 48088  | time: 0.235s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9922 -- iter: 368/405\n",
            "Training Step: 48089  | time: 0.241s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9930 -- iter: 376/405\n",
            "Training Step: 48090  | time: 0.247s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9937 -- iter: 384/405\n",
            "Training Step: 48091  | time: 0.252s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9943 -- iter: 392/405\n",
            "Training Step: 48092  | time: 0.257s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9949 -- iter: 400/405\n",
            "Training Step: 48093  | time: 0.261s\n",
            "| Adam | epoch: 943 | loss: 0.00000 - acc: 0.9954 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48094  | time: 0.005s\n",
            "| Adam | epoch: 944 | loss: 0.00000 - acc: 0.9959 -- iter: 008/405\n",
            "Training Step: 48095  | time: 0.010s\n",
            "| Adam | epoch: 944 | loss: 0.00000 - acc: 0.9963 -- iter: 016/405\n",
            "Training Step: 48096  | time: 0.015s\n",
            "| Adam | epoch: 944 | loss: 0.00000 - acc: 0.9967 -- iter: 024/405\n",
            "Training Step: 48097  | time: 0.022s\n",
            "| Adam | epoch: 944 | loss: 0.00000 - acc: 0.9970 -- iter: 032/405\n",
            "Training Step: 48098  | time: 0.027s\n",
            "| Adam | epoch: 944 | loss: 0.00000 - acc: 0.9973 -- iter: 040/405\n",
            "Training Step: 48099  | time: 0.033s\n",
            "| Adam | epoch: 944 | loss: 0.00000 - acc: 0.9976 -- iter: 048/405\n",
            "Training Step: 48209  | time: 0.078s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9999 -- iter: 112/405\n",
            "Training Step: 48210  | time: 0.082s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9999 -- iter: 120/405\n",
            "Training Step: 48211  | time: 0.087s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9999 -- iter: 128/405\n",
            "Training Step: 48212  | time: 0.094s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9999 -- iter: 136/405\n",
            "Training Step: 48213  | time: 0.099s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9999 -- iter: 144/405\n",
            "Training Step: 48214  | time: 0.105s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 1.0000 -- iter: 152/405\n",
            "Training Step: 48215  | time: 0.110s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 1.0000 -- iter: 160/405\n",
            "Training Step: 48216  | time: 0.116s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 1.0000 -- iter: 168/405\n",
            "Training Step: 48217  | time: 0.120s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9875 -- iter: 176/405\n",
            "Training Step: 48218  | time: 0.125s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9887 -- iter: 184/405\n",
            "Training Step: 48219  | time: 0.131s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9898 -- iter: 192/405\n",
            "Training Step: 48220  | time: 0.135s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9784 -- iter: 200/405\n",
            "Training Step: 48221  | time: 0.142s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9805 -- iter: 208/405\n",
            "Training Step: 48222  | time: 0.149s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9825 -- iter: 216/405\n",
            "Training Step: 48223  | time: 0.158s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9842 -- iter: 224/405\n",
            "Training Step: 48224  | time: 0.162s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9108 -- iter: 232/405\n",
            "Training Step: 48225  | time: 0.166s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9197 -- iter: 240/405\n",
            "Training Step: 48226  | time: 0.171s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9278 -- iter: 248/405\n",
            "Training Step: 48227  | time: 0.175s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9350 -- iter: 256/405\n",
            "Training Step: 48228  | time: 0.182s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9415 -- iter: 264/405\n",
            "Training Step: 48229  | time: 0.186s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9473 -- iter: 272/405\n",
            "Training Step: 48230  | time: 0.191s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9526 -- iter: 280/405\n",
            "Training Step: 48231  | time: 0.197s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9573 -- iter: 288/405\n",
            "Training Step: 48232  | time: 0.201s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9616 -- iter: 296/405\n",
            "Training Step: 48233  | time: 0.207s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9654 -- iter: 304/405\n",
            "Training Step: 48234  | time: 0.211s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9689 -- iter: 312/405\n",
            "Training Step: 48235  | time: 0.214s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9720 -- iter: 320/405\n",
            "Training Step: 48236  | time: 0.219s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9748 -- iter: 328/405\n",
            "Training Step: 48237  | time: 0.226s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9648 -- iter: 336/405\n",
            "Training Step: 48238  | time: 0.231s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9683 -- iter: 344/405\n",
            "Training Step: 48239  | time: 0.237s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9715 -- iter: 352/405\n",
            "Training Step: 48240  | time: 0.241s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9744 -- iter: 360/405\n",
            "Training Step: 48241  | time: 0.246s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9769 -- iter: 368/405\n",
            "Training Step: 48242  | time: 0.251s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9792 -- iter: 376/405\n",
            "Training Step: 48243  | time: 0.255s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9813 -- iter: 384/405\n",
            "Training Step: 48244  | time: 0.260s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9832 -- iter: 392/405\n",
            "Training Step: 48245  | time: 0.266s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9849 -- iter: 400/405\n",
            "Training Step: 48246  | time: 0.272s\n",
            "| Adam | epoch: 946 | loss: 0.00000 - acc: 0.9864 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48247  | time: 0.006s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9877 -- iter: 008/405\n",
            "Training Step: 48248  | time: 0.013s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9890 -- iter: 016/405\n",
            "Training Step: 48249  | time: 0.019s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9901 -- iter: 024/405\n",
            "Training Step: 48250  | time: 0.023s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9911 -- iter: 032/405\n",
            "Training Step: 48251  | time: 0.028s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9920 -- iter: 040/405\n",
            "Training Step: 48252  | time: 0.033s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9928 -- iter: 048/405\n",
            "Training Step: 48253  | time: 0.038s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9935 -- iter: 056/405\n",
            "Training Step: 48254  | time: 0.043s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9941 -- iter: 064/405\n",
            "Training Step: 48255  | time: 0.047s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9947 -- iter: 072/405\n",
            "Training Step: 48256  | time: 0.053s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9952 -- iter: 080/405\n",
            "Training Step: 48257  | time: 0.059s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9957 -- iter: 088/405\n",
            "Training Step: 48258  | time: 0.064s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9962 -- iter: 096/405\n",
            "Training Step: 48259  | time: 0.070s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9965 -- iter: 104/405\n",
            "Training Step: 48260  | time: 0.076s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9969 -- iter: 112/405\n",
            "Training Step: 48261  | time: 0.082s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9972 -- iter: 120/405\n",
            "Training Step: 48262  | time: 0.087s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9975 -- iter: 128/405\n",
            "Training Step: 48263  | time: 0.096s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9977 -- iter: 136/405\n",
            "Training Step: 48264  | time: 0.102s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9980 -- iter: 144/405\n",
            "Training Step: 48265  | time: 0.108s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9857 -- iter: 152/405\n",
            "Training Step: 48266  | time: 0.112s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9871 -- iter: 160/405\n",
            "Training Step: 48267  | time: 0.117s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9884 -- iter: 168/405\n",
            "Training Step: 48268  | time: 0.122s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9895 -- iter: 176/405\n",
            "Training Step: 48269  | time: 0.126s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9906 -- iter: 184/405\n",
            "Training Step: 48270  | time: 0.132s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9915 -- iter: 192/405\n",
            "Training Step: 48271  | time: 0.138s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9924 -- iter: 200/405\n",
            "Training Step: 48272  | time: 0.143s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9931 -- iter: 208/405\n",
            "Training Step: 48273  | time: 0.149s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9938 -- iter: 216/405\n",
            "Training Step: 48274  | time: 0.157s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9944 -- iter: 224/405\n",
            "Training Step: 48275  | time: 0.164s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9950 -- iter: 232/405\n",
            "Training Step: 48276  | time: 0.170s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9955 -- iter: 240/405\n",
            "Training Step: 48277  | time: 0.175s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9959 -- iter: 248/405\n",
            "Training Step: 48278  | time: 0.182s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9964 -- iter: 256/405\n",
            "Training Step: 48279  | time: 0.187s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9967 -- iter: 264/405\n",
            "Training Step: 48280  | time: 0.195s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9970 -- iter: 272/405\n",
            "Training Step: 48281  | time: 0.200s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9973 -- iter: 280/405\n",
            "Training Step: 48282  | time: 0.207s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9976 -- iter: 288/405\n",
            "Training Step: 48283  | time: 0.214s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9978 -- iter: 296/405\n",
            "Training Step: 48284  | time: 0.219s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9981 -- iter: 304/405\n",
            "Training Step: 48285  | time: 0.226s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9983 -- iter: 312/405\n",
            "Training Step: 48286  | time: 0.232s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9984 -- iter: 320/405\n",
            "Training Step: 48287  | time: 0.237s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9986 -- iter: 328/405\n",
            "Training Step: 48288  | time: 0.243s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9987 -- iter: 336/405\n",
            "Training Step: 48289  | time: 0.248s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9989 -- iter: 344/405\n",
            "Training Step: 48290  | time: 0.255s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9990 -- iter: 352/405\n",
            "Training Step: 48291  | time: 0.260s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9991 -- iter: 360/405\n",
            "Training Step: 48292  | time: 0.264s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9992 -- iter: 368/405\n",
            "Training Step: 48293  | time: 0.271s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9992 -- iter: 376/405\n",
            "Training Step: 48294  | time: 0.274s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9993 -- iter: 384/405\n",
            "Training Step: 48295  | time: 0.281s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9994 -- iter: 392/405\n",
            "Training Step: 48296  | time: 0.286s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9995 -- iter: 400/405\n",
            "Training Step: 48297  | time: 0.292s\n",
            "| Adam | epoch: 947 | loss: 0.00000 - acc: 0.9995 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48298  | time: 0.004s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9996 -- iter: 008/405\n",
            "Training Step: 48299  | time: 0.009s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9996 -- iter: 016/405\n",
            "Training Step: 48300  | time: 0.016s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9996 -- iter: 024/405\n",
            "Training Step: 48301  | time: 0.020s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9997 -- iter: 032/405\n",
            "Training Step: 48302  | time: 0.026s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9872 -- iter: 040/405\n",
            "Training Step: 48303  | time: 0.030s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9885 -- iter: 048/405\n",
            "Training Step: 48304  | time: 0.034s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9896 -- iter: 056/405\n",
            "Training Step: 48305  | time: 0.039s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9907 -- iter: 064/405\n",
            "Training Step: 48306  | time: 0.044s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9916 -- iter: 072/405\n",
            "Training Step: 48307  | time: 0.048s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9924 -- iter: 080/405\n",
            "Training Step: 48308  | time: 0.053s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9932 -- iter: 088/405\n",
            "Training Step: 48309  | time: 0.058s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9939 -- iter: 096/405\n",
            "Training Step: 48310  | time: 0.063s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9945 -- iter: 104/405\n",
            "Training Step: 48311  | time: 0.068s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9950 -- iter: 112/405\n",
            "Training Step: 48312  | time: 0.075s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9955 -- iter: 120/405\n",
            "Training Step: 48313  | time: 0.080s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9960 -- iter: 128/405\n",
            "Training Step: 48314  | time: 0.084s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9964 -- iter: 136/405\n",
            "Training Step: 48315  | time: 0.089s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9967 -- iter: 144/405\n",
            "Training Step: 48316  | time: 0.095s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9971 -- iter: 152/405\n",
            "Training Step: 48317  | time: 0.102s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9849 -- iter: 160/405\n",
            "Training Step: 48318  | time: 0.107s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9864 -- iter: 168/405\n",
            "Training Step: 48319  | time: 0.112s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9877 -- iter: 176/405\n",
            "Training Step: 48320  | time: 0.118s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9890 -- iter: 184/405\n",
            "Training Step: 48321  | time: 0.122s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9901 -- iter: 192/405\n",
            "Training Step: 48322  | time: 0.129s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9911 -- iter: 200/405\n",
            "Training Step: 48323  | time: 0.133s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9920 -- iter: 208/405\n",
            "Training Step: 48324  | time: 0.137s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9928 -- iter: 216/405\n",
            "Training Step: 48325  | time: 0.141s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9935 -- iter: 224/405\n",
            "Training Step: 48326  | time: 0.145s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9941 -- iter: 232/405\n",
            "Training Step: 48327  | time: 0.148s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9947 -- iter: 240/405\n",
            "Training Step: 48328  | time: 0.154s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9953 -- iter: 248/405\n",
            "Training Step: 48329  | time: 0.158s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9957 -- iter: 256/405\n",
            "Training Step: 48330  | time: 0.163s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9962 -- iter: 264/405\n",
            "Training Step: 48331  | time: 0.169s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9965 -- iter: 272/405\n",
            "Training Step: 48332  | time: 0.174s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9969 -- iter: 280/405\n",
            "Training Step: 48333  | time: 0.177s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9972 -- iter: 288/405\n",
            "Training Step: 48334  | time: 0.182s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9975 -- iter: 296/405\n",
            "Training Step: 48335  | time: 0.187s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9977 -- iter: 304/405\n",
            "Training Step: 48336  | time: 0.192s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9980 -- iter: 312/405\n",
            "Training Step: 48337  | time: 0.197s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9982 -- iter: 320/405\n",
            "Training Step: 48338  | time: 0.202s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9983 -- iter: 328/405\n",
            "Training Step: 48339  | time: 0.207s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9985 -- iter: 336/405\n",
            "Training Step: 48340  | time: 0.211s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9987 -- iter: 344/405\n",
            "Training Step: 48341  | time: 0.215s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9988 -- iter: 352/405\n",
            "Training Step: 48342  | time: 0.221s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9989 -- iter: 360/405\n",
            "Training Step: 48343  | time: 0.227s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9990 -- iter: 368/405\n",
            "Training Step: 48344  | time: 0.232s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9991 -- iter: 376/405\n",
            "Training Step: 48345  | time: 0.237s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9992 -- iter: 384/405\n",
            "Training Step: 48346  | time: 0.242s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9993 -- iter: 392/405\n",
            "Training Step: 48347  | time: 0.247s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9994 -- iter: 400/405\n",
            "Training Step: 48348  | time: 0.252s\n",
            "| Adam | epoch: 948 | loss: 0.00000 - acc: 0.9994 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48349  | time: 0.004s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9995 -- iter: 008/405\n",
            "Training Step: 48350  | time: 0.009s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9995 -- iter: 016/405\n",
            "Training Step: 48351  | time: 0.013s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9996 -- iter: 024/405\n",
            "Training Step: 48352  | time: 0.017s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9996 -- iter: 032/405\n",
            "Training Step: 48353  | time: 0.021s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9997 -- iter: 040/405\n",
            "Training Step: 48354  | time: 0.026s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9997 -- iter: 048/405\n",
            "Training Step: 48355  | time: 0.031s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9872 -- iter: 056/405\n",
            "Training Step: 48356  | time: 0.036s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9885 -- iter: 064/405\n",
            "Training Step: 48357  | time: 0.041s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9897 -- iter: 072/405\n",
            "Training Step: 48358  | time: 0.046s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9907 -- iter: 080/405\n",
            "Training Step: 48359  | time: 0.051s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9916 -- iter: 088/405\n",
            "Training Step: 48360  | time: 0.056s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9925 -- iter: 096/405\n",
            "Training Step: 48361  | time: 0.059s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9932 -- iter: 104/405\n",
            "Training Step: 48362  | time: 0.065s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9939 -- iter: 112/405\n",
            "Training Step: 48363  | time: 0.071s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9945 -- iter: 120/405\n",
            "Training Step: 48364  | time: 0.076s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9951 -- iter: 128/405\n",
            "Training Step: 48365  | time: 0.081s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9955 -- iter: 136/405\n",
            "Training Step: 48366  | time: 0.086s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9960 -- iter: 144/405\n",
            "Training Step: 48367  | time: 0.091s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9964 -- iter: 152/405\n",
            "Training Step: 48368  | time: 0.094s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9968 -- iter: 160/405\n",
            "Training Step: 48369  | time: 0.098s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9971 -- iter: 168/405\n",
            "Training Step: 48370  | time: 0.103s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9974 -- iter: 176/405\n",
            "Training Step: 48371  | time: 0.107s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9976 -- iter: 184/405\n",
            "Training Step: 48372  | time: 0.113s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9979 -- iter: 192/405\n",
            "Training Step: 48373  | time: 0.118s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9981 -- iter: 200/405\n",
            "Training Step: 48374  | time: 0.124s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9983 -- iter: 208/405\n",
            "Training Step: 48375  | time: 0.129s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9984 -- iter: 216/405\n",
            "Training Step: 48376  | time: 0.133s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9986 -- iter: 224/405\n",
            "Training Step: 48377  | time: 0.139s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9987 -- iter: 232/405\n",
            "Training Step: 48378  | time: 0.143s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9989 -- iter: 240/405\n",
            "Training Step: 48379  | time: 0.149s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9990 -- iter: 248/405\n",
            "Training Step: 48380  | time: 0.153s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9991 -- iter: 256/405\n",
            "Training Step: 48381  | time: 0.158s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9992 -- iter: 264/405\n",
            "Training Step: 48382  | time: 0.162s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9993 -- iter: 272/405\n",
            "Training Step: 48383  | time: 0.167s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9993 -- iter: 280/405\n",
            "Training Step: 48384  | time: 0.172s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9994 -- iter: 288/405\n",
            "Training Step: 48385  | time: 0.176s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9995 -- iter: 296/405\n",
            "Training Step: 48386  | time: 0.181s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9995 -- iter: 304/405\n",
            "Training Step: 48387  | time: 0.187s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9996 -- iter: 312/405\n",
            "Training Step: 48388  | time: 0.192s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9996 -- iter: 320/405\n",
            "Training Step: 48389  | time: 0.199s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9996 -- iter: 328/405\n",
            "Training Step: 48390  | time: 0.204s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9997 -- iter: 336/405\n",
            "Training Step: 48391  | time: 0.209s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9997 -- iter: 344/405\n",
            "Training Step: 48392  | time: 0.216s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9997 -- iter: 352/405\n",
            "Training Step: 48393  | time: 0.220s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9998 -- iter: 360/405\n",
            "Training Step: 48394  | time: 0.227s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9998 -- iter: 368/405\n",
            "Training Step: 48395  | time: 0.233s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9998 -- iter: 376/405\n",
            "Training Step: 48396  | time: 0.237s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9998 -- iter: 384/405\n",
            "Training Step: 48397  | time: 0.243s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9998 -- iter: 392/405\n",
            "Training Step: 48398  | time: 0.247s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9999 -- iter: 400/405\n",
            "Training Step: 48399  | time: 0.251s\n",
            "| Adam | epoch: 949 | loss: 0.00000 - acc: 0.9999 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48400  | time: 0.006s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 008/405\n",
            "Training Step: 48401  | time: 0.009s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 016/405\n",
            "Training Step: 48402  | time: 0.013s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 024/405\n",
            "Training Step: 48403  | time: 0.016s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 032/405\n",
            "Training Step: 48404  | time: 0.022s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 040/405\n",
            "Training Step: 48405  | time: 0.027s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 048/405\n",
            "Training Step: 48406  | time: 0.031s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 056/405\n",
            "Training Step: 48407  | time: 0.035s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9999 -- iter: 064/405\n",
            "Training Step: 48408  | time: 0.039s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 072/405\n",
            "Training Step: 48409  | time: 0.046s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 080/405\n",
            "Training Step: 48410  | time: 0.050s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 088/405\n",
            "Training Step: 48411  | time: 0.056s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 096/405\n",
            "Training Step: 48412  | time: 0.063s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 104/405\n",
            "Training Step: 48413  | time: 0.067s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 112/405\n",
            "Training Step: 48414  | time: 0.071s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 120/405\n",
            "Training Step: 48415  | time: 0.075s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 128/405\n",
            "Training Step: 48416  | time: 0.081s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 136/405\n",
            "Training Step: 48417  | time: 0.086s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 144/405\n",
            "Training Step: 48418  | time: 0.093s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 152/405\n",
            "Training Step: 48419  | time: 0.098s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 160/405\n",
            "Training Step: 48420  | time: 0.104s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 168/405\n",
            "Training Step: 48421  | time: 0.109s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 176/405\n",
            "Training Step: 48422  | time: 0.114s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 184/405\n",
            "Training Step: 48423  | time: 0.118s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 192/405\n",
            "Training Step: 48424  | time: 0.124s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 200/405\n",
            "Training Step: 48425  | time: 0.129s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 208/405\n",
            "Training Step: 48426  | time: 0.133s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 216/405\n",
            "Training Step: 48427  | time: 0.139s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 224/405\n",
            "Training Step: 48428  | time: 0.144s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 232/405\n",
            "Training Step: 48429  | time: 0.150s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 240/405\n",
            "Training Step: 48430  | time: 0.154s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 1.0000 -- iter: 248/405\n",
            "Training Step: 48431  | time: 0.159s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9875 -- iter: 256/405\n",
            "Training Step: 48432  | time: 0.165s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9887 -- iter: 264/405\n",
            "Training Step: 48433  | time: 0.170s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9899 -- iter: 272/405\n",
            "Training Step: 48434  | time: 0.174s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9909 -- iter: 280/405\n",
            "Training Step: 48435  | time: 0.180s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9918 -- iter: 288/405\n",
            "Training Step: 48436  | time: 0.184s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9926 -- iter: 296/405\n",
            "Training Step: 48437  | time: 0.190s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9934 -- iter: 304/405\n",
            "Training Step: 48438  | time: 0.196s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9940 -- iter: 312/405\n",
            "Training Step: 48439  | time: 0.200s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9946 -- iter: 320/405\n",
            "Training Step: 48440  | time: 0.204s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9952 -- iter: 328/405\n",
            "Training Step: 48441  | time: 0.210s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9956 -- iter: 336/405\n",
            "Training Step: 48442  | time: 0.216s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9961 -- iter: 344/405\n",
            "Training Step: 48443  | time: 0.222s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9965 -- iter: 352/405\n",
            "Training Step: 48444  | time: 0.225s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9968 -- iter: 360/405\n",
            "Training Step: 48445  | time: 0.231s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9971 -- iter: 368/405\n",
            "Training Step: 48446  | time: 0.235s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9974 -- iter: 376/405\n",
            "Training Step: 48447  | time: 0.240s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9977 -- iter: 384/405\n",
            "Training Step: 48448  | time: 0.245s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9979 -- iter: 392/405\n",
            "Training Step: 48449  | time: 0.249s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9981 -- iter: 400/405\n",
            "Training Step: 48450  | time: 0.253s\n",
            "| Adam | epoch: 950 | loss: 0.00000 - acc: 0.9983 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48451  | time: 0.005s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9985 -- iter: 008/405\n",
            "Training Step: 48452  | time: 0.009s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9986 -- iter: 016/405\n",
            "Training Step: 48453  | time: 0.014s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9988 -- iter: 024/405\n",
            "Training Step: 48454  | time: 0.017s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9989 -- iter: 032/405\n",
            "Training Step: 48455  | time: 0.022s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9990 -- iter: 040/405\n",
            "Training Step: 48456  | time: 0.026s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9991 -- iter: 048/405\n",
            "Training Step: 48457  | time: 0.031s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9992 -- iter: 056/405\n",
            "Training Step: 48458  | time: 0.035s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9993 -- iter: 064/405\n",
            "Training Step: 48459  | time: 0.039s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9993 -- iter: 072/405\n",
            "Training Step: 48460  | time: 0.043s\n",
            "| Adam | epoch: 951 | loss: 0.00000 - acc: 0.9994 -- iter: 080/405\n",
            "Training Step: 48526  | time: 0.114s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9939 -- iter: 200/405\n",
            "Training Step: 48527  | time: 0.118s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9945 -- iter: 208/405\n",
            "Training Step: 48528  | time: 0.123s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9951 -- iter: 216/405\n",
            "Training Step: 48529  | time: 0.128s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9956 -- iter: 224/405\n",
            "Training Step: 48530  | time: 0.133s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9960 -- iter: 232/405\n",
            "Training Step: 48531  | time: 0.138s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9964 -- iter: 240/405\n",
            "Training Step: 48532  | time: 0.142s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9968 -- iter: 248/405\n",
            "Training Step: 48533  | time: 0.147s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9971 -- iter: 256/405\n",
            "Training Step: 48534  | time: 0.152s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9974 -- iter: 264/405\n",
            "Training Step: 48535  | time: 0.156s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9976 -- iter: 272/405\n",
            "Training Step: 48536  | time: 0.160s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9979 -- iter: 280/405\n",
            "Training Step: 48537  | time: 0.165s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9981 -- iter: 288/405\n",
            "Training Step: 48538  | time: 0.169s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9983 -- iter: 296/405\n",
            "Training Step: 48539  | time: 0.174s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9985 -- iter: 304/405\n",
            "Training Step: 48540  | time: 0.180s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9986 -- iter: 312/405\n",
            "Training Step: 48541  | time: 0.186s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9987 -- iter: 320/405\n",
            "Training Step: 48542  | time: 0.190s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9989 -- iter: 328/405\n",
            "Training Step: 48543  | time: 0.194s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9990 -- iter: 336/405\n",
            "Training Step: 48544  | time: 0.199s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9991 -- iter: 344/405\n",
            "Training Step: 48545  | time: 0.204s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9992 -- iter: 352/405\n",
            "Training Step: 48546  | time: 0.208s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9993 -- iter: 360/405\n",
            "Training Step: 48547  | time: 0.213s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9993 -- iter: 368/405\n",
            "Training Step: 48548  | time: 0.217s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9994 -- iter: 376/405\n",
            "Training Step: 48549  | time: 0.223s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9995 -- iter: 384/405\n",
            "Training Step: 48550  | time: 0.226s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9995 -- iter: 392/405\n",
            "Training Step: 48551  | time: 0.231s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9996 -- iter: 400/405\n",
            "Training Step: 48552  | time: 0.237s\n",
            "| Adam | epoch: 952 | loss: 0.00000 - acc: 0.9996 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48553  | time: 0.005s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9996 -- iter: 008/405\n",
            "Training Step: 48554  | time: 0.010s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9997 -- iter: 016/405\n",
            "Training Step: 48555  | time: 0.014s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9997 -- iter: 024/405\n",
            "Training Step: 48556  | time: 0.018s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9997 -- iter: 032/405\n",
            "Training Step: 48557  | time: 0.023s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9998 -- iter: 040/405\n",
            "Training Step: 48558  | time: 0.030s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9998 -- iter: 048/405\n",
            "Training Step: 48559  | time: 0.034s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9998 -- iter: 056/405\n",
            "Training Step: 48560  | time: 0.038s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9998 -- iter: 064/405\n",
            "Training Step: 48561  | time: 0.044s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9998 -- iter: 072/405\n",
            "Training Step: 48562  | time: 0.049s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 080/405\n",
            "Training Step: 48563  | time: 0.053s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 088/405\n",
            "Training Step: 48564  | time: 0.059s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 096/405\n",
            "Training Step: 48565  | time: 0.063s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 104/405\n",
            "Training Step: 48566  | time: 0.068s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 112/405\n",
            "Training Step: 48567  | time: 0.072s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 120/405\n",
            "Training Step: 48568  | time: 0.077s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 128/405\n",
            "Training Step: 48569  | time: 0.082s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 136/405\n",
            "Training Step: 48570  | time: 0.086s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 144/405\n",
            "Training Step: 48571  | time: 0.092s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9999 -- iter: 152/405\n",
            "Training Step: 48572  | time: 0.098s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 160/405\n",
            "Training Step: 48573  | time: 0.103s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 168/405\n",
            "Training Step: 48574  | time: 0.109s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 176/405\n",
            "Training Step: 48575  | time: 0.115s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 184/405\n",
            "Training Step: 48576  | time: 0.120s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 192/405\n",
            "Training Step: 48577  | time: 0.125s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 200/405\n",
            "Training Step: 48578  | time: 0.131s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 208/405\n",
            "Training Step: 48579  | time: 0.136s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 216/405\n",
            "Training Step: 48580  | time: 0.142s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 224/405\n",
            "Training Step: 48581  | time: 0.146s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 1.0000 -- iter: 232/405\n",
            "Training Step: 48582  | time: 0.152s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9875 -- iter: 240/405\n",
            "Training Step: 48583  | time: 0.159s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9887 -- iter: 248/405\n",
            "Training Step: 48584  | time: 0.164s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9899 -- iter: 256/405\n",
            "Training Step: 48585  | time: 0.169s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9909 -- iter: 264/405\n",
            "Training Step: 48586  | time: 0.174s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9918 -- iter: 272/405\n",
            "Training Step: 48587  | time: 0.178s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9926 -- iter: 280/405\n",
            "Training Step: 48588  | time: 0.184s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9933 -- iter: 288/405\n",
            "Training Step: 48589  | time: 0.187s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9940 -- iter: 296/405\n",
            "Training Step: 48590  | time: 0.193s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9946 -- iter: 304/405\n",
            "Training Step: 48591  | time: 0.200s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9952 -- iter: 312/405\n",
            "Training Step: 48592  | time: 0.205s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9956 -- iter: 320/405\n",
            "Training Step: 48593  | time: 0.211s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9961 -- iter: 328/405\n",
            "Training Step: 48594  | time: 0.216s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9965 -- iter: 336/405\n",
            "Training Step: 48595  | time: 0.220s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9968 -- iter: 344/405\n",
            "Training Step: 48596  | time: 0.226s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9971 -- iter: 352/405\n",
            "Training Step: 48597  | time: 0.231s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9974 -- iter: 360/405\n",
            "Training Step: 48598  | time: 0.237s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9977 -- iter: 368/405\n",
            "Training Step: 48599  | time: 0.241s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9979 -- iter: 376/405\n",
            "Training Step: 48600  | time: 0.247s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9981 -- iter: 384/405\n",
            "Training Step: 48601  | time: 0.252s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9983 -- iter: 392/405\n",
            "Training Step: 48602  | time: 0.258s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9985 -- iter: 400/405\n",
            "Training Step: 48603  | time: 0.264s\n",
            "| Adam | epoch: 953 | loss: 0.00000 - acc: 0.9986 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48604  | time: 0.006s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9988 -- iter: 008/405\n",
            "Training Step: 48605  | time: 0.012s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9989 -- iter: 016/405\n",
            "Training Step: 48606  | time: 0.019s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9990 -- iter: 024/405\n",
            "Training Step: 48607  | time: 0.023s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9991 -- iter: 032/405\n",
            "Training Step: 48608  | time: 0.029s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9992 -- iter: 040/405\n",
            "Training Step: 48609  | time: 0.035s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9993 -- iter: 048/405\n",
            "Training Step: 48610  | time: 0.040s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9993 -- iter: 056/405\n",
            "Training Step: 48611  | time: 0.045s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9994 -- iter: 064/405\n",
            "Training Step: 48612  | time: 0.051s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9995 -- iter: 072/405\n",
            "Training Step: 48613  | time: 0.058s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9995 -- iter: 080/405\n",
            "Training Step: 48614  | time: 0.064s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9996 -- iter: 088/405\n",
            "Training Step: 48615  | time: 0.072s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9996 -- iter: 096/405\n",
            "Training Step: 48616  | time: 0.080s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9997 -- iter: 104/405\n",
            "Training Step: 48617  | time: 0.084s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9997 -- iter: 112/405\n",
            "Training Step: 48618  | time: 0.089s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9997 -- iter: 120/405\n",
            "Training Step: 48619  | time: 0.093s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9872 -- iter: 128/405\n",
            "Training Step: 48620  | time: 0.099s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9885 -- iter: 136/405\n",
            "Training Step: 48621  | time: 0.103s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9897 -- iter: 144/405\n",
            "Training Step: 48622  | time: 0.107s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9907 -- iter: 152/405\n",
            "Training Step: 48623  | time: 0.112s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9916 -- iter: 160/405\n",
            "Training Step: 48624  | time: 0.118s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9925 -- iter: 168/405\n",
            "Training Step: 48625  | time: 0.122s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9932 -- iter: 176/405\n",
            "Training Step: 48626  | time: 0.128s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9939 -- iter: 184/405\n",
            "Training Step: 48627  | time: 0.131s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9945 -- iter: 192/405\n",
            "Training Step: 48628  | time: 0.136s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9951 -- iter: 200/405\n",
            "Training Step: 48629  | time: 0.141s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9956 -- iter: 208/405\n",
            "Training Step: 48630  | time: 0.146s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9960 -- iter: 216/405\n",
            "Training Step: 48631  | time: 0.152s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9964 -- iter: 224/405\n",
            "Training Step: 48632  | time: 0.157s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9968 -- iter: 232/405\n",
            "Training Step: 48633  | time: 0.163s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9971 -- iter: 240/405\n",
            "Training Step: 48634  | time: 0.166s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9974 -- iter: 248/405\n",
            "Training Step: 48635  | time: 0.172s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9976 -- iter: 256/405\n",
            "Training Step: 48636  | time: 0.176s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9979 -- iter: 264/405\n",
            "Training Step: 48637  | time: 0.183s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9981 -- iter: 272/405\n",
            "Training Step: 48638  | time: 0.190s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9983 -- iter: 280/405\n",
            "Training Step: 48639  | time: 0.195s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9984 -- iter: 288/405\n",
            "Training Step: 48640  | time: 0.200s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9986 -- iter: 296/405\n",
            "Training Step: 48641  | time: 0.206s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9987 -- iter: 304/405\n",
            "Training Step: 48642  | time: 0.217s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9989 -- iter: 312/405\n",
            "Training Step: 48643  | time: 0.225s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9865 -- iter: 320/405\n",
            "Training Step: 48644  | time: 0.231s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9878 -- iter: 328/405\n",
            "Training Step: 48645  | time: 0.240s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9891 -- iter: 336/405\n",
            "Training Step: 48646  | time: 0.247s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9901 -- iter: 344/405\n",
            "Training Step: 48647  | time: 0.253s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9911 -- iter: 352/405\n",
            "Training Step: 48648  | time: 0.257s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9920 -- iter: 360/405\n",
            "Training Step: 48649  | time: 0.267s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9928 -- iter: 368/405\n",
            "Training Step: 48650  | time: 0.273s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9935 -- iter: 376/405\n",
            "Training Step: 48651  | time: 0.282s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9942 -- iter: 384/405\n",
            "Training Step: 48652  | time: 0.286s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9948 -- iter: 392/405\n",
            "Training Step: 48653  | time: 0.289s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9953 -- iter: 400/405\n",
            "Training Step: 48654  | time: 0.295s\n",
            "| Adam | epoch: 954 | loss: 0.00000 - acc: 0.9958 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48655  | time: 0.006s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9962 -- iter: 008/405\n",
            "Training Step: 48656  | time: 0.011s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9966 -- iter: 016/405\n",
            "Training Step: 48657  | time: 0.019s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9969 -- iter: 024/405\n",
            "Training Step: 48658  | time: 0.024s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9972 -- iter: 032/405\n",
            "Training Step: 48659  | time: 0.031s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9975 -- iter: 040/405\n",
            "Training Step: 48660  | time: 0.034s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9977 -- iter: 048/405\n",
            "Training Step: 48661  | time: 0.043s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9980 -- iter: 056/405\n",
            "Training Step: 48662  | time: 0.048s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9982 -- iter: 064/405\n",
            "Training Step: 48663  | time: 0.053s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9984 -- iter: 072/405\n",
            "Training Step: 48664  | time: 0.059s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9985 -- iter: 080/405\n",
            "Training Step: 48665  | time: 0.066s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9987 -- iter: 088/405\n",
            "Training Step: 48666  | time: 0.073s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9988 -- iter: 096/405\n",
            "Training Step: 48667  | time: 0.078s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9989 -- iter: 104/405\n",
            "Training Step: 48668  | time: 0.086s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9990 -- iter: 112/405\n",
            "Training Step: 48669  | time: 0.100s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9991 -- iter: 120/405\n",
            "Training Step: 48670  | time: 0.106s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9992 -- iter: 128/405\n",
            "Training Step: 48671  | time: 0.111s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9993 -- iter: 136/405\n",
            "Training Step: 48672  | time: 0.116s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9994 -- iter: 144/405\n",
            "Training Step: 48673  | time: 0.120s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9994 -- iter: 152/405\n",
            "Training Step: 48674  | time: 0.125s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9995 -- iter: 160/405\n",
            "Training Step: 48675  | time: 0.130s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9995 -- iter: 168/405\n",
            "Training Step: 48676  | time: 0.135s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9996 -- iter: 176/405\n",
            "Training Step: 48677  | time: 0.140s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9996 -- iter: 184/405\n",
            "Training Step: 48678  | time: 0.144s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9997 -- iter: 192/405\n",
            "Training Step: 48679  | time: 0.150s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9872 -- iter: 200/405\n",
            "Training Step: 48680  | time: 0.156s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9885 -- iter: 208/405\n",
            "Training Step: 48681  | time: 0.161s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9896 -- iter: 216/405\n",
            "Training Step: 48682  | time: 0.167s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9907 -- iter: 224/405\n",
            "Training Step: 48683  | time: 0.173s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9916 -- iter: 232/405\n",
            "Training Step: 48684  | time: 0.178s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9924 -- iter: 240/405\n",
            "Training Step: 48685  | time: 0.185s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9932 -- iter: 248/405\n",
            "Training Step: 48686  | time: 0.189s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9939 -- iter: 256/405\n",
            "Training Step: 48687  | time: 0.195s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9945 -- iter: 264/405\n",
            "Training Step: 48688  | time: 0.200s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9950 -- iter: 272/405\n",
            "Training Step: 48689  | time: 0.204s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9955 -- iter: 280/405\n",
            "Training Step: 48690  | time: 0.208s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9960 -- iter: 288/405\n",
            "Training Step: 48691  | time: 0.211s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9964 -- iter: 296/405\n",
            "Training Step: 48692  | time: 0.215s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9092 -- iter: 304/405\n",
            "Training Step: 48693  | time: 0.221s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9183 -- iter: 312/405\n",
            "Training Step: 48694  | time: 0.225s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9265 -- iter: 320/405\n",
            "Training Step: 48695  | time: 0.230s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9338 -- iter: 328/405\n",
            "Training Step: 48696  | time: 0.234s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9405 -- iter: 336/405\n",
            "Training Step: 48697  | time: 0.238s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9464 -- iter: 344/405\n",
            "Training Step: 48698  | time: 0.246s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9518 -- iter: 352/405\n",
            "Training Step: 48699  | time: 0.250s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9566 -- iter: 360/405\n",
            "Training Step: 48700  | time: 0.257s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9609 -- iter: 368/405\n",
            "Training Step: 48701  | time: 0.261s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9648 -- iter: 376/405\n",
            "Training Step: 48702  | time: 0.266s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9684 -- iter: 384/405\n",
            "Training Step: 48703  | time: 0.271s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9715 -- iter: 392/405\n",
            "Training Step: 48704  | time: 0.276s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9744 -- iter: 400/405\n",
            "Training Step: 48705  | time: 0.281s\n",
            "| Adam | epoch: 955 | loss: 0.00000 - acc: 0.9769 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48706  | time: 0.005s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9792 -- iter: 008/405\n",
            "Training Step: 48707  | time: 0.010s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9813 -- iter: 016/405\n",
            "Training Step: 48708  | time: 0.015s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9832 -- iter: 024/405\n",
            "Training Step: 48709  | time: 0.021s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9849 -- iter: 032/405\n",
            "Training Step: 48710  | time: 0.028s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9864 -- iter: 040/405\n",
            "Training Step: 48711  | time: 0.034s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9877 -- iter: 048/405\n",
            "Training Step: 48712  | time: 0.039s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9890 -- iter: 056/405\n",
            "Training Step: 48713  | time: 0.044s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9901 -- iter: 064/405\n",
            "Training Step: 48714  | time: 0.048s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9911 -- iter: 072/405\n",
            "Training Step: 48715  | time: 0.053s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9920 -- iter: 080/405\n",
            "Training Step: 48716  | time: 0.060s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9928 -- iter: 088/405\n",
            "Training Step: 48717  | time: 0.066s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9935 -- iter: 096/405\n",
            "Training Step: 48718  | time: 0.071s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9941 -- iter: 104/405\n",
            "Training Step: 48719  | time: 0.076s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9947 -- iter: 112/405\n",
            "Training Step: 48720  | time: 0.085s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9953 -- iter: 120/405\n",
            "Training Step: 48721  | time: 0.090s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9957 -- iter: 128/405\n",
            "Training Step: 48722  | time: 0.095s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9962 -- iter: 136/405\n",
            "Training Step: 48723  | time: 0.101s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9965 -- iter: 144/405\n",
            "Training Step: 48724  | time: 0.106s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9969 -- iter: 152/405\n",
            "Training Step: 48725  | time: 0.112s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9972 -- iter: 160/405\n",
            "Training Step: 48726  | time: 0.117s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9975 -- iter: 168/405\n",
            "Training Step: 48727  | time: 0.121s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9977 -- iter: 176/405\n",
            "Training Step: 48728  | time: 0.126s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9980 -- iter: 184/405\n",
            "Training Step: 48729  | time: 0.129s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9982 -- iter: 192/405\n",
            "Training Step: 48730  | time: 0.134s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9983 -- iter: 200/405\n",
            "Training Step: 48731  | time: 0.139s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9860 -- iter: 208/405\n",
            "Training Step: 48732  | time: 0.144s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9874 -- iter: 216/405\n",
            "Training Step: 48733  | time: 0.149s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9887 -- iter: 224/405\n",
            "Training Step: 48734  | time: 0.153s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9898 -- iter: 232/405\n",
            "Training Step: 48735  | time: 0.157s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9908 -- iter: 240/405\n",
            "Training Step: 48736  | time: 0.161s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9917 -- iter: 248/405\n",
            "Training Step: 48737  | time: 0.165s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9926 -- iter: 256/405\n",
            "Training Step: 48738  | time: 0.169s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9933 -- iter: 264/405\n",
            "Training Step: 48739  | time: 0.178s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9940 -- iter: 272/405\n",
            "Training Step: 48740  | time: 0.186s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9946 -- iter: 280/405\n",
            "Training Step: 48741  | time: 0.196s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9951 -- iter: 288/405\n",
            "Training Step: 48742  | time: 0.202s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9956 -- iter: 296/405\n",
            "Training Step: 48743  | time: 0.210s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9960 -- iter: 304/405\n",
            "Training Step: 48744  | time: 0.216s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9839 -- iter: 312/405\n",
            "Training Step: 48745  | time: 0.221s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9855 -- iter: 320/405\n",
            "Training Step: 48746  | time: 0.229s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9883 -- iter: 328/405\n",
            "Training Step: 48747  | time: 0.234s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9895 -- iter: 336/405\n",
            "Training Step: 48748  | time: 0.239s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9895 -- iter: 344/405\n",
            "Training Step: 48749  | time: 0.248s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9905 -- iter: 352/405\n",
            "Training Step: 48750  | time: 0.258s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9923 -- iter: 360/405\n",
            "Training Step: 48751  | time: 0.266s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9931 -- iter: 368/405\n",
            "Training Step: 48752  | time: 0.275s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9938 -- iter: 376/405\n",
            "Training Step: 48753  | time: 0.283s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9938 -- iter: 384/405\n",
            "Training Step: 48754  | time: 0.288s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9944 -- iter: 392/405\n",
            "Training Step: 48755  | time: 0.294s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9950 -- iter: 400/405\n",
            "Training Step: 48756  | time: 0.299s\n",
            "| Adam | epoch: 956 | loss: 0.00000 - acc: 0.9959 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48757  | time: 0.006s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9963 -- iter: 008/405\n",
            "Training Step: 48758  | time: 0.011s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9963 -- iter: 016/405\n",
            "Training Step: 48759  | time: 0.018s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9967 -- iter: 024/405\n",
            "Training Step: 48760  | time: 0.024s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9973 -- iter: 032/405\n",
            "Training Step: 48761  | time: 0.031s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9976 -- iter: 040/405\n",
            "Training Step: 48762  | time: 0.037s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9976 -- iter: 048/405\n",
            "Training Step: 48763  | time: 0.044s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9978 -- iter: 056/405\n",
            "Training Step: 48764  | time: 0.051s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9982 -- iter: 064/405\n",
            "Training Step: 48765  | time: 0.057s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9982 -- iter: 072/405\n",
            "Training Step: 48766  | time: 0.063s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9984 -- iter: 080/405\n",
            "Training Step: 48767  | time: 0.068s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9986 -- iter: 088/405\n",
            "Training Step: 48768  | time: 0.074s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9987 -- iter: 096/405\n",
            "Training Step: 48769  | time: 0.081s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9988 -- iter: 104/405\n",
            "Training Step: 48770  | time: 0.086s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9990 -- iter: 112/405\n",
            "Training Step: 48771  | time: 0.095s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9991 -- iter: 120/405\n",
            "Training Step: 48772  | time: 0.101s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9992 -- iter: 128/405\n",
            "Training Step: 48773  | time: 0.109s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9992 -- iter: 136/405\n",
            "Training Step: 48774  | time: 0.114s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9993 -- iter: 144/405\n",
            "Training Step: 48775  | time: 0.122s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9994 -- iter: 152/405\n",
            "Training Step: 48776  | time: 0.125s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9994 -- iter: 160/405\n",
            "Training Step: 48777  | time: 0.128s\n",
            "| Adam | epoch: 957 | loss: 0.00000 - acc: 0.9995 -- iter: 168/405\n",
            "Training Step: 48849  | time: 0.230s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9964 -- iter: 336/405\n",
            "Training Step: 48850  | time: 0.237s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9968 -- iter: 344/405\n",
            "Training Step: 48851  | time: 0.244s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9971 -- iter: 352/405\n",
            "Training Step: 48852  | time: 0.248s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9974 -- iter: 360/405\n",
            "Training Step: 48853  | time: 0.253s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9977 -- iter: 368/405\n",
            "Training Step: 48854  | time: 0.259s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9979 -- iter: 376/405\n",
            "Training Step: 48855  | time: 0.265s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9981 -- iter: 384/405\n",
            "Training Step: 48856  | time: 0.270s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9983 -- iter: 392/405\n",
            "Training Step: 48857  | time: 0.274s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9985 -- iter: 400/405\n",
            "Training Step: 48858  | time: 0.281s\n",
            "| Adam | epoch: 958 | loss: 0.00000 - acc: 0.9986 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48859  | time: 0.004s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9988 -- iter: 008/405\n",
            "Training Step: 48860  | time: 0.009s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9989 -- iter: 016/405\n",
            "Training Step: 48861  | time: 0.014s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9990 -- iter: 024/405\n",
            "Training Step: 48862  | time: 0.019s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9991 -- iter: 032/405\n",
            "Training Step: 48863  | time: 0.024s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9992 -- iter: 040/405\n",
            "Training Step: 48864  | time: 0.029s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9993 -- iter: 048/405\n",
            "Training Step: 48865  | time: 0.034s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9993 -- iter: 056/405\n",
            "Training Step: 48866  | time: 0.040s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9994 -- iter: 064/405\n",
            "Training Step: 48867  | time: 0.045s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9995 -- iter: 072/405\n",
            "Training Step: 48868  | time: 0.050s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9995 -- iter: 080/405\n",
            "Training Step: 48869  | time: 0.056s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9996 -- iter: 088/405\n",
            "Training Step: 48870  | time: 0.061s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9996 -- iter: 096/405\n",
            "Training Step: 48871  | time: 0.065s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9996 -- iter: 104/405\n",
            "Training Step: 48872  | time: 0.070s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9997 -- iter: 112/405\n",
            "Training Step: 48873  | time: 0.075s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9997 -- iter: 120/405\n",
            "Training Step: 48874  | time: 0.079s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9997 -- iter: 128/405\n",
            "Training Step: 48875  | time: 0.084s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9998 -- iter: 136/405\n",
            "Training Step: 48876  | time: 0.089s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9998 -- iter: 144/405\n",
            "Training Step: 48877  | time: 0.095s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9998 -- iter: 152/405\n",
            "Training Step: 48878  | time: 0.100s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9998 -- iter: 160/405\n",
            "Training Step: 48879  | time: 0.104s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9998 -- iter: 168/405\n",
            "Training Step: 48880  | time: 0.110s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9999 -- iter: 176/405\n",
            "Training Step: 48881  | time: 0.116s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9999 -- iter: 184/405\n",
            "Training Step: 48882  | time: 0.120s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9874 -- iter: 192/405\n",
            "Training Step: 48883  | time: 0.127s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9887 -- iter: 200/405\n",
            "Training Step: 48884  | time: 0.132s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9898 -- iter: 208/405\n",
            "Training Step: 48885  | time: 0.137s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9908 -- iter: 216/405\n",
            "Training Step: 48886  | time: 0.143s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9917 -- iter: 224/405\n",
            "Training Step: 48887  | time: 0.149s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9926 -- iter: 232/405\n",
            "Training Step: 48888  | time: 0.154s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9933 -- iter: 240/405\n",
            "Training Step: 48889  | time: 0.159s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9940 -- iter: 248/405\n",
            "Training Step: 48890  | time: 0.165s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9946 -- iter: 256/405\n",
            "Training Step: 48891  | time: 0.169s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9951 -- iter: 264/405\n",
            "Training Step: 48892  | time: 0.175s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9956 -- iter: 272/405\n",
            "Training Step: 48893  | time: 0.181s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9960 -- iter: 280/405\n",
            "Training Step: 48894  | time: 0.186s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9964 -- iter: 288/405\n",
            "Training Step: 48895  | time: 0.192s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9968 -- iter: 296/405\n",
            "Training Step: 48896  | time: 0.197s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9971 -- iter: 304/405\n",
            "Training Step: 48897  | time: 0.202s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9974 -- iter: 312/405\n",
            "Training Step: 48898  | time: 0.208s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9977 -- iter: 320/405\n",
            "Training Step: 48899  | time: 0.215s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9979 -- iter: 328/405\n",
            "Training Step: 48900  | time: 0.221s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9981 -- iter: 336/405\n",
            "Training Step: 48901  | time: 0.226s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9983 -- iter: 344/405\n",
            "Training Step: 48902  | time: 0.231s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9985 -- iter: 352/405\n",
            "Training Step: 48903  | time: 0.236s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9986 -- iter: 360/405\n",
            "Training Step: 48904  | time: 0.240s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9988 -- iter: 368/405\n",
            "Training Step: 48905  | time: 0.247s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9989 -- iter: 376/405\n",
            "Training Step: 48906  | time: 0.252s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9990 -- iter: 384/405\n",
            "Training Step: 48907  | time: 0.257s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9991 -- iter: 392/405\n",
            "Training Step: 48908  | time: 0.265s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9992 -- iter: 400/405\n",
            "Training Step: 48909  | time: 0.268s\n",
            "| Adam | epoch: 959 | loss: 0.00000 - acc: 0.9993 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48910  | time: 0.004s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9993 -- iter: 008/405\n",
            "Training Step: 48911  | time: 0.009s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9994 -- iter: 016/405\n",
            "Training Step: 48912  | time: 0.015s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9995 -- iter: 024/405\n",
            "Training Step: 48913  | time: 0.024s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9995 -- iter: 032/405\n",
            "Training Step: 48914  | time: 0.029s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9996 -- iter: 040/405\n",
            "Training Step: 48915  | time: 0.036s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9996 -- iter: 048/405\n",
            "Training Step: 48916  | time: 0.044s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9996 -- iter: 056/405\n",
            "Training Step: 48917  | time: 0.050s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9997 -- iter: 064/405\n",
            "Training Step: 48918  | time: 0.059s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9997 -- iter: 072/405\n",
            "Training Step: 48919  | time: 0.064s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9997 -- iter: 080/405\n",
            "Training Step: 48920  | time: 0.070s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9998 -- iter: 088/405\n",
            "Training Step: 48921  | time: 0.075s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9998 -- iter: 096/405\n",
            "Training Step: 48922  | time: 0.079s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9998 -- iter: 104/405\n",
            "Training Step: 48923  | time: 0.085s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9998 -- iter: 112/405\n",
            "Training Step: 48924  | time: 0.093s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9998 -- iter: 120/405\n",
            "Training Step: 48925  | time: 0.099s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9999 -- iter: 128/405\n",
            "Training Step: 48926  | time: 0.104s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9999 -- iter: 136/405\n",
            "Training Step: 48927  | time: 0.110s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9999 -- iter: 144/405\n",
            "Training Step: 48928  | time: 0.115s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9999 -- iter: 152/405\n",
            "Training Step: 48929  | time: 0.122s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9874 -- iter: 160/405\n",
            "Training Step: 48930  | time: 0.127s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9887 -- iter: 168/405\n",
            "Training Step: 48931  | time: 0.134s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9898 -- iter: 176/405\n",
            "Training Step: 48932  | time: 0.139s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9908 -- iter: 184/405\n",
            "Training Step: 48933  | time: 0.145s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9917 -- iter: 192/405\n",
            "Training Step: 48934  | time: 0.151s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9926 -- iter: 200/405\n",
            "Training Step: 48935  | time: 0.160s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9933 -- iter: 208/405\n",
            "Training Step: 48936  | time: 0.165s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9940 -- iter: 216/405\n",
            "Training Step: 48937  | time: 0.170s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9946 -- iter: 224/405\n",
            "Training Step: 48938  | time: 0.177s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9951 -- iter: 232/405\n",
            "Training Step: 48939  | time: 0.182s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9956 -- iter: 240/405\n",
            "Training Step: 48940  | time: 0.191s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9960 -- iter: 248/405\n",
            "Training Step: 48941  | time: 0.197s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9964 -- iter: 256/405\n",
            "Training Step: 48942  | time: 0.206s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9968 -- iter: 264/405\n",
            "Training Step: 48943  | time: 0.212s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9971 -- iter: 272/405\n",
            "Training Step: 48944  | time: 0.219s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9974 -- iter: 280/405\n",
            "Training Step: 48945  | time: 0.224s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9977 -- iter: 288/405\n",
            "Training Step: 48946  | time: 0.230s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9979 -- iter: 296/405\n",
            "Training Step: 48947  | time: 0.236s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9981 -- iter: 304/405\n",
            "Training Step: 48948  | time: 0.241s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9983 -- iter: 312/405\n",
            "Training Step: 48949  | time: 0.246s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9985 -- iter: 320/405\n",
            "Training Step: 48950  | time: 0.251s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9986 -- iter: 328/405\n",
            "Training Step: 48951  | time: 0.256s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9988 -- iter: 336/405\n",
            "Training Step: 48952  | time: 0.262s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9989 -- iter: 344/405\n",
            "Training Step: 48953  | time: 0.269s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9990 -- iter: 352/405\n",
            "Training Step: 48954  | time: 0.274s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9991 -- iter: 360/405\n",
            "Training Step: 48955  | time: 0.279s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9992 -- iter: 368/405\n",
            "Training Step: 48956  | time: 0.289s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9993 -- iter: 376/405\n",
            "Training Step: 48957  | time: 0.295s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9993 -- iter: 384/405\n",
            "Training Step: 48958  | time: 0.298s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9994 -- iter: 392/405\n",
            "Training Step: 48959  | time: 0.303s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9995 -- iter: 400/405\n",
            "Training Step: 48960  | time: 0.307s\n",
            "| Adam | epoch: 960 | loss: 0.00000 - acc: 0.9995 -- iter: 405/405\n",
            "--\n",
            "Training Step: 48961  | time: 0.005s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9996 -- iter: 008/405\n",
            "Training Step: 48962  | time: 0.009s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9996 -- iter: 016/405\n",
            "Training Step: 48963  | time: 0.015s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9996 -- iter: 024/405\n",
            "Training Step: 48964  | time: 0.021s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9997 -- iter: 032/405\n",
            "Training Step: 48965  | time: 0.028s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9997 -- iter: 040/405\n",
            "Training Step: 48966  | time: 0.035s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9997 -- iter: 048/405\n",
            "Training Step: 48967  | time: 0.042s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9998 -- iter: 056/405\n",
            "Training Step: 48968  | time: 0.048s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9998 -- iter: 064/405\n",
            "Training Step: 48969  | time: 0.053s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9998 -- iter: 072/405\n",
            "Training Step: 48970  | time: 0.059s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9998 -- iter: 080/405\n",
            "Training Step: 48971  | time: 0.064s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9998 -- iter: 088/405\n",
            "Training Step: 48972  | time: 0.069s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 096/405\n",
            "Training Step: 48973  | time: 0.074s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 104/405\n",
            "Training Step: 48974  | time: 0.082s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 112/405\n",
            "Training Step: 48975  | time: 0.086s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 120/405\n",
            "Training Step: 48976  | time: 0.092s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 128/405\n",
            "Training Step: 48977  | time: 0.098s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 136/405\n",
            "Training Step: 48978  | time: 0.104s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 144/405\n",
            "Training Step: 48979  | time: 0.110s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 152/405\n",
            "Training Step: 48980  | time: 0.115s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9999 -- iter: 160/405\n",
            "Training Step: 48981  | time: 0.120s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9874 -- iter: 168/405\n",
            "Training Step: 48982  | time: 0.124s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9887 -- iter: 176/405\n",
            "Training Step: 48983  | time: 0.128s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9898 -- iter: 184/405\n",
            "Training Step: 48984  | time: 0.132s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9908 -- iter: 192/405\n",
            "Training Step: 48985  | time: 0.138s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9918 -- iter: 200/405\n",
            "Training Step: 48986  | time: 0.146s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9926 -- iter: 208/405\n",
            "Training Step: 48987  | time: 0.151s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9933 -- iter: 216/405\n",
            "Training Step: 48988  | time: 0.156s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9940 -- iter: 224/405\n",
            "Training Step: 48989  | time: 0.161s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9946 -- iter: 232/405\n",
            "Training Step: 48990  | time: 0.165s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9951 -- iter: 240/405\n",
            "Training Step: 48991  | time: 0.170s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9956 -- iter: 248/405\n",
            "Training Step: 48992  | time: 0.175s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9961 -- iter: 256/405\n",
            "Training Step: 48993  | time: 0.181s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9965 -- iter: 264/405\n",
            "Training Step: 48994  | time: 0.185s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9968 -- iter: 272/405\n",
            "Training Step: 48995  | time: 0.190s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9971 -- iter: 280/405\n",
            "Training Step: 48996  | time: 0.195s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9974 -- iter: 288/405\n",
            "Training Step: 48997  | time: 0.200s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9977 -- iter: 296/405\n",
            "Training Step: 48998  | time: 0.204s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9979 -- iter: 304/405\n",
            "Training Step: 48999  | time: 0.210s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9981 -- iter: 312/405\n",
            "Training Step: 49000  | time: 0.215s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9983 -- iter: 320/405\n",
            "Training Step: 49001  | time: 0.219s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9985 -- iter: 328/405\n",
            "Training Step: 49002  | time: 0.225s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9986 -- iter: 336/405\n",
            "Training Step: 49003  | time: 0.230s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9988 -- iter: 344/405\n",
            "Training Step: 49004  | time: 0.234s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9989 -- iter: 352/405\n",
            "Training Step: 49005  | time: 0.239s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9990 -- iter: 360/405\n",
            "Training Step: 49006  | time: 0.244s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9991 -- iter: 368/405\n",
            "Training Step: 49007  | time: 0.248s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9992 -- iter: 376/405\n",
            "Training Step: 49008  | time: 0.253s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9993 -- iter: 384/405\n",
            "Training Step: 49009  | time: 0.258s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9993 -- iter: 392/405\n",
            "Training Step: 49010  | time: 0.262s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9994 -- iter: 400/405\n",
            "Training Step: 49011  | time: 0.267s\n",
            "| Adam | epoch: 961 | loss: 0.00000 - acc: 0.9995 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49012  | time: 0.005s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9995 -- iter: 008/405\n",
            "Training Step: 49013  | time: 0.012s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9996 -- iter: 016/405\n",
            "Training Step: 49014  | time: 0.018s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9996 -- iter: 024/405\n",
            "Training Step: 49015  | time: 0.028s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9872 -- iter: 032/405\n",
            "Training Step: 49016  | time: 0.033s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9884 -- iter: 040/405\n",
            "Training Step: 49017  | time: 0.039s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9896 -- iter: 048/405\n",
            "Training Step: 49018  | time: 0.044s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9906 -- iter: 056/405\n",
            "Training Step: 49019  | time: 0.053s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9916 -- iter: 064/405\n",
            "Training Step: 49020  | time: 0.058s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9924 -- iter: 072/405\n",
            "Training Step: 49021  | time: 0.065s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9932 -- iter: 080/405\n",
            "Training Step: 49022  | time: 0.070s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9939 -- iter: 088/405\n",
            "Training Step: 49023  | time: 0.077s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9945 -- iter: 096/405\n",
            "Training Step: 49024  | time: 0.084s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9950 -- iter: 104/405\n",
            "Training Step: 49025  | time: 0.091s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9955 -- iter: 112/405\n",
            "Training Step: 49026  | time: 0.097s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9960 -- iter: 120/405\n",
            "Training Step: 49027  | time: 0.103s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9964 -- iter: 128/405\n",
            "Training Step: 49028  | time: 0.107s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9967 -- iter: 136/405\n",
            "Training Step: 49029  | time: 0.114s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9971 -- iter: 144/405\n",
            "Training Step: 49030  | time: 0.118s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9974 -- iter: 152/405\n",
            "Training Step: 49031  | time: 0.125s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9976 -- iter: 160/405\n",
            "Training Step: 49032  | time: 0.131s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9979 -- iter: 168/405\n",
            "Training Step: 49033  | time: 0.136s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9981 -- iter: 176/405\n",
            "Training Step: 49034  | time: 0.142s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9983 -- iter: 184/405\n",
            "Training Step: 49035  | time: 0.148s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9984 -- iter: 192/405\n",
            "Training Step: 49036  | time: 0.153s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9986 -- iter: 200/405\n",
            "Training Step: 49037  | time: 0.157s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9987 -- iter: 208/405\n",
            "Training Step: 49038  | time: 0.161s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9989 -- iter: 216/405\n",
            "Training Step: 49039  | time: 0.168s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9990 -- iter: 224/405\n",
            "Training Step: 49040  | time: 0.173s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9991 -- iter: 232/405\n",
            "Training Step: 49041  | time: 0.180s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9992 -- iter: 240/405\n",
            "Training Step: 49042  | time: 0.187s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9993 -- iter: 248/405\n",
            "Training Step: 49043  | time: 0.193s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9993 -- iter: 256/405\n",
            "Training Step: 49044  | time: 0.197s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9994 -- iter: 264/405\n",
            "Training Step: 49045  | time: 0.205s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9870 -- iter: 272/405\n",
            "Training Step: 49046  | time: 0.212s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9883 -- iter: 280/405\n",
            "Training Step: 49047  | time: 0.217s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9894 -- iter: 288/405\n",
            "Training Step: 49048  | time: 0.223s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9905 -- iter: 296/405\n",
            "Training Step: 49049  | time: 0.229s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9914 -- iter: 304/405\n",
            "Training Step: 49050  | time: 0.237s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9923 -- iter: 312/405\n",
            "Training Step: 49051  | time: 0.242s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9931 -- iter: 320/405\n",
            "Training Step: 49052  | time: 0.249s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9938 -- iter: 328/405\n",
            "Training Step: 49053  | time: 0.254s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9944 -- iter: 336/405\n",
            "Training Step: 49054  | time: 0.260s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9949 -- iter: 344/405\n",
            "Training Step: 49055  | time: 0.266s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9955 -- iter: 352/405\n",
            "Training Step: 49056  | time: 0.272s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9959 -- iter: 360/405\n",
            "Training Step: 49057  | time: 0.278s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9963 -- iter: 368/405\n",
            "Training Step: 49058  | time: 0.284s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9967 -- iter: 376/405\n",
            "Training Step: 49059  | time: 0.290s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9970 -- iter: 384/405\n",
            "Training Step: 49060  | time: 0.297s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9973 -- iter: 392/405\n",
            "Training Step: 49061  | time: 0.303s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9976 -- iter: 400/405\n",
            "Training Step: 49062  | time: 0.309s\n",
            "| Adam | epoch: 962 | loss: 0.00000 - acc: 0.9978 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49063  | time: 0.007s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9980 -- iter: 008/405\n",
            "Training Step: 49064  | time: 0.011s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9982 -- iter: 016/405\n",
            "Training Step: 49065  | time: 0.019s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9984 -- iter: 024/405\n",
            "Training Step: 49066  | time: 0.025s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9986 -- iter: 032/405\n",
            "Training Step: 49067  | time: 0.030s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9987 -- iter: 040/405\n",
            "Training Step: 49068  | time: 0.035s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9988 -- iter: 048/405\n",
            "Training Step: 49069  | time: 0.040s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9865 -- iter: 056/405\n",
            "Training Step: 49070  | time: 0.047s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9878 -- iter: 064/405\n",
            "Training Step: 49071  | time: 0.054s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9890 -- iter: 072/405\n",
            "Training Step: 49072  | time: 0.061s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9901 -- iter: 080/405\n",
            "Training Step: 49073  | time: 0.067s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9911 -- iter: 088/405\n",
            "Training Step: 49074  | time: 0.074s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9920 -- iter: 096/405\n",
            "Training Step: 49075  | time: 0.082s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9928 -- iter: 104/405\n",
            "Training Step: 49076  | time: 0.088s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9935 -- iter: 112/405\n",
            "Training Step: 49077  | time: 0.095s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9942 -- iter: 120/405\n",
            "Training Step: 49078  | time: 0.100s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9948 -- iter: 128/405\n",
            "Training Step: 49079  | time: 0.107s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9953 -- iter: 136/405\n",
            "Training Step: 49080  | time: 0.112s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9958 -- iter: 144/405\n",
            "Training Step: 49081  | time: 0.118s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9962 -- iter: 152/405\n",
            "Training Step: 49082  | time: 0.126s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9966 -- iter: 160/405\n",
            "Training Step: 49083  | time: 0.132s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9969 -- iter: 168/405\n",
            "Training Step: 49084  | time: 0.138s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9972 -- iter: 176/405\n",
            "Training Step: 49085  | time: 0.146s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9975 -- iter: 184/405\n",
            "Training Step: 49086  | time: 0.151s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9977 -- iter: 192/405\n",
            "Training Step: 49087  | time: 0.158s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9980 -- iter: 200/405\n",
            "Training Step: 49088  | time: 0.163s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9982 -- iter: 208/405\n",
            "Training Step: 49089  | time: 0.168s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9984 -- iter: 216/405\n",
            "Training Step: 49090  | time: 0.175s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9985 -- iter: 224/405\n",
            "Training Step: 49091  | time: 0.180s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9987 -- iter: 232/405\n",
            "Training Step: 49092  | time: 0.187s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9988 -- iter: 240/405\n",
            "Training Step: 49093  | time: 0.192s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9989 -- iter: 248/405\n",
            "Training Step: 49094  | time: 0.202s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9990 -- iter: 256/405\n",
            "Training Step: 49095  | time: 0.208s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9991 -- iter: 264/405\n",
            "Training Step: 49096  | time: 0.213s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9992 -- iter: 272/405\n",
            "Training Step: 49097  | time: 0.219s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9993 -- iter: 280/405\n",
            "Training Step: 49098  | time: 0.224s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9994 -- iter: 288/405\n",
            "Training Step: 49099  | time: 0.231s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9994 -- iter: 296/405\n",
            "Training Step: 49100  | time: 0.237s\n",
            "| Adam | epoch: 963 | loss: 0.00000 - acc: 0.9995 -- iter: 304/405\n",
            "Training Step: 49474  | time: 0.028s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9966 -- iter: 032/405\n",
            "Training Step: 49475  | time: 0.036s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9970 -- iter: 040/405\n",
            "Training Step: 49476  | time: 0.043s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9973 -- iter: 048/405\n",
            "Training Step: 49477  | time: 0.049s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9975 -- iter: 056/405\n",
            "Training Step: 49478  | time: 0.058s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9978 -- iter: 064/405\n",
            "Training Step: 49479  | time: 0.063s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9980 -- iter: 072/405\n",
            "Training Step: 49480  | time: 0.068s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9982 -- iter: 080/405\n",
            "Training Step: 49481  | time: 0.075s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9984 -- iter: 088/405\n",
            "Training Step: 49482  | time: 0.081s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9985 -- iter: 096/405\n",
            "Training Step: 49483  | time: 0.087s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9987 -- iter: 104/405\n",
            "Training Step: 49484  | time: 0.094s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9988 -- iter: 112/405\n",
            "Training Step: 49485  | time: 0.100s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9989 -- iter: 120/405\n",
            "Training Step: 49486  | time: 0.105s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9990 -- iter: 128/405\n",
            "Training Step: 49487  | time: 0.110s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9991 -- iter: 136/405\n",
            "Training Step: 49488  | time: 0.115s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9992 -- iter: 144/405\n",
            "Training Step: 49489  | time: 0.120s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9993 -- iter: 152/405\n",
            "Training Step: 49490  | time: 0.125s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9994 -- iter: 160/405\n",
            "Training Step: 49491  | time: 0.129s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9994 -- iter: 168/405\n",
            "Training Step: 49492  | time: 0.133s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9995 -- iter: 176/405\n",
            "Training Step: 49493  | time: 0.139s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9995 -- iter: 184/405\n",
            "Training Step: 49494  | time: 0.144s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9996 -- iter: 192/405\n",
            "Training Step: 49495  | time: 0.148s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9996 -- iter: 200/405\n",
            "Training Step: 49496  | time: 0.155s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9997 -- iter: 208/405\n",
            "Training Step: 49497  | time: 0.158s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9997 -- iter: 216/405\n",
            "Training Step: 49498  | time: 0.162s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9997 -- iter: 224/405\n",
            "Training Step: 49499  | time: 0.168s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9998 -- iter: 232/405\n",
            "Training Step: 49500  | time: 0.176s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9998 -- iter: 240/405\n",
            "Training Step: 49501  | time: 0.182s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9998 -- iter: 248/405\n",
            "Training Step: 49502  | time: 0.186s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9998 -- iter: 256/405\n",
            "Training Step: 49503  | time: 0.192s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9998 -- iter: 264/405\n",
            "Training Step: 49504  | time: 0.199s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 272/405\n",
            "Training Step: 49505  | time: 0.205s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 280/405\n",
            "Training Step: 49506  | time: 0.211s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 288/405\n",
            "Training Step: 49507  | time: 0.217s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 296/405\n",
            "Training Step: 49508  | time: 0.223s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 304/405\n",
            "Training Step: 49509  | time: 0.228s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 312/405\n",
            "Training Step: 49510  | time: 0.236s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9999 -- iter: 320/405\n",
            "Training Step: 49511  | time: 0.241s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9874 -- iter: 328/405\n",
            "Training Step: 49512  | time: 0.246s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9887 -- iter: 336/405\n",
            "Training Step: 49513  | time: 0.251s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9898 -- iter: 344/405\n",
            "Training Step: 49514  | time: 0.258s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9908 -- iter: 352/405\n",
            "Training Step: 49515  | time: 0.264s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9918 -- iter: 360/405\n",
            "Training Step: 49516  | time: 0.272s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9926 -- iter: 368/405\n",
            "Training Step: 49517  | time: 0.276s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9933 -- iter: 376/405\n",
            "Training Step: 49518  | time: 0.280s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9940 -- iter: 384/405\n",
            "Training Step: 49519  | time: 0.285s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9946 -- iter: 392/405\n",
            "Training Step: 49520  | time: 0.291s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9951 -- iter: 400/405\n",
            "Training Step: 49521  | time: 0.296s\n",
            "| Adam | epoch: 971 | loss: 0.00000 - acc: 0.9956 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49522  | time: 0.004s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9961 -- iter: 008/405\n",
            "Training Step: 49523  | time: 0.010s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9965 -- iter: 016/405\n",
            "Training Step: 49524  | time: 0.015s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9968 -- iter: 024/405\n",
            "Training Step: 49525  | time: 0.022s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9971 -- iter: 032/405\n",
            "Training Step: 49526  | time: 0.028s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9974 -- iter: 040/405\n",
            "Training Step: 49527  | time: 0.033s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9977 -- iter: 048/405\n",
            "Training Step: 49528  | time: 0.038s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9979 -- iter: 056/405\n",
            "Training Step: 49529  | time: 0.043s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9981 -- iter: 064/405\n",
            "Training Step: 49530  | time: 0.050s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9983 -- iter: 072/405\n",
            "Training Step: 49531  | time: 0.055s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9985 -- iter: 080/405\n",
            "Training Step: 49532  | time: 0.059s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9986 -- iter: 088/405\n",
            "Training Step: 49533  | time: 0.064s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9988 -- iter: 096/405\n",
            "Training Step: 49534  | time: 0.070s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9989 -- iter: 104/405\n",
            "Training Step: 49535  | time: 0.077s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9990 -- iter: 112/405\n",
            "Training Step: 49536  | time: 0.082s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9991 -- iter: 120/405\n",
            "Training Step: 49537  | time: 0.089s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9992 -- iter: 128/405\n",
            "Training Step: 49538  | time: 0.094s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9993 -- iter: 136/405\n",
            "Training Step: 49539  | time: 0.099s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9993 -- iter: 144/405\n",
            "Training Step: 49540  | time: 0.104s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9994 -- iter: 152/405\n",
            "Training Step: 49541  | time: 0.109s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9995 -- iter: 160/405\n",
            "Training Step: 49542  | time: 0.116s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9995 -- iter: 168/405\n",
            "Training Step: 49543  | time: 0.124s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9996 -- iter: 176/405\n",
            "Training Step: 49544  | time: 0.130s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9996 -- iter: 184/405\n",
            "Training Step: 49545  | time: 0.138s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9997 -- iter: 192/405\n",
            "Training Step: 49546  | time: 0.144s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9997 -- iter: 200/405\n",
            "Training Step: 49547  | time: 0.148s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9997 -- iter: 208/405\n",
            "Training Step: 49548  | time: 0.153s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9997 -- iter: 216/405\n",
            "Training Step: 49549  | time: 0.159s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9998 -- iter: 224/405\n",
            "Training Step: 49550  | time: 0.165s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9998 -- iter: 232/405\n",
            "Training Step: 49551  | time: 0.171s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9998 -- iter: 240/405\n",
            "Training Step: 49552  | time: 0.177s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9998 -- iter: 248/405\n",
            "Training Step: 49553  | time: 0.182s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9998 -- iter: 256/405\n",
            "Training Step: 49554  | time: 0.188s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 264/405\n",
            "Training Step: 49555  | time: 0.193s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 272/405\n",
            "Training Step: 49556  | time: 0.199s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 280/405\n",
            "Training Step: 49557  | time: 0.205s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 288/405\n",
            "Training Step: 49558  | time: 0.211s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 296/405\n",
            "Training Step: 49559  | time: 0.216s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 304/405\n",
            "Training Step: 49560  | time: 0.222s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 312/405\n",
            "Training Step: 49561  | time: 0.229s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 320/405\n",
            "Training Step: 49562  | time: 0.234s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 328/405\n",
            "Training Step: 49563  | time: 0.242s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9999 -- iter: 336/405\n",
            "Training Step: 49564  | time: 0.246s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 1.0000 -- iter: 344/405\n",
            "Training Step: 49565  | time: 0.253s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 1.0000 -- iter: 352/405\n",
            "Training Step: 49566  | time: 0.258s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 1.0000 -- iter: 360/405\n",
            "Training Step: 49567  | time: 0.265s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 1.0000 -- iter: 368/405\n",
            "Training Step: 49568  | time: 0.270s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 1.0000 -- iter: 376/405\n",
            "Training Step: 49569  | time: 0.277s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9875 -- iter: 384/405\n",
            "Training Step: 49570  | time: 0.282s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9887 -- iter: 392/405\n",
            "Training Step: 49571  | time: 0.287s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9899 -- iter: 400/405\n",
            "Training Step: 49572  | time: 0.293s\n",
            "| Adam | epoch: 972 | loss: 0.00000 - acc: 0.9909 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49573  | time: 0.005s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9918 -- iter: 008/405\n",
            "Training Step: 49574  | time: 0.010s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9926 -- iter: 016/405\n",
            "Training Step: 49575  | time: 0.017s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9808 -- iter: 024/405\n",
            "Training Step: 49576  | time: 0.022s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9828 -- iter: 032/405\n",
            "Training Step: 49577  | time: 0.027s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9845 -- iter: 040/405\n",
            "Training Step: 49578  | time: 0.034s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9860 -- iter: 048/405\n",
            "Training Step: 49579  | time: 0.039s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9874 -- iter: 056/405\n",
            "Training Step: 49580  | time: 0.045s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9887 -- iter: 064/405\n",
            "Training Step: 49581  | time: 0.050s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9898 -- iter: 072/405\n",
            "Training Step: 49582  | time: 0.058s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9908 -- iter: 080/405\n",
            "Training Step: 49583  | time: 0.064s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9918 -- iter: 088/405\n",
            "Training Step: 49584  | time: 0.069s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9926 -- iter: 096/405\n",
            "Training Step: 49585  | time: 0.074s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9933 -- iter: 104/405\n",
            "Training Step: 49586  | time: 0.081s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9940 -- iter: 112/405\n",
            "Training Step: 49587  | time: 0.086s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9946 -- iter: 120/405\n",
            "Training Step: 49588  | time: 0.093s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9951 -- iter: 128/405\n",
            "Training Step: 49589  | time: 0.099s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9956 -- iter: 136/405\n",
            "Training Step: 49590  | time: 0.104s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9961 -- iter: 144/405\n",
            "Training Step: 49591  | time: 0.110s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9965 -- iter: 152/405\n",
            "Training Step: 49592  | time: 0.116s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9968 -- iter: 160/405\n",
            "Training Step: 49593  | time: 0.122s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9971 -- iter: 168/405\n",
            "Training Step: 49594  | time: 0.127s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9974 -- iter: 176/405\n",
            "Training Step: 49595  | time: 0.131s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9977 -- iter: 184/405\n",
            "Training Step: 49596  | time: 0.137s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9854 -- iter: 192/405\n",
            "Training Step: 49597  | time: 0.142s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9869 -- iter: 200/405\n",
            "Training Step: 49598  | time: 0.147s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9882 -- iter: 208/405\n",
            "Training Step: 49599  | time: 0.150s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9894 -- iter: 216/405\n",
            "Training Step: 49600  | time: 0.155s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9904 -- iter: 224/405\n",
            "Training Step: 49601  | time: 0.162s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9914 -- iter: 232/405\n",
            "Training Step: 49602  | time: 0.167s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9922 -- iter: 240/405\n",
            "Training Step: 49603  | time: 0.172s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9930 -- iter: 248/405\n",
            "Training Step: 49604  | time: 0.180s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9937 -- iter: 256/405\n",
            "Training Step: 49605  | time: 0.185s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9943 -- iter: 264/405\n",
            "Training Step: 49606  | time: 0.190s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9949 -- iter: 272/405\n",
            "Training Step: 49607  | time: 0.194s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9954 -- iter: 280/405\n",
            "Training Step: 49608  | time: 0.199s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9959 -- iter: 288/405\n",
            "Training Step: 49609  | time: 0.203s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9963 -- iter: 296/405\n",
            "Training Step: 49610  | time: 0.207s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9967 -- iter: 304/405\n",
            "Training Step: 49611  | time: 0.212s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9970 -- iter: 312/405\n",
            "Training Step: 49612  | time: 0.224s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9973 -- iter: 320/405\n",
            "Training Step: 49613  | time: 0.230s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9976 -- iter: 328/405\n",
            "Training Step: 49614  | time: 0.235s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9978 -- iter: 336/405\n",
            "Training Step: 49615  | time: 0.240s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9980 -- iter: 344/405\n",
            "Training Step: 49616  | time: 0.246s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9982 -- iter: 352/405\n",
            "Training Step: 49617  | time: 0.251s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9984 -- iter: 360/405\n",
            "Training Step: 49618  | time: 0.255s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9986 -- iter: 368/405\n",
            "Training Step: 49619  | time: 0.260s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9987 -- iter: 376/405\n",
            "Training Step: 49620  | time: 0.265s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9863 -- iter: 384/405\n",
            "Training Step: 49621  | time: 0.270s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9877 -- iter: 392/405\n",
            "Training Step: 49622  | time: 0.274s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9889 -- iter: 400/405\n",
            "Training Step: 49623  | time: 0.278s\n",
            "| Adam | epoch: 973 | loss: 0.00000 - acc: 0.9900 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49624  | time: 0.003s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9910 -- iter: 008/405\n",
            "Training Step: 49625  | time: 0.007s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9919 -- iter: 016/405\n",
            "Training Step: 49626  | time: 0.011s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9927 -- iter: 024/405\n",
            "Training Step: 49627  | time: 0.015s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9935 -- iter: 032/405\n",
            "Training Step: 49628  | time: 0.019s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9066 -- iter: 040/405\n",
            "Training Step: 49629  | time: 0.022s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9160 -- iter: 048/405\n",
            "Training Step: 49630  | time: 0.025s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9244 -- iter: 056/405\n",
            "Training Step: 49631  | time: 0.029s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9319 -- iter: 064/405\n",
            "Training Step: 49632  | time: 0.033s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9262 -- iter: 072/405\n",
            "Training Step: 49633  | time: 0.037s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9336 -- iter: 080/405\n",
            "Training Step: 49634  | time: 0.041s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9402 -- iter: 088/405\n",
            "Training Step: 49635  | time: 0.044s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9462 -- iter: 096/405\n",
            "Training Step: 49636  | time: 0.048s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9516 -- iter: 104/405\n",
            "Training Step: 49637  | time: 0.051s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9564 -- iter: 112/405\n",
            "Training Step: 49638  | time: 0.057s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9608 -- iter: 120/405\n",
            "Training Step: 49639  | time: 0.061s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9647 -- iter: 128/405\n",
            "Training Step: 49640  | time: 0.064s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9682 -- iter: 136/405\n",
            "Training Step: 49641  | time: 0.067s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9714 -- iter: 144/405\n",
            "Training Step: 49642  | time: 0.071s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9743 -- iter: 152/405\n",
            "Training Step: 49643  | time: 0.074s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9769 -- iter: 160/405\n",
            "Training Step: 49644  | time: 0.084s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9792 -- iter: 168/405\n",
            "Training Step: 49645  | time: 0.089s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9812 -- iter: 176/405\n",
            "Training Step: 49646  | time: 0.093s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9831 -- iter: 184/405\n",
            "Training Step: 49647  | time: 0.097s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9848 -- iter: 192/405\n",
            "Training Step: 49648  | time: 0.102s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9863 -- iter: 200/405\n",
            "Training Step: 49649  | time: 0.106s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9877 -- iter: 208/405\n",
            "Training Step: 49650  | time: 0.110s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9889 -- iter: 216/405\n",
            "Training Step: 49651  | time: 0.114s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9900 -- iter: 224/405\n",
            "Training Step: 49652  | time: 0.118s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9910 -- iter: 232/405\n",
            "Training Step: 49653  | time: 0.122s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9919 -- iter: 240/405\n",
            "Training Step: 49654  | time: 0.126s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9927 -- iter: 248/405\n",
            "Training Step: 49655  | time: 0.131s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9935 -- iter: 256/405\n",
            "Training Step: 49656  | time: 0.135s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9941 -- iter: 264/405\n",
            "Training Step: 49657  | time: 0.149s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9947 -- iter: 272/405\n",
            "Training Step: 49658  | time: 0.156s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9952 -- iter: 280/405\n",
            "Training Step: 49659  | time: 0.162s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9957 -- iter: 288/405\n",
            "Training Step: 49660  | time: 0.169s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9961 -- iter: 296/405\n",
            "Training Step: 49661  | time: 0.175s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9965 -- iter: 304/405\n",
            "Training Step: 49662  | time: 0.182s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9969 -- iter: 312/405\n",
            "Training Step: 49663  | time: 0.188s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9972 -- iter: 320/405\n",
            "Training Step: 49664  | time: 0.194s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9975 -- iter: 328/405\n",
            "Training Step: 49665  | time: 0.200s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9977 -- iter: 336/405\n",
            "Training Step: 49666  | time: 0.206s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9979 -- iter: 344/405\n",
            "Training Step: 49667  | time: 0.211s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9982 -- iter: 352/405\n",
            "Training Step: 49668  | time: 0.218s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9983 -- iter: 360/405\n",
            "Training Step: 49669  | time: 0.223s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9985 -- iter: 368/405\n",
            "Training Step: 49670  | time: 0.228s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9987 -- iter: 376/405\n",
            "Training Step: 49671  | time: 0.237s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9988 -- iter: 384/405\n",
            "Training Step: 49672  | time: 0.243s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9989 -- iter: 392/405\n",
            "Training Step: 49673  | time: 0.249s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9990 -- iter: 400/405\n",
            "Training Step: 49674  | time: 0.255s\n",
            "| Adam | epoch: 974 | loss: 0.00000 - acc: 0.9991 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49675  | time: 0.006s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9992 -- iter: 008/405\n",
            "Training Step: 49676  | time: 0.012s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9993 -- iter: 016/405\n",
            "Training Step: 49677  | time: 0.019s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9994 -- iter: 024/405\n",
            "Training Step: 49678  | time: 0.023s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9994 -- iter: 032/405\n",
            "Training Step: 49679  | time: 0.027s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9995 -- iter: 040/405\n",
            "Training Step: 49680  | time: 0.032s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9995 -- iter: 048/405\n",
            "Training Step: 49681  | time: 0.037s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9996 -- iter: 056/405\n",
            "Training Step: 49682  | time: 0.046s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9996 -- iter: 064/405\n",
            "Training Step: 49683  | time: 0.051s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9997 -- iter: 072/405\n",
            "Training Step: 49684  | time: 0.055s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9997 -- iter: 080/405\n",
            "Training Step: 49685  | time: 0.060s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9997 -- iter: 088/405\n",
            "Training Step: 49686  | time: 0.065s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9998 -- iter: 096/405\n",
            "Training Step: 49687  | time: 0.071s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9998 -- iter: 104/405\n",
            "Training Step: 49688  | time: 0.076s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9998 -- iter: 112/405\n",
            "Training Step: 49689  | time: 0.082s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9998 -- iter: 120/405\n",
            "Training Step: 49690  | time: 0.087s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9998 -- iter: 128/405\n",
            "Training Step: 49691  | time: 0.093s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 136/405\n",
            "Training Step: 49692  | time: 0.102s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 144/405\n",
            "Training Step: 49693  | time: 0.110s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 152/405\n",
            "Training Step: 49694  | time: 0.115s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 160/405\n",
            "Training Step: 49695  | time: 0.122s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 168/405\n",
            "Training Step: 49696  | time: 0.126s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 176/405\n",
            "Training Step: 49697  | time: 0.132s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 184/405\n",
            "Training Step: 49698  | time: 0.137s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 192/405\n",
            "Training Step: 49699  | time: 0.142s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 200/405\n",
            "Training Step: 49700  | time: 0.147s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 208/405\n",
            "Training Step: 49701  | time: 0.150s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9999 -- iter: 216/405\n",
            "Training Step: 49702  | time: 0.157s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 224/405\n",
            "Training Step: 49703  | time: 0.164s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 232/405\n",
            "Training Step: 49704  | time: 0.170s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 240/405\n",
            "Training Step: 49705  | time: 0.176s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 248/405\n",
            "Training Step: 49706  | time: 0.181s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 256/405\n",
            "Training Step: 49707  | time: 0.190s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 264/405\n",
            "Training Step: 49708  | time: 0.198s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 272/405\n",
            "Training Step: 49709  | time: 0.205s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 280/405\n",
            "Training Step: 49710  | time: 0.209s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 288/405\n",
            "Training Step: 49711  | time: 0.214s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 296/405\n",
            "Training Step: 49712  | time: 0.219s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 304/405\n",
            "Training Step: 49713  | time: 0.224s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 312/405\n",
            "Training Step: 49714  | time: 0.227s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 320/405\n",
            "Training Step: 49715  | time: 0.232s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 328/405\n",
            "Training Step: 49716  | time: 0.237s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 336/405\n",
            "Training Step: 49717  | time: 0.241s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 344/405\n",
            "Training Step: 49718  | time: 0.247s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 1.0000 -- iter: 352/405\n",
            "Training Step: 49719  | time: 0.253s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9875 -- iter: 360/405\n",
            "Training Step: 49720  | time: 0.258s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9887 -- iter: 368/405\n",
            "Training Step: 49721  | time: 0.264s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9899 -- iter: 376/405\n",
            "Training Step: 49722  | time: 0.268s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9909 -- iter: 384/405\n",
            "Training Step: 49723  | time: 0.273s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9918 -- iter: 392/405\n",
            "Training Step: 49724  | time: 0.280s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9926 -- iter: 400/405\n",
            "Training Step: 49725  | time: 0.285s\n",
            "| Adam | epoch: 975 | loss: 0.00000 - acc: 0.9934 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49900  | time: 0.125s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9701 -- iter: 176/405\n",
            "Training Step: 49901  | time: 0.132s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9731 -- iter: 184/405\n",
            "Training Step: 49902  | time: 0.137s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9758 -- iter: 192/405\n",
            "Training Step: 49903  | time: 0.143s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9782 -- iter: 200/405\n",
            "Training Step: 49904  | time: 0.150s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9804 -- iter: 208/405\n",
            "Training Step: 49905  | time: 0.156s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9824 -- iter: 216/405\n",
            "Training Step: 49906  | time: 0.164s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9841 -- iter: 224/405\n",
            "Training Step: 49907  | time: 0.171s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9857 -- iter: 232/405\n",
            "Training Step: 49908  | time: 0.179s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9871 -- iter: 240/405\n",
            "Training Step: 49909  | time: 0.184s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9884 -- iter: 248/405\n",
            "Training Step: 49910  | time: 0.190s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9896 -- iter: 256/405\n",
            "Training Step: 49911  | time: 0.195s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9906 -- iter: 264/405\n",
            "Training Step: 49912  | time: 0.200s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9916 -- iter: 272/405\n",
            "Training Step: 49913  | time: 0.206s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9924 -- iter: 280/405\n",
            "Training Step: 49914  | time: 0.210s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9932 -- iter: 288/405\n",
            "Training Step: 49915  | time: 0.218s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9938 -- iter: 296/405\n",
            "Training Step: 49916  | time: 0.224s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9945 -- iter: 304/405\n",
            "Training Step: 49917  | time: 0.230s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9950 -- iter: 312/405\n",
            "Training Step: 49918  | time: 0.235s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9955 -- iter: 320/405\n",
            "Training Step: 49919  | time: 0.240s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9960 -- iter: 328/405\n",
            "Training Step: 49920  | time: 0.249s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9964 -- iter: 336/405\n",
            "Training Step: 49921  | time: 0.253s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9967 -- iter: 344/405\n",
            "Training Step: 49922  | time: 0.258s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9971 -- iter: 352/405\n",
            "Training Step: 49923  | time: 0.263s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9974 -- iter: 360/405\n",
            "Training Step: 49924  | time: 0.267s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9976 -- iter: 368/405\n",
            "Training Step: 49925  | time: 0.274s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9979 -- iter: 376/405\n",
            "Training Step: 49926  | time: 0.278s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9981 -- iter: 384/405\n",
            "Training Step: 49927  | time: 0.286s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9983 -- iter: 392/405\n",
            "Training Step: 49928  | time: 0.290s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9984 -- iter: 400/405\n",
            "Training Step: 49929  | time: 0.295s\n",
            "| Adam | epoch: 979 | loss: 0.00000 - acc: 0.9986 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49930  | time: 0.005s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9987 -- iter: 008/405\n",
            "Training Step: 49931  | time: 0.010s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9989 -- iter: 016/405\n",
            "Training Step: 49932  | time: 0.016s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9865 -- iter: 024/405\n",
            "Training Step: 49933  | time: 0.020s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9878 -- iter: 032/405\n",
            "Training Step: 49934  | time: 0.025s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9890 -- iter: 040/405\n",
            "Training Step: 49935  | time: 0.033s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9901 -- iter: 048/405\n",
            "Training Step: 49936  | time: 0.040s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9911 -- iter: 056/405\n",
            "Training Step: 49937  | time: 0.044s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9920 -- iter: 064/405\n",
            "Training Step: 49938  | time: 0.051s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9928 -- iter: 072/405\n",
            "Training Step: 49939  | time: 0.057s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9935 -- iter: 080/405\n",
            "Training Step: 49940  | time: 0.062s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9942 -- iter: 088/405\n",
            "Training Step: 49941  | time: 0.068s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9948 -- iter: 096/405\n",
            "Training Step: 49942  | time: 0.074s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9953 -- iter: 104/405\n",
            "Training Step: 49943  | time: 0.081s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9958 -- iter: 112/405\n",
            "Training Step: 49944  | time: 0.087s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9962 -- iter: 120/405\n",
            "Training Step: 49945  | time: 0.092s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9966 -- iter: 128/405\n",
            "Training Step: 49946  | time: 0.099s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9969 -- iter: 136/405\n",
            "Training Step: 49947  | time: 0.108s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9972 -- iter: 144/405\n",
            "Training Step: 49948  | time: 0.114s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9975 -- iter: 152/405\n",
            "Training Step: 49949  | time: 0.120s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9977 -- iter: 160/405\n",
            "Training Step: 49950  | time: 0.125s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9980 -- iter: 168/405\n",
            "Training Step: 49951  | time: 0.131s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9982 -- iter: 176/405\n",
            "Training Step: 49952  | time: 0.136s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9984 -- iter: 184/405\n",
            "Training Step: 49953  | time: 0.142s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9985 -- iter: 192/405\n",
            "Training Step: 49954  | time: 0.147s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9987 -- iter: 200/405\n",
            "Training Step: 49955  | time: 0.154s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9988 -- iter: 208/405\n",
            "Training Step: 49956  | time: 0.160s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9989 -- iter: 216/405\n",
            "Training Step: 49957  | time: 0.166s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9990 -- iter: 224/405\n",
            "Training Step: 49958  | time: 0.174s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9992 -- iter: 232/405\n",
            "Training Step: 49959  | time: 0.183s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9992 -- iter: 240/405\n",
            "Training Step: 49960  | time: 0.191s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9993 -- iter: 248/405\n",
            "Training Step: 49961  | time: 0.199s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9994 -- iter: 256/405\n",
            "Training Step: 49962  | time: 0.205s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9994 -- iter: 264/405\n",
            "Training Step: 49963  | time: 0.213s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9995 -- iter: 272/405\n",
            "Training Step: 49964  | time: 0.219s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9870 -- iter: 280/405\n",
            "Training Step: 49965  | time: 0.226s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9883 -- iter: 288/405\n",
            "Training Step: 49966  | time: 0.234s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9895 -- iter: 296/405\n",
            "Training Step: 49967  | time: 0.242s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9905 -- iter: 304/405\n",
            "Training Step: 49968  | time: 0.249s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9915 -- iter: 312/405\n",
            "Training Step: 49969  | time: 0.256s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9923 -- iter: 320/405\n",
            "Training Step: 49970  | time: 0.264s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9931 -- iter: 328/405\n",
            "Training Step: 49971  | time: 0.272s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9938 -- iter: 336/405\n",
            "Training Step: 49972  | time: 0.280s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9950 -- iter: 344/405\n",
            "Training Step: 49973  | time: 0.289s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9950 -- iter: 352/405\n",
            "Training Step: 49974  | time: 0.298s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9959 -- iter: 360/405\n",
            "Training Step: 49975  | time: 0.306s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9959 -- iter: 368/405\n",
            "Training Step: 49976  | time: 0.314s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9967 -- iter: 376/405\n",
            "Training Step: 49977  | time: 0.321s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9967 -- iter: 384/405\n",
            "Training Step: 49978  | time: 0.328s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9973 -- iter: 392/405\n",
            "Training Step: 49979  | time: 0.336s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9976 -- iter: 400/405\n",
            "Training Step: 49980  | time: 0.342s\n",
            "| Adam | epoch: 980 | loss: 0.00000 - acc: 0.9976 -- iter: 405/405\n",
            "--\n",
            "Training Step: 49981  | time: 0.011s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9978 -- iter: 008/405\n",
            "Training Step: 49982  | time: 0.018s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9981 -- iter: 016/405\n",
            "Training Step: 49983  | time: 0.025s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9982 -- iter: 024/405\n",
            "Training Step: 49984  | time: 0.032s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9984 -- iter: 032/405\n",
            "Training Step: 49985  | time: 0.038s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9986 -- iter: 040/405\n",
            "Training Step: 49986  | time: 0.044s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9987 -- iter: 048/405\n",
            "Training Step: 49987  | time: 0.051s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9989 -- iter: 056/405\n",
            "Training Step: 49988  | time: 0.059s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9990 -- iter: 064/405\n",
            "Training Step: 49989  | time: 0.066s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9992 -- iter: 072/405\n",
            "Training Step: 49990  | time: 0.073s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9992 -- iter: 080/405\n",
            "Training Step: 49991  | time: 0.078s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9992 -- iter: 088/405\n",
            "Training Step: 49992  | time: 0.086s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9993 -- iter: 096/405\n",
            "Training Step: 49993  | time: 0.097s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9994 -- iter: 104/405\n",
            "Training Step: 49994  | time: 0.107s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9995 -- iter: 112/405\n",
            "Training Step: 49995  | time: 0.118s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9995 -- iter: 120/405\n",
            "Training Step: 49996  | time: 0.125s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9996 -- iter: 128/405\n",
            "Training Step: 49997  | time: 0.134s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9996 -- iter: 136/405\n",
            "Training Step: 49998  | time: 0.143s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9997 -- iter: 144/405\n",
            "Training Step: 49999  | time: 0.150s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9997 -- iter: 152/405\n",
            "Training Step: 50000  | time: 0.159s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9997 -- iter: 160/405\n",
            "Training Step: 50001  | time: 0.166s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9997 -- iter: 168/405\n",
            "Training Step: 50002  | time: 0.173s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9998 -- iter: 176/405\n",
            "Training Step: 50003  | time: 0.184s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9998 -- iter: 184/405\n",
            "Training Step: 50004  | time: 0.191s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9998 -- iter: 192/405\n",
            "Training Step: 50005  | time: 0.200s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9998 -- iter: 200/405\n",
            "Training Step: 50006  | time: 0.207s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 208/405\n",
            "Training Step: 50007  | time: 0.214s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 216/405\n",
            "Training Step: 50008  | time: 0.225s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 224/405\n",
            "Training Step: 50009  | time: 0.231s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 232/405\n",
            "Training Step: 50010  | time: 0.240s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 240/405\n",
            "Training Step: 50011  | time: 0.246s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 248/405\n",
            "Training Step: 50012  | time: 0.252s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 256/405\n",
            "Training Step: 50013  | time: 0.260s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 264/405\n",
            "Training Step: 50014  | time: 0.266s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 272/405\n",
            "Training Step: 50015  | time: 0.273s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 280/405\n",
            "Training Step: 50016  | time: 0.281s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9999 -- iter: 288/405\n",
            "Training Step: 50017  | time: 0.286s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 296/405\n",
            "Training Step: 50018  | time: 0.294s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 304/405\n",
            "Training Step: 50019  | time: 0.300s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 312/405\n",
            "Training Step: 50020  | time: 0.306s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 320/405\n",
            "Training Step: 50021  | time: 0.315s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 328/405\n",
            "Training Step: 50022  | time: 0.320s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 336/405\n",
            "Training Step: 50023  | time: 0.324s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 344/405\n",
            "Training Step: 50024  | time: 0.330s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 352/405\n",
            "Training Step: 50025  | time: 0.336s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 360/405\n",
            "Training Step: 50026  | time: 0.342s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 1.0000 -- iter: 368/405\n",
            "Training Step: 50027  | time: 0.348s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9887 -- iter: 376/405\n",
            "Training Step: 50028  | time: 0.356s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9887 -- iter: 384/405\n",
            "Training Step: 50029  | time: 0.364s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9899 -- iter: 392/405\n",
            "Training Step: 50030  | time: 0.374s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9918 -- iter: 400/405\n",
            "Training Step: 50031  | time: 0.380s\n",
            "| Adam | epoch: 981 | loss: 0.00000 - acc: 0.9926 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50032  | time: 0.005s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9926 -- iter: 008/405\n",
            "Training Step: 50033  | time: 0.013s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9933 -- iter: 016/405\n",
            "Training Step: 50034  | time: 0.018s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9946 -- iter: 024/405\n",
            "Training Step: 50035  | time: 0.024s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9946 -- iter: 032/405\n",
            "Training Step: 50036  | time: 0.031s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9952 -- iter: 040/405\n",
            "Training Step: 50037  | time: 0.037s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9956 -- iter: 048/405\n",
            "Training Step: 50038  | time: 0.045s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9961 -- iter: 056/405\n",
            "Training Step: 50039  | time: 0.050s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9965 -- iter: 064/405\n",
            "Training Step: 50040  | time: 0.055s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9968 -- iter: 072/405\n",
            "Training Step: 50041  | time: 0.062s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9971 -- iter: 080/405\n",
            "Training Step: 50042  | time: 0.068s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9974 -- iter: 088/405\n",
            "Training Step: 50043  | time: 0.072s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9977 -- iter: 096/405\n",
            "Training Step: 50044  | time: 0.078s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.8979 -- iter: 104/405\n",
            "Training Step: 50045  | time: 0.083s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9081 -- iter: 112/405\n",
            "Training Step: 50046  | time: 0.088s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9173 -- iter: 120/405\n",
            "Training Step: 50047  | time: 0.094s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9256 -- iter: 128/405\n",
            "Training Step: 50048  | time: 0.101s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9330 -- iter: 136/405\n",
            "Training Step: 50049  | time: 0.109s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9397 -- iter: 144/405\n",
            "Training Step: 50050  | time: 0.116s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9457 -- iter: 152/405\n",
            "Training Step: 50051  | time: 0.122s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9512 -- iter: 160/405\n",
            "Training Step: 50052  | time: 0.128s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9561 -- iter: 168/405\n",
            "Training Step: 50053  | time: 0.133s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9604 -- iter: 176/405\n",
            "Training Step: 50054  | time: 0.140s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9644 -- iter: 184/405\n",
            "Training Step: 50055  | time: 0.148s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9680 -- iter: 192/405\n",
            "Training Step: 50056  | time: 0.152s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9712 -- iter: 200/405\n",
            "Training Step: 50057  | time: 0.158s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9741 -- iter: 208/405\n",
            "Training Step: 50058  | time: 0.162s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9766 -- iter: 216/405\n",
            "Training Step: 50059  | time: 0.168s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9790 -- iter: 224/405\n",
            "Training Step: 50060  | time: 0.172s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9811 -- iter: 232/405\n",
            "Training Step: 50061  | time: 0.177s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9830 -- iter: 240/405\n",
            "Training Step: 50062  | time: 0.182s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9847 -- iter: 248/405\n",
            "Training Step: 50063  | time: 0.188s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9862 -- iter: 256/405\n",
            "Training Step: 50064  | time: 0.193s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9876 -- iter: 264/405\n",
            "Training Step: 50065  | time: 0.200s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9888 -- iter: 272/405\n",
            "Training Step: 50066  | time: 0.208s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9899 -- iter: 280/405\n",
            "Training Step: 50067  | time: 0.214s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9910 -- iter: 288/405\n",
            "Training Step: 50068  | time: 0.220s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9919 -- iter: 296/405\n",
            "Training Step: 50069  | time: 0.224s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9927 -- iter: 304/405\n",
            "Training Step: 50070  | time: 0.230s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9934 -- iter: 312/405\n",
            "Training Step: 50071  | time: 0.234s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9941 -- iter: 320/405\n",
            "Training Step: 50072  | time: 0.241s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9947 -- iter: 328/405\n",
            "Training Step: 50073  | time: 0.244s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9952 -- iter: 336/405\n",
            "Training Step: 50074  | time: 0.250s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9957 -- iter: 344/405\n",
            "Training Step: 50075  | time: 0.255s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9961 -- iter: 352/405\n",
            "Training Step: 50076  | time: 0.260s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9965 -- iter: 360/405\n",
            "Training Step: 50077  | time: 0.263s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9968 -- iter: 368/405\n",
            "Training Step: 50078  | time: 0.269s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9972 -- iter: 376/405\n",
            "Training Step: 50079  | time: 0.275s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9974 -- iter: 384/405\n",
            "Training Step: 50080  | time: 0.279s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9977 -- iter: 392/405\n",
            "Training Step: 50081  | time: 0.285s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9979 -- iter: 400/405\n",
            "Training Step: 50082  | time: 0.289s\n",
            "| Adam | epoch: 982 | loss: 0.00000 - acc: 0.9856 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50083  | time: 0.007s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9871 -- iter: 008/405\n",
            "Training Step: 50084  | time: 0.012s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9884 -- iter: 016/405\n",
            "Training Step: 50085  | time: 0.019s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9895 -- iter: 024/405\n",
            "Training Step: 50086  | time: 0.024s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9906 -- iter: 032/405\n",
            "Training Step: 50087  | time: 0.029s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9915 -- iter: 040/405\n",
            "Training Step: 50088  | time: 0.033s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9924 -- iter: 048/405\n",
            "Training Step: 50089  | time: 0.038s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9931 -- iter: 056/405\n",
            "Training Step: 50090  | time: 0.043s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9938 -- iter: 064/405\n",
            "Training Step: 50091  | time: 0.048s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9944 -- iter: 072/405\n",
            "Training Step: 50092  | time: 0.052s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9950 -- iter: 080/405\n",
            "Training Step: 50093  | time: 0.057s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9955 -- iter: 088/405\n",
            "Training Step: 50094  | time: 0.062s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9959 -- iter: 096/405\n",
            "Training Step: 50095  | time: 0.068s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9838 -- iter: 104/405\n",
            "Training Step: 50096  | time: 0.073s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9855 -- iter: 112/405\n",
            "Training Step: 50097  | time: 0.079s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9869 -- iter: 120/405\n",
            "Training Step: 50098  | time: 0.082s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9882 -- iter: 128/405\n",
            "Training Step: 50099  | time: 0.089s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9894 -- iter: 136/405\n",
            "Training Step: 50100  | time: 0.095s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9905 -- iter: 144/405\n",
            "Training Step: 50101  | time: 0.099s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9914 -- iter: 152/405\n",
            "Training Step: 50102  | time: 0.104s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9923 -- iter: 160/405\n",
            "Training Step: 50103  | time: 0.109s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9930 -- iter: 168/405\n",
            "Training Step: 50104  | time: 0.113s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9937 -- iter: 176/405\n",
            "Training Step: 50105  | time: 0.117s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9944 -- iter: 184/405\n",
            "Training Step: 50106  | time: 0.122s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9949 -- iter: 192/405\n",
            "Training Step: 50107  | time: 0.128s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9954 -- iter: 200/405\n",
            "Training Step: 50108  | time: 0.134s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9959 -- iter: 208/405\n",
            "Training Step: 50109  | time: 0.141s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9963 -- iter: 216/405\n",
            "Training Step: 50110  | time: 0.147s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9967 -- iter: 224/405\n",
            "Training Step: 50111  | time: 0.154s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9970 -- iter: 232/405\n",
            "Training Step: 50112  | time: 0.160s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9973 -- iter: 240/405\n",
            "Training Step: 50113  | time: 0.166s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9976 -- iter: 248/405\n",
            "Training Step: 50114  | time: 0.171s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9978 -- iter: 256/405\n",
            "Training Step: 50115  | time: 0.179s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9980 -- iter: 264/405\n",
            "Training Step: 50116  | time: 0.185s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9982 -- iter: 272/405\n",
            "Training Step: 50117  | time: 0.190s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9984 -- iter: 280/405\n",
            "Training Step: 50118  | time: 0.195s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9986 -- iter: 288/405\n",
            "Training Step: 50119  | time: 0.202s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9987 -- iter: 296/405\n",
            "Training Step: 50120  | time: 0.209s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9988 -- iter: 304/405\n",
            "Training Step: 50121  | time: 0.216s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9990 -- iter: 312/405\n",
            "Training Step: 50122  | time: 0.223s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9991 -- iter: 320/405\n",
            "Training Step: 50123  | time: 0.229s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9992 -- iter: 328/405\n",
            "Training Step: 50124  | time: 0.235s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9992 -- iter: 336/405\n",
            "Training Step: 50125  | time: 0.243s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9993 -- iter: 344/405\n",
            "Training Step: 50126  | time: 0.251s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9994 -- iter: 352/405\n",
            "Training Step: 50127  | time: 0.258s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9994 -- iter: 360/405\n",
            "Training Step: 50128  | time: 0.263s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9995 -- iter: 368/405\n",
            "Training Step: 50129  | time: 0.267s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9996 -- iter: 376/405\n",
            "Training Step: 50130  | time: 0.272s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9996 -- iter: 384/405\n",
            "Training Step: 50131  | time: 0.278s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9996 -- iter: 392/405\n",
            "Training Step: 50132  | time: 0.283s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9997 -- iter: 400/405\n",
            "Training Step: 50133  | time: 0.290s\n",
            "| Adam | epoch: 983 | loss: 0.00000 - acc: 0.9997 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50134  | time: 0.006s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9997 -- iter: 008/405\n",
            "Training Step: 50135  | time: 0.011s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9998 -- iter: 016/405\n",
            "Training Step: 50136  | time: 0.019s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9998 -- iter: 024/405\n",
            "Training Step: 50137  | time: 0.026s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9998 -- iter: 032/405\n",
            "Training Step: 50138  | time: 0.031s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9873 -- iter: 040/405\n",
            "Training Step: 50139  | time: 0.036s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9886 -- iter: 048/405\n",
            "Training Step: 50140  | time: 0.043s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9897 -- iter: 056/405\n",
            "Training Step: 50141  | time: 0.048s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9908 -- iter: 064/405\n",
            "Training Step: 50142  | time: 0.052s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9917 -- iter: 072/405\n",
            "Training Step: 50143  | time: 0.058s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9925 -- iter: 080/405\n",
            "Training Step: 50144  | time: 0.063s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9933 -- iter: 088/405\n",
            "Training Step: 50145  | time: 0.069s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9939 -- iter: 096/405\n",
            "Training Step: 50146  | time: 0.075s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9945 -- iter: 104/405\n",
            "Training Step: 50147  | time: 0.080s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9951 -- iter: 112/405\n",
            "Training Step: 50148  | time: 0.086s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9956 -- iter: 120/405\n",
            "Training Step: 50149  | time: 0.092s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9960 -- iter: 128/405\n",
            "Training Step: 50150  | time: 0.099s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9964 -- iter: 136/405\n",
            "Training Step: 50151  | time: 0.104s\n",
            "| Adam | epoch: 984 | loss: 0.00000 - acc: 0.9968 -- iter: 144/405\n",
            "Training Step: 50203  | time: 0.121s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9981 -- iter: 152/405\n",
            "Training Step: 50204  | time: 0.127s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9983 -- iter: 160/405\n",
            "Training Step: 50205  | time: 0.134s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9985 -- iter: 168/405\n",
            "Training Step: 50206  | time: 0.144s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9986 -- iter: 176/405\n",
            "Training Step: 50207  | time: 0.152s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9988 -- iter: 184/405\n",
            "Training Step: 50208  | time: 0.161s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9989 -- iter: 192/405\n",
            "Training Step: 50209  | time: 0.168s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9990 -- iter: 200/405\n",
            "Training Step: 50210  | time: 0.177s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9991 -- iter: 208/405\n",
            "Training Step: 50211  | time: 0.184s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9992 -- iter: 216/405\n",
            "Training Step: 50212  | time: 0.191s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9993 -- iter: 224/405\n",
            "Training Step: 50213  | time: 0.196s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9993 -- iter: 232/405\n",
            "Training Step: 50214  | time: 0.203s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9994 -- iter: 240/405\n",
            "Training Step: 50215  | time: 0.209s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9995 -- iter: 248/405\n",
            "Training Step: 50216  | time: 0.216s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9995 -- iter: 256/405\n",
            "Training Step: 50217  | time: 0.222s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9996 -- iter: 264/405\n",
            "Training Step: 50218  | time: 0.227s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9996 -- iter: 272/405\n",
            "Training Step: 50219  | time: 0.233s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9996 -- iter: 280/405\n",
            "Training Step: 50220  | time: 0.239s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9997 -- iter: 288/405\n",
            "Training Step: 50221  | time: 0.247s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9997 -- iter: 296/405\n",
            "Training Step: 50222  | time: 0.253s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9997 -- iter: 304/405\n",
            "Training Step: 50223  | time: 0.260s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9873 -- iter: 312/405\n",
            "Training Step: 50224  | time: 0.265s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9885 -- iter: 320/405\n",
            "Training Step: 50225  | time: 0.277s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9897 -- iter: 328/405\n",
            "Training Step: 50226  | time: 0.284s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9907 -- iter: 336/405\n",
            "Training Step: 50227  | time: 0.292s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9925 -- iter: 344/405\n",
            "Training Step: 50228  | time: 0.299s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9932 -- iter: 352/405\n",
            "Training Step: 50229  | time: 0.305s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9939 -- iter: 360/405\n",
            "Training Step: 50230  | time: 0.313s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9939 -- iter: 368/405\n",
            "Training Step: 50231  | time: 0.319s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9945 -- iter: 376/405\n",
            "Training Step: 50232  | time: 0.325s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9951 -- iter: 384/405\n",
            "Training Step: 50233  | time: 0.333s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9956 -- iter: 392/405\n",
            "Training Step: 50234  | time: 0.339s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9960 -- iter: 400/405\n",
            "Training Step: 50235  | time: 0.345s\n",
            "| Adam | epoch: 985 | loss: 0.00000 - acc: 0.9964 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50236  | time: 0.005s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9971 -- iter: 008/405\n",
            "Training Step: 50237  | time: 0.011s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9974 -- iter: 016/405\n",
            "Training Step: 50238  | time: 0.019s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9974 -- iter: 024/405\n",
            "Training Step: 50239  | time: 0.028s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9976 -- iter: 032/405\n",
            "Training Step: 50240  | time: 0.035s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9981 -- iter: 040/405\n",
            "Training Step: 50241  | time: 0.041s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9981 -- iter: 048/405\n",
            "Training Step: 50242  | time: 0.046s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9983 -- iter: 056/405\n",
            "Training Step: 50243  | time: 0.051s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9985 -- iter: 064/405\n",
            "Training Step: 50244  | time: 0.055s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9986 -- iter: 072/405\n",
            "Training Step: 50245  | time: 0.060s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9987 -- iter: 080/405\n",
            "Training Step: 50246  | time: 0.065s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9864 -- iter: 088/405\n",
            "Training Step: 50247  | time: 0.071s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9877 -- iter: 096/405\n",
            "Training Step: 50248  | time: 0.076s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9890 -- iter: 104/405\n",
            "Training Step: 50249  | time: 0.081s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9901 -- iter: 112/405\n",
            "Training Step: 50250  | time: 0.086s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9911 -- iter: 120/405\n",
            "Training Step: 50251  | time: 0.091s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9920 -- iter: 128/405\n",
            "Training Step: 50252  | time: 0.099s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.8928 -- iter: 136/405\n",
            "Training Step: 50253  | time: 0.104s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9035 -- iter: 144/405\n",
            "Training Step: 50254  | time: 0.108s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9131 -- iter: 152/405\n",
            "Training Step: 50255  | time: 0.112s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9218 -- iter: 160/405\n",
            "Training Step: 50256  | time: 0.117s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9296 -- iter: 168/405\n",
            "Training Step: 50257  | time: 0.121s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9367 -- iter: 176/405\n",
            "Training Step: 50258  | time: 0.126s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9430 -- iter: 184/405\n",
            "Training Step: 50259  | time: 0.131s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9487 -- iter: 192/405\n",
            "Training Step: 50260  | time: 0.136s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9538 -- iter: 200/405\n",
            "Training Step: 50261  | time: 0.141s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9585 -- iter: 208/405\n",
            "Training Step: 50262  | time: 0.146s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9626 -- iter: 216/405\n",
            "Training Step: 50263  | time: 0.154s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9663 -- iter: 224/405\n",
            "Training Step: 50264  | time: 0.161s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9697 -- iter: 232/405\n",
            "Training Step: 50265  | time: 0.166s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9727 -- iter: 240/405\n",
            "Training Step: 50266  | time: 0.172s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9755 -- iter: 248/405\n",
            "Training Step: 50267  | time: 0.179s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9779 -- iter: 256/405\n",
            "Training Step: 50268  | time: 0.184s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9801 -- iter: 264/405\n",
            "Training Step: 50269  | time: 0.188s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9821 -- iter: 272/405\n",
            "Training Step: 50270  | time: 0.192s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9839 -- iter: 280/405\n",
            "Training Step: 50271  | time: 0.196s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9855 -- iter: 288/405\n",
            "Training Step: 50272  | time: 0.202s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9870 -- iter: 296/405\n",
            "Training Step: 50273  | time: 0.208s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9883 -- iter: 304/405\n",
            "Training Step: 50274  | time: 0.212s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9894 -- iter: 312/405\n",
            "Training Step: 50275  | time: 0.217s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9905 -- iter: 320/405\n",
            "Training Step: 50276  | time: 0.223s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9914 -- iter: 328/405\n",
            "Training Step: 50277  | time: 0.229s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9923 -- iter: 336/405\n",
            "Training Step: 50278  | time: 0.232s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9931 -- iter: 344/405\n",
            "Training Step: 50279  | time: 0.237s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9938 -- iter: 352/405\n",
            "Training Step: 50280  | time: 0.243s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9944 -- iter: 360/405\n",
            "Training Step: 50281  | time: 0.249s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9949 -- iter: 368/405\n",
            "Training Step: 50282  | time: 0.255s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9955 -- iter: 376/405\n",
            "Training Step: 50283  | time: 0.260s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9959 -- iter: 384/405\n",
            "Training Step: 50284  | time: 0.265s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9963 -- iter: 392/405\n",
            "Training Step: 50285  | time: 0.269s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9967 -- iter: 400/405\n",
            "Training Step: 50286  | time: 0.274s\n",
            "| Adam | epoch: 986 | loss: 0.00000 - acc: 0.9845 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50287  | time: 0.005s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9861 -- iter: 008/405\n",
            "Training Step: 50288  | time: 0.009s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9875 -- iter: 016/405\n",
            "Training Step: 50289  | time: 0.014s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9887 -- iter: 024/405\n",
            "Training Step: 50290  | time: 0.019s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9898 -- iter: 032/405\n",
            "Training Step: 50291  | time: 0.024s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9909 -- iter: 040/405\n",
            "Training Step: 50292  | time: 0.030s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9918 -- iter: 048/405\n",
            "Training Step: 50293  | time: 0.035s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9926 -- iter: 056/405\n",
            "Training Step: 50294  | time: 0.039s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9933 -- iter: 064/405\n",
            "Training Step: 50295  | time: 0.043s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9940 -- iter: 072/405\n",
            "Training Step: 50296  | time: 0.048s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9821 -- iter: 080/405\n",
            "Training Step: 50297  | time: 0.054s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9839 -- iter: 088/405\n",
            "Training Step: 50298  | time: 0.059s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9855 -- iter: 096/405\n",
            "Training Step: 50299  | time: 0.066s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9870 -- iter: 104/405\n",
            "Training Step: 50300  | time: 0.071s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9883 -- iter: 112/405\n",
            "Training Step: 50301  | time: 0.078s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9894 -- iter: 120/405\n",
            "Training Step: 50302  | time: 0.085s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9905 -- iter: 128/405\n",
            "Training Step: 50303  | time: 0.091s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9914 -- iter: 136/405\n",
            "Training Step: 50304  | time: 0.095s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9923 -- iter: 144/405\n",
            "Training Step: 50305  | time: 0.100s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9931 -- iter: 152/405\n",
            "Training Step: 50306  | time: 0.105s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9938 -- iter: 160/405\n",
            "Training Step: 50307  | time: 0.109s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9944 -- iter: 168/405\n",
            "Training Step: 50308  | time: 0.115s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9949 -- iter: 176/405\n",
            "Training Step: 50309  | time: 0.119s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9955 -- iter: 184/405\n",
            "Training Step: 50310  | time: 0.124s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9959 -- iter: 192/405\n",
            "Training Step: 50311  | time: 0.130s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9963 -- iter: 200/405\n",
            "Training Step: 50312  | time: 0.135s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9967 -- iter: 208/405\n",
            "Training Step: 50313  | time: 0.140s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9970 -- iter: 216/405\n",
            "Training Step: 50314  | time: 0.144s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9973 -- iter: 224/405\n",
            "Training Step: 50315  | time: 0.147s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9976 -- iter: 232/405\n",
            "Training Step: 50316  | time: 0.151s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9978 -- iter: 240/405\n",
            "Training Step: 50317  | time: 0.155s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9980 -- iter: 248/405\n",
            "Training Step: 50318  | time: 0.160s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9982 -- iter: 256/405\n",
            "Training Step: 50319  | time: 0.166s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9984 -- iter: 264/405\n",
            "Training Step: 50320  | time: 0.170s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9986 -- iter: 272/405\n",
            "Training Step: 50321  | time: 0.175s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9987 -- iter: 280/405\n",
            "Training Step: 50322  | time: 0.182s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9988 -- iter: 288/405\n",
            "Training Step: 50323  | time: 0.186s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9990 -- iter: 296/405\n",
            "Training Step: 50324  | time: 0.191s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9991 -- iter: 304/405\n",
            "Training Step: 50325  | time: 0.198s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9992 -- iter: 312/405\n",
            "Training Step: 50326  | time: 0.203s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9992 -- iter: 320/405\n",
            "Training Step: 50327  | time: 0.208s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9993 -- iter: 328/405\n",
            "Training Step: 50328  | time: 0.214s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9994 -- iter: 336/405\n",
            "Training Step: 50329  | time: 0.221s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9994 -- iter: 344/405\n",
            "Training Step: 50330  | time: 0.225s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9995 -- iter: 352/405\n",
            "Training Step: 50331  | time: 0.229s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9996 -- iter: 360/405\n",
            "Training Step: 50332  | time: 0.234s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9996 -- iter: 368/405\n",
            "Training Step: 50333  | time: 0.239s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9996 -- iter: 376/405\n",
            "Training Step: 50334  | time: 0.242s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9872 -- iter: 384/405\n",
            "Training Step: 50335  | time: 0.247s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9885 -- iter: 392/405\n",
            "Training Step: 50336  | time: 0.252s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9896 -- iter: 400/405\n",
            "Training Step: 50337  | time: 0.257s\n",
            "| Adam | epoch: 987 | loss: 0.00000 - acc: 0.9906 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50338  | time: 0.004s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9916 -- iter: 008/405\n",
            "Training Step: 50339  | time: 0.011s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9924 -- iter: 016/405\n",
            "Training Step: 50340  | time: 0.015s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9932 -- iter: 024/405\n",
            "Training Step: 50341  | time: 0.020s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9939 -- iter: 032/405\n",
            "Training Step: 50342  | time: 0.024s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9945 -- iter: 040/405\n",
            "Training Step: 50343  | time: 0.030s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9950 -- iter: 048/405\n",
            "Training Step: 50344  | time: 0.034s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9955 -- iter: 056/405\n",
            "Training Step: 50345  | time: 0.041s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9960 -- iter: 064/405\n",
            "Training Step: 50346  | time: 0.046s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9964 -- iter: 072/405\n",
            "Training Step: 50347  | time: 0.051s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9967 -- iter: 080/405\n",
            "Training Step: 50348  | time: 0.056s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9971 -- iter: 088/405\n",
            "Training Step: 50349  | time: 0.064s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9974 -- iter: 096/405\n",
            "Training Step: 50350  | time: 0.069s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9976 -- iter: 104/405\n",
            "Training Step: 50351  | time: 0.077s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9979 -- iter: 112/405\n",
            "Training Step: 50352  | time: 0.083s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9981 -- iter: 120/405\n",
            "Training Step: 50353  | time: 0.091s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9983 -- iter: 128/405\n",
            "Training Step: 50354  | time: 0.095s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9984 -- iter: 136/405\n",
            "Training Step: 50355  | time: 0.099s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9986 -- iter: 144/405\n",
            "Training Step: 50356  | time: 0.105s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9112 -- iter: 152/405\n",
            "Training Step: 50357  | time: 0.110s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9201 -- iter: 160/405\n",
            "Training Step: 50358  | time: 0.116s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9281 -- iter: 168/405\n",
            "Training Step: 50359  | time: 0.122s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9353 -- iter: 176/405\n",
            "Training Step: 50360  | time: 0.128s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9418 -- iter: 184/405\n",
            "Training Step: 50361  | time: 0.133s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9476 -- iter: 192/405\n",
            "Training Step: 50362  | time: 0.137s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9528 -- iter: 200/405\n",
            "Training Step: 50363  | time: 0.142s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9575 -- iter: 208/405\n",
            "Training Step: 50364  | time: 0.147s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9618 -- iter: 216/405\n",
            "Training Step: 50365  | time: 0.152s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9656 -- iter: 224/405\n",
            "Training Step: 50366  | time: 0.157s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9691 -- iter: 232/405\n",
            "Training Step: 50367  | time: 0.161s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9721 -- iter: 240/405\n",
            "Training Step: 50368  | time: 0.167s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9749 -- iter: 248/405\n",
            "Training Step: 50369  | time: 0.172s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9774 -- iter: 256/405\n",
            "Training Step: 50370  | time: 0.177s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9797 -- iter: 264/405\n",
            "Training Step: 50371  | time: 0.181s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9817 -- iter: 272/405\n",
            "Training Step: 50372  | time: 0.185s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9836 -- iter: 280/405\n",
            "Training Step: 50373  | time: 0.188s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9852 -- iter: 288/405\n",
            "Training Step: 50374  | time: 0.194s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9867 -- iter: 296/405\n",
            "Training Step: 50375  | time: 0.199s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9880 -- iter: 304/405\n",
            "Training Step: 50376  | time: 0.202s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9892 -- iter: 312/405\n",
            "Training Step: 50377  | time: 0.206s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9903 -- iter: 320/405\n",
            "Training Step: 50378  | time: 0.212s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9913 -- iter: 328/405\n",
            "Training Step: 50379  | time: 0.217s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9921 -- iter: 336/405\n",
            "Training Step: 50380  | time: 0.222s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9929 -- iter: 344/405\n",
            "Training Step: 50381  | time: 0.227s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9936 -- iter: 352/405\n",
            "Training Step: 50382  | time: 0.232s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9943 -- iter: 360/405\n",
            "Training Step: 50383  | time: 0.238s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9948 -- iter: 368/405\n",
            "Training Step: 50384  | time: 0.244s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9954 -- iter: 376/405\n",
            "Training Step: 50385  | time: 0.249s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9958 -- iter: 384/405\n",
            "Training Step: 50386  | time: 0.255s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9837 -- iter: 392/405\n",
            "Training Step: 50387  | time: 0.259s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9854 -- iter: 400/405\n",
            "Training Step: 50388  | time: 0.264s\n",
            "| Adam | epoch: 988 | loss: 0.00000 - acc: 0.9868 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50389  | time: 0.003s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9881 -- iter: 008/405\n",
            "Training Step: 50390  | time: 0.008s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9893 -- iter: 016/405\n",
            "Training Step: 50391  | time: 0.014s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9904 -- iter: 024/405\n",
            "Training Step: 50392  | time: 0.019s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9914 -- iter: 032/405\n",
            "Training Step: 50393  | time: 0.025s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9922 -- iter: 040/405\n",
            "Training Step: 50394  | time: 0.030s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9930 -- iter: 048/405\n",
            "Training Step: 50395  | time: 0.035s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9937 -- iter: 056/405\n",
            "Training Step: 50396  | time: 0.041s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9943 -- iter: 064/405\n",
            "Training Step: 50397  | time: 0.046s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9949 -- iter: 072/405\n",
            "Training Step: 50398  | time: 0.050s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9954 -- iter: 080/405\n",
            "Training Step: 50399  | time: 0.054s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9959 -- iter: 088/405\n",
            "Training Step: 50400  | time: 0.060s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9963 -- iter: 096/405\n",
            "Training Step: 50401  | time: 0.065s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9967 -- iter: 104/405\n",
            "Training Step: 50402  | time: 0.072s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9970 -- iter: 112/405\n",
            "Training Step: 50403  | time: 0.078s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9973 -- iter: 120/405\n",
            "Training Step: 50404  | time: 0.084s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9976 -- iter: 128/405\n",
            "Training Step: 50405  | time: 0.088s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9978 -- iter: 136/405\n",
            "Training Step: 50406  | time: 0.092s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9980 -- iter: 144/405\n",
            "Training Step: 50407  | time: 0.100s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9982 -- iter: 152/405\n",
            "Training Step: 50408  | time: 0.105s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9984 -- iter: 160/405\n",
            "Training Step: 50409  | time: 0.110s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9986 -- iter: 168/405\n",
            "Training Step: 50410  | time: 0.115s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9987 -- iter: 176/405\n",
            "Training Step: 50411  | time: 0.119s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9988 -- iter: 184/405\n",
            "Training Step: 50412  | time: 0.123s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9989 -- iter: 192/405\n",
            "Training Step: 50413  | time: 0.129s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9991 -- iter: 200/405\n",
            "Training Step: 50414  | time: 0.135s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9991 -- iter: 208/405\n",
            "Training Step: 50415  | time: 0.143s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9992 -- iter: 216/405\n",
            "Training Step: 50416  | time: 0.147s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9993 -- iter: 224/405\n",
            "Training Step: 50417  | time: 0.152s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9994 -- iter: 232/405\n",
            "Training Step: 50418  | time: 0.155s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9994 -- iter: 240/405\n",
            "Training Step: 50419  | time: 0.159s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9995 -- iter: 248/405\n",
            "Training Step: 50420  | time: 0.163s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9995 -- iter: 256/405\n",
            "Training Step: 50421  | time: 0.167s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9996 -- iter: 264/405\n",
            "Training Step: 50422  | time: 0.171s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9996 -- iter: 272/405\n",
            "Training Step: 50423  | time: 0.176s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9997 -- iter: 280/405\n",
            "Training Step: 50424  | time: 0.183s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9997 -- iter: 288/405\n",
            "Training Step: 50425  | time: 0.189s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9997 -- iter: 296/405\n",
            "Training Step: 50426  | time: 0.193s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9998 -- iter: 304/405\n",
            "Training Step: 50427  | time: 0.197s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9998 -- iter: 312/405\n",
            "Training Step: 50428  | time: 0.203s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9998 -- iter: 320/405\n",
            "Training Step: 50429  | time: 0.208s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9998 -- iter: 328/405\n",
            "Training Step: 50430  | time: 0.214s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9998 -- iter: 336/405\n",
            "Training Step: 50431  | time: 0.219s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 344/405\n",
            "Training Step: 50432  | time: 0.223s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 352/405\n",
            "Training Step: 50433  | time: 0.229s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 360/405\n",
            "Training Step: 50434  | time: 0.236s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 368/405\n",
            "Training Step: 50435  | time: 0.243s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 376/405\n",
            "Training Step: 50436  | time: 0.250s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 384/405\n",
            "Training Step: 50437  | time: 0.257s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 392/405\n",
            "Training Step: 50438  | time: 0.262s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 400/405\n",
            "Training Step: 50439  | time: 0.266s\n",
            "| Adam | epoch: 989 | loss: 0.00000 - acc: 0.9999 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50440  | time: 0.004s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 0.9999 -- iter: 008/405\n",
            "Training Step: 50441  | time: 0.008s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 016/405\n",
            "Training Step: 50442  | time: 0.012s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 024/405\n",
            "Training Step: 50443  | time: 0.017s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 032/405\n",
            "Training Step: 50444  | time: 0.024s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 040/405\n",
            "Training Step: 50445  | time: 0.030s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 048/405\n",
            "Training Step: 50446  | time: 0.036s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 056/405\n",
            "Training Step: 50447  | time: 0.041s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 064/405\n",
            "Training Step: 50448  | time: 0.045s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 072/405\n",
            "Training Step: 50449  | time: 0.052s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 080/405\n",
            "Training Step: 50450  | time: 0.060s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 088/405\n",
            "Training Step: 50451  | time: 0.064s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 096/405\n",
            "Training Step: 50452  | time: 0.069s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 1.0000 -- iter: 104/405\n",
            "Training Step: 50453  | time: 0.076s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 0.9875 -- iter: 112/405\n",
            "Training Step: 50454  | time: 0.081s\n",
            "| Adam | epoch: 990 | loss: 0.00000 - acc: 0.9887 -- iter: 120/405\n",
            "Training Step: 50519  | time: 0.158s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9510 -- iter: 232/405\n",
            "Training Step: 50520  | time: 0.163s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9559 -- iter: 240/405\n",
            "Training Step: 50521  | time: 0.170s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9603 -- iter: 248/405\n",
            "Training Step: 50522  | time: 0.175s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9643 -- iter: 256/405\n",
            "Training Step: 50523  | time: 0.180s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9679 -- iter: 264/405\n",
            "Training Step: 50524  | time: 0.187s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9586 -- iter: 272/405\n",
            "Training Step: 50525  | time: 0.192s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9627 -- iter: 280/405\n",
            "Training Step: 50526  | time: 0.199s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9664 -- iter: 288/405\n",
            "Training Step: 50527  | time: 0.204s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9698 -- iter: 296/405\n",
            "Training Step: 50528  | time: 0.209s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9728 -- iter: 304/405\n",
            "Training Step: 50529  | time: 0.215s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9755 -- iter: 312/405\n",
            "Training Step: 50530  | time: 0.220s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9780 -- iter: 320/405\n",
            "Training Step: 50531  | time: 0.226s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9802 -- iter: 328/405\n",
            "Training Step: 50532  | time: 0.231s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9822 -- iter: 336/405\n",
            "Training Step: 50533  | time: 0.236s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9840 -- iter: 344/405\n",
            "Training Step: 50534  | time: 0.243s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9856 -- iter: 352/405\n",
            "Training Step: 50535  | time: 0.249s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9870 -- iter: 360/405\n",
            "Training Step: 50536  | time: 0.254s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9883 -- iter: 368/405\n",
            "Training Step: 50537  | time: 0.260s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9895 -- iter: 376/405\n",
            "Training Step: 50538  | time: 0.265s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9905 -- iter: 384/405\n",
            "Training Step: 50539  | time: 0.270s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9915 -- iter: 392/405\n",
            "Training Step: 50540  | time: 0.274s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9923 -- iter: 400/405\n",
            "Training Step: 50541  | time: 0.279s\n",
            "| Adam | epoch: 991 | loss: 0.00000 - acc: 0.9931 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50542  | time: 0.005s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9938 -- iter: 008/405\n",
            "Training Step: 50543  | time: 0.010s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9944 -- iter: 016/405\n",
            "Training Step: 50544  | time: 0.015s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9950 -- iter: 024/405\n",
            "Training Step: 50545  | time: 0.021s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9955 -- iter: 032/405\n",
            "Training Step: 50546  | time: 0.025s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9959 -- iter: 040/405\n",
            "Training Step: 50547  | time: 0.031s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9963 -- iter: 048/405\n",
            "Training Step: 50548  | time: 0.036s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9967 -- iter: 056/405\n",
            "Training Step: 50549  | time: 0.041s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9970 -- iter: 064/405\n",
            "Training Step: 50550  | time: 0.047s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9973 -- iter: 072/405\n",
            "Training Step: 50551  | time: 0.052s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9976 -- iter: 080/405\n",
            "Training Step: 50552  | time: 0.057s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9978 -- iter: 088/405\n",
            "Training Step: 50553  | time: 0.062s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9980 -- iter: 096/405\n",
            "Training Step: 50554  | time: 0.067s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9982 -- iter: 104/405\n",
            "Training Step: 50555  | time: 0.072s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9984 -- iter: 112/405\n",
            "Training Step: 50556  | time: 0.077s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9986 -- iter: 120/405\n",
            "Training Step: 50557  | time: 0.083s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9987 -- iter: 128/405\n",
            "Training Step: 50558  | time: 0.088s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9988 -- iter: 136/405\n",
            "Training Step: 50559  | time: 0.092s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9990 -- iter: 144/405\n",
            "Training Step: 50560  | time: 0.097s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9991 -- iter: 152/405\n",
            "Training Step: 50561  | time: 0.102s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9992 -- iter: 160/405\n",
            "Training Step: 50562  | time: 0.107s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9992 -- iter: 168/405\n",
            "Training Step: 50563  | time: 0.113s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9993 -- iter: 176/405\n",
            "Training Step: 50564  | time: 0.119s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.8994 -- iter: 184/405\n",
            "Training Step: 50565  | time: 0.124s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9094 -- iter: 192/405\n",
            "Training Step: 50566  | time: 0.130s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9185 -- iter: 200/405\n",
            "Training Step: 50567  | time: 0.137s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9267 -- iter: 208/405\n",
            "Training Step: 50568  | time: 0.143s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9340 -- iter: 216/405\n",
            "Training Step: 50569  | time: 0.149s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9406 -- iter: 224/405\n",
            "Training Step: 50570  | time: 0.155s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9465 -- iter: 232/405\n",
            "Training Step: 50571  | time: 0.161s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9519 -- iter: 240/405\n",
            "Training Step: 50572  | time: 0.167s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9567 -- iter: 248/405\n",
            "Training Step: 50573  | time: 0.172s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9610 -- iter: 256/405\n",
            "Training Step: 50574  | time: 0.176s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9649 -- iter: 264/405\n",
            "Training Step: 50575  | time: 0.181s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9684 -- iter: 272/405\n",
            "Training Step: 50576  | time: 0.187s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9716 -- iter: 280/405\n",
            "Training Step: 50577  | time: 0.198s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9744 -- iter: 288/405\n",
            "Training Step: 50578  | time: 0.206s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9770 -- iter: 296/405\n",
            "Training Step: 50579  | time: 0.211s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9793 -- iter: 304/405\n",
            "Training Step: 50580  | time: 0.219s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9814 -- iter: 312/405\n",
            "Training Step: 50581  | time: 0.228s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9832 -- iter: 320/405\n",
            "Training Step: 50582  | time: 0.234s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9849 -- iter: 328/405\n",
            "Training Step: 50583  | time: 0.240s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9864 -- iter: 336/405\n",
            "Training Step: 50584  | time: 0.246s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9878 -- iter: 344/405\n",
            "Training Step: 50585  | time: 0.252s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9890 -- iter: 352/405\n",
            "Training Step: 50586  | time: 0.257s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9901 -- iter: 360/405\n",
            "Training Step: 50587  | time: 0.263s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9911 -- iter: 368/405\n",
            "Training Step: 50588  | time: 0.270s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9920 -- iter: 376/405\n",
            "Training Step: 50589  | time: 0.276s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9928 -- iter: 384/405\n",
            "Training Step: 50590  | time: 0.283s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9935 -- iter: 392/405\n",
            "Training Step: 50591  | time: 0.289s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9941 -- iter: 400/405\n",
            "Training Step: 50592  | time: 0.294s\n",
            "| Adam | epoch: 992 | loss: 0.00000 - acc: 0.9947 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50593  | time: 0.004s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9953 -- iter: 008/405\n",
            "Training Step: 50594  | time: 0.010s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9957 -- iter: 016/405\n",
            "Training Step: 50595  | time: 0.015s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9962 -- iter: 024/405\n",
            "Training Step: 50596  | time: 0.022s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9965 -- iter: 032/405\n",
            "Training Step: 50597  | time: 0.026s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9969 -- iter: 040/405\n",
            "Training Step: 50598  | time: 0.034s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9972 -- iter: 048/405\n",
            "Training Step: 50599  | time: 0.039s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9850 -- iter: 056/405\n",
            "Training Step: 50600  | time: 0.045s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9865 -- iter: 064/405\n",
            "Training Step: 50601  | time: 0.050s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9878 -- iter: 072/405\n",
            "Training Step: 50602  | time: 0.056s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9766 -- iter: 080/405\n",
            "Training Step: 50603  | time: 0.064s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9789 -- iter: 088/405\n",
            "Training Step: 50604  | time: 0.070s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9810 -- iter: 096/405\n",
            "Training Step: 50605  | time: 0.077s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9829 -- iter: 104/405\n",
            "Training Step: 50606  | time: 0.083s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9846 -- iter: 112/405\n",
            "Training Step: 50607  | time: 0.090s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9862 -- iter: 120/405\n",
            "Training Step: 50608  | time: 0.095s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9875 -- iter: 128/405\n",
            "Training Step: 50609  | time: 0.103s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9888 -- iter: 136/405\n",
            "Training Step: 50610  | time: 0.108s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9899 -- iter: 144/405\n",
            "Training Step: 50611  | time: 0.114s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9909 -- iter: 152/405\n",
            "Training Step: 50612  | time: 0.120s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9918 -- iter: 160/405\n",
            "Training Step: 50613  | time: 0.128s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9926 -- iter: 168/405\n",
            "Training Step: 50614  | time: 0.137s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9934 -- iter: 176/405\n",
            "Training Step: 50615  | time: 0.142s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9940 -- iter: 184/405\n",
            "Training Step: 50616  | time: 0.147s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9946 -- iter: 192/405\n",
            "Training Step: 50617  | time: 0.151s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9952 -- iter: 200/405\n",
            "Training Step: 50618  | time: 0.157s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9957 -- iter: 208/405\n",
            "Training Step: 50619  | time: 0.165s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9961 -- iter: 216/405\n",
            "Training Step: 50620  | time: 0.169s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9965 -- iter: 224/405\n",
            "Training Step: 50621  | time: 0.175s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9968 -- iter: 232/405\n",
            "Training Step: 50622  | time: 0.181s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9971 -- iter: 240/405\n",
            "Training Step: 50623  | time: 0.186s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9974 -- iter: 248/405\n",
            "Training Step: 50624  | time: 0.194s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9977 -- iter: 256/405\n",
            "Training Step: 50625  | time: 0.201s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9979 -- iter: 264/405\n",
            "Training Step: 50626  | time: 0.208s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9981 -- iter: 272/405\n",
            "Training Step: 50627  | time: 0.215s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9983 -- iter: 280/405\n",
            "Training Step: 50628  | time: 0.220s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9985 -- iter: 288/405\n",
            "Training Step: 50629  | time: 0.228s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9986 -- iter: 296/405\n",
            "Training Step: 50630  | time: 0.233s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9988 -- iter: 304/405\n",
            "Training Step: 50631  | time: 0.241s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9989 -- iter: 312/405\n",
            "Training Step: 50632  | time: 0.245s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9990 -- iter: 320/405\n",
            "Training Step: 50633  | time: 0.254s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9991 -- iter: 328/405\n",
            "Training Step: 50634  | time: 0.260s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9992 -- iter: 336/405\n",
            "Training Step: 50635  | time: 0.265s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9868 -- iter: 344/405\n",
            "Training Step: 50636  | time: 0.272s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9881 -- iter: 352/405\n",
            "Training Step: 50637  | time: 0.276s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9893 -- iter: 360/405\n",
            "Training Step: 50638  | time: 0.282s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9779 -- iter: 368/405\n",
            "Training Step: 50639  | time: 0.290s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9801 -- iter: 376/405\n",
            "Training Step: 50640  | time: 0.297s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9821 -- iter: 384/405\n",
            "Training Step: 50641  | time: 0.302s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9839 -- iter: 392/405\n",
            "Training Step: 50642  | time: 0.307s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9855 -- iter: 400/405\n",
            "Training Step: 50643  | time: 0.312s\n",
            "| Adam | epoch: 993 | loss: 0.00000 - acc: 0.9869 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50644  | time: 0.005s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9882 -- iter: 008/405\n",
            "Training Step: 50645  | time: 0.012s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9894 -- iter: 016/405\n",
            "Training Step: 50646  | time: 0.018s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9905 -- iter: 024/405\n",
            "Training Step: 50647  | time: 0.023s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9914 -- iter: 032/405\n",
            "Training Step: 50648  | time: 0.030s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9923 -- iter: 040/405\n",
            "Training Step: 50649  | time: 0.035s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9931 -- iter: 048/405\n",
            "Training Step: 50650  | time: 0.042s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9937 -- iter: 056/405\n",
            "Training Step: 50651  | time: 0.048s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9944 -- iter: 064/405\n",
            "Training Step: 50652  | time: 0.052s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9949 -- iter: 072/405\n",
            "Training Step: 50653  | time: 0.058s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9954 -- iter: 080/405\n",
            "Training Step: 50654  | time: 0.063s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9959 -- iter: 088/405\n",
            "Training Step: 50655  | time: 0.068s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9963 -- iter: 096/405\n",
            "Training Step: 50656  | time: 0.074s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9967 -- iter: 104/405\n",
            "Training Step: 50657  | time: 0.079s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9970 -- iter: 112/405\n",
            "Training Step: 50658  | time: 0.086s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9973 -- iter: 120/405\n",
            "Training Step: 50659  | time: 0.092s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9976 -- iter: 128/405\n",
            "Training Step: 50660  | time: 0.098s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9978 -- iter: 136/405\n",
            "Training Step: 50661  | time: 0.105s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9980 -- iter: 144/405\n",
            "Training Step: 50662  | time: 0.110s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9982 -- iter: 152/405\n",
            "Training Step: 50663  | time: 0.116s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9984 -- iter: 160/405\n",
            "Training Step: 50664  | time: 0.121s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9986 -- iter: 168/405\n",
            "Training Step: 50665  | time: 0.127s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9987 -- iter: 176/405\n",
            "Training Step: 50666  | time: 0.133s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9988 -- iter: 184/405\n",
            "Training Step: 50667  | time: 0.140s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9990 -- iter: 192/405\n",
            "Training Step: 50668  | time: 0.147s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9116 -- iter: 200/405\n",
            "Training Step: 50669  | time: 0.153s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9204 -- iter: 208/405\n",
            "Training Step: 50670  | time: 0.158s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9284 -- iter: 216/405\n",
            "Training Step: 50671  | time: 0.164s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9355 -- iter: 224/405\n",
            "Training Step: 50672  | time: 0.169s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9420 -- iter: 232/405\n",
            "Training Step: 50673  | time: 0.174s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9478 -- iter: 240/405\n",
            "Training Step: 50674  | time: 0.179s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9530 -- iter: 248/405\n",
            "Training Step: 50675  | time: 0.184s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9577 -- iter: 256/405\n",
            "Training Step: 50676  | time: 0.190s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9619 -- iter: 264/405\n",
            "Training Step: 50677  | time: 0.198s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9657 -- iter: 272/405\n",
            "Training Step: 50678  | time: 0.203s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9692 -- iter: 280/405\n",
            "Training Step: 50679  | time: 0.210s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9722 -- iter: 288/405\n",
            "Training Step: 50680  | time: 0.217s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9750 -- iter: 296/405\n",
            "Training Step: 50681  | time: 0.222s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9650 -- iter: 304/405\n",
            "Training Step: 50682  | time: 0.227s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9685 -- iter: 312/405\n",
            "Training Step: 50683  | time: 0.232s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9592 -- iter: 320/405\n",
            "Training Step: 50684  | time: 0.237s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9632 -- iter: 328/405\n",
            "Training Step: 50685  | time: 0.244s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9669 -- iter: 336/405\n",
            "Training Step: 50686  | time: 0.253s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9702 -- iter: 344/405\n",
            "Training Step: 50687  | time: 0.260s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9607 -- iter: 352/405\n",
            "Training Step: 50688  | time: 0.267s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9646 -- iter: 360/405\n",
            "Training Step: 50689  | time: 0.273s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9682 -- iter: 368/405\n",
            "Training Step: 50690  | time: 0.280s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9714 -- iter: 376/405\n",
            "Training Step: 50691  | time: 0.285s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9742 -- iter: 384/405\n",
            "Training Step: 50692  | time: 0.292s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9768 -- iter: 392/405\n",
            "Training Step: 50693  | time: 0.296s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9791 -- iter: 400/405\n",
            "Training Step: 50694  | time: 0.305s\n",
            "| Adam | epoch: 994 | loss: 0.00000 - acc: 0.9812 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50695  | time: 0.005s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9831 -- iter: 008/405\n",
            "Training Step: 50696  | time: 0.011s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9848 -- iter: 016/405\n",
            "Training Step: 50697  | time: 0.017s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9863 -- iter: 024/405\n",
            "Training Step: 50698  | time: 0.025s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9877 -- iter: 032/405\n",
            "Training Step: 50699  | time: 0.032s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9889 -- iter: 040/405\n",
            "Training Step: 50700  | time: 0.036s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9700 -- iter: 048/405\n",
            "Training Step: 50701  | time: 0.041s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9530 -- iter: 056/405\n",
            "Training Step: 50702  | time: 0.053s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9577 -- iter: 064/405\n",
            "Training Step: 50703  | time: 0.058s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9619 -- iter: 072/405\n",
            "Training Step: 50704  | time: 0.063s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9657 -- iter: 080/405\n",
            "Training Step: 50705  | time: 0.069s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9692 -- iter: 088/405\n",
            "Training Step: 50706  | time: 0.076s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9723 -- iter: 096/405\n",
            "Training Step: 50707  | time: 0.084s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9750 -- iter: 104/405\n",
            "Training Step: 50708  | time: 0.091s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9775 -- iter: 112/405\n",
            "Training Step: 50709  | time: 0.096s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9798 -- iter: 120/405\n",
            "Training Step: 50710  | time: 0.101s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9818 -- iter: 128/405\n",
            "Training Step: 50711  | time: 0.106s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9836 -- iter: 136/405\n",
            "Training Step: 50712  | time: 0.111s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9853 -- iter: 144/405\n",
            "Training Step: 50713  | time: 0.117s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9867 -- iter: 152/405\n",
            "Training Step: 50714  | time: 0.125s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9881 -- iter: 160/405\n",
            "Training Step: 50715  | time: 0.131s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9893 -- iter: 168/405\n",
            "Training Step: 50716  | time: 0.139s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9903 -- iter: 176/405\n",
            "Training Step: 50717  | time: 0.144s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9913 -- iter: 184/405\n",
            "Training Step: 50718  | time: 0.150s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9922 -- iter: 192/405\n",
            "Training Step: 50719  | time: 0.157s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9929 -- iter: 200/405\n",
            "Training Step: 50720  | time: 0.162s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9062 -- iter: 208/405\n",
            "Training Step: 50721  | time: 0.168s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9155 -- iter: 216/405\n",
            "Training Step: 50722  | time: 0.174s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9240 -- iter: 224/405\n",
            "Training Step: 50723  | time: 0.179s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9316 -- iter: 232/405\n",
            "Training Step: 50724  | time: 0.184s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9384 -- iter: 240/405\n",
            "Training Step: 50725  | time: 0.192s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9446 -- iter: 248/405\n",
            "Training Step: 50726  | time: 0.198s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9501 -- iter: 256/405\n",
            "Training Step: 50727  | time: 0.204s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9551 -- iter: 264/405\n",
            "Training Step: 50728  | time: 0.210s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9596 -- iter: 272/405\n",
            "Training Step: 50729  | time: 0.214s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9636 -- iter: 280/405\n",
            "Training Step: 50730  | time: 0.218s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9673 -- iter: 288/405\n",
            "Training Step: 50731  | time: 0.223s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9705 -- iter: 296/405\n",
            "Training Step: 50732  | time: 0.229s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9735 -- iter: 304/405\n",
            "Training Step: 50733  | time: 0.236s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9761 -- iter: 312/405\n",
            "Training Step: 50734  | time: 0.243s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9660 -- iter: 320/405\n",
            "Training Step: 50735  | time: 0.250s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9694 -- iter: 328/405\n",
            "Training Step: 50736  | time: 0.257s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9725 -- iter: 336/405\n",
            "Training Step: 50737  | time: 0.264s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9752 -- iter: 344/405\n",
            "Training Step: 50738  | time: 0.269s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9777 -- iter: 352/405\n",
            "Training Step: 50739  | time: 0.275s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9799 -- iter: 360/405\n",
            "Training Step: 50740  | time: 0.279s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9819 -- iter: 368/405\n",
            "Training Step: 50741  | time: 0.286s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9838 -- iter: 376/405\n",
            "Training Step: 50742  | time: 0.292s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9854 -- iter: 384/405\n",
            "Training Step: 50743  | time: 0.296s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9868 -- iter: 392/405\n",
            "Training Step: 50744  | time: 0.304s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9882 -- iter: 400/405\n",
            "Training Step: 50745  | time: 0.309s\n",
            "| Adam | epoch: 995 | loss: 0.00000 - acc: 0.9893 -- iter: 405/405\n",
            "--\n",
            "Training Step: 50746  | time: 0.005s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9904 -- iter: 008/405\n",
            "Training Step: 50747  | time: 0.010s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9914 -- iter: 016/405\n",
            "Training Step: 50748  | time: 0.016s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9922 -- iter: 024/405\n",
            "Training Step: 50749  | time: 0.021s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9930 -- iter: 032/405\n",
            "Training Step: 50750  | time: 0.025s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9937 -- iter: 040/405\n",
            "Training Step: 50751  | time: 0.031s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9943 -- iter: 048/405\n",
            "Training Step: 50752  | time: 0.037s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9949 -- iter: 056/405\n",
            "Training Step: 50753  | time: 0.045s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9954 -- iter: 064/405\n",
            "Training Step: 50754  | time: 0.050s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9959 -- iter: 072/405\n",
            "Training Step: 50755  | time: 0.054s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9963 -- iter: 080/405\n",
            "Training Step: 50756  | time: 0.064s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9967 -- iter: 088/405\n",
            "Training Step: 50757  | time: 0.071s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9970 -- iter: 096/405\n",
            "Training Step: 50758  | time: 0.078s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9973 -- iter: 104/405\n",
            "Training Step: 50759  | time: 0.083s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9976 -- iter: 112/405\n",
            "Training Step: 50760  | time: 0.088s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9978 -- iter: 120/405\n",
            "Training Step: 50761  | time: 0.092s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9980 -- iter: 128/405\n",
            "Training Step: 50762  | time: 0.096s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9982 -- iter: 136/405\n",
            "Training Step: 50763  | time: 0.101s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9984 -- iter: 144/405\n",
            "Training Step: 50764  | time: 0.105s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9986 -- iter: 152/405\n",
            "Training Step: 50765  | time: 0.110s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9987 -- iter: 160/405\n",
            "Training Step: 50766  | time: 0.115s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9988 -- iter: 168/405\n",
            "Training Step: 50767  | time: 0.119s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9990 -- iter: 176/405\n",
            "Training Step: 50768  | time: 0.124s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9991 -- iter: 184/405\n",
            "Training Step: 50769  | time: 0.128s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9991 -- iter: 192/405\n",
            "Training Step: 50770  | time: 0.133s\n",
            "| Adam | epoch: 996 | loss: 0.00000 - acc: 0.9992 -- iter: 200/405\n"
          ]
        }
      ],
      "source": [
        "# Try to load the saved model from \"model.tflearn\" file. If the file doesn't exist, train a new model and save it.\n",
        "\n",
        "try:\n",
        "  model.load(\"model.tflearn\")\n",
        "except:\n",
        "  model = tflearn.DNN(net)\n",
        "  model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "  model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me-f0tYBR9oG"
      },
      "outputs": [],
      "source": [
        "# This function takes a sentence and a list of words and returns a bag of words representation of the sentence.\n",
        "\n",
        "def bag_of_words(s, words):\n",
        "  bag = [0 for _ in range(len(words))]\n",
        "  s_words = nltk.word_tokenize(s)\n",
        "  s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "  for se in s_words:\n",
        "    for i, w in enumerate(words):\n",
        "      if w == se:\n",
        "        bag[i] = 1\n",
        "\n",
        "  return numpy.array(bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv8vlqaXkcOP",
        "outputId": "2079c536-7e5c-4357-b07c-652a0d1e5001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "     admission       1.00      1.00      1.00         6\n",
            "       canteen       1.00      1.00      1.00        11\n",
            "college intake       1.00      1.00      1.00         9\n",
            "     committee       1.00      1.00      1.00         6\n",
            "   computerhod       1.00      1.00      1.00         4\n",
            "        course       1.00      1.00      1.00        27\n",
            "       creator       1.00      1.00      1.00        16\n",
            "      director       1.00      1.00      1.00         7\n",
            "      document       1.00      1.00      1.00        13\n",
            "        ecehod       1.00      1.00      1.00         4\n",
            "         event       1.00      1.00      1.00        11\n",
            "    facilities       1.00      1.00      1.00         5\n",
            "          fees       1.00      0.96      0.98        23\n",
            "        floors       1.00      1.00      1.00         7\n",
            "       goodbye       1.00      1.00      1.00        12\n",
            "      greeting       1.00      1.00      1.00        10\n",
            "           hod       1.00      1.00      1.00         3\n",
            "        hostel       0.96      1.00      0.98        22\n",
            "         hours       1.00      1.00      1.00        17\n",
            "infrastructure       1.00      1.00      1.00         3\n",
            "         ithod       1.00      1.00      1.00         4\n",
            "       library       1.00      1.00      1.00        14\n",
            "      location       1.00      1.00      1.00        14\n",
            "          menu       1.00      1.00      1.00         7\n",
            "          name       1.00      1.00      1.00        13\n",
            "        number       1.00      1.00      1.00        15\n",
            "     placement       1.00      1.00      1.00         9\n",
            "       ragging       1.00      1.00      1.00        10\n",
            "        random       1.00      1.00      1.00         3\n",
            "     salutaion       1.00      1.00      1.00        13\n",
            "   scholarship       1.00      1.00      1.00        26\n",
            "           sem       1.00      1.00      1.00        11\n",
            "        sports       1.00      1.00      1.00         7\n",
            "         swear       1.00      1.00      1.00         9\n",
            "      syllabus       1.00      1.00      1.00         7\n",
            "          task       1.00      1.00      1.00         6\n",
            "       uniform       1.00      1.00      1.00         9\n",
            "      vacation       1.00      1.00      1.00        12\n",
            "\n",
            "      accuracy                           1.00       405\n",
            "     macro avg       1.00      1.00      1.00       405\n",
            "  weighted avg       1.00      1.00      1.00       405\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the trained model\n",
        "model.load(\"model.tflearn\")\n",
        "\n",
        "# Create empty lists to store true and predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Loop over all intents in the data file\n",
        "for intent in data[\"intents\"]:\n",
        "    # Loop over all patterns in the intent\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        # Tokenize and stem the pattern\n",
        "        tokens = nltk.word_tokenize(pattern)\n",
        "        stemmed = [stemmer.stem(token.lower()) for token in tokens]\n",
        "        # Convert the pattern to a bag of words\n",
        "        bow = bag_of_words(pattern, words)\n",
        "        # Predict the intent for the pattern\n",
        "        results = model.predict([bow])[0]\n",
        "        predicted_intent = labels[numpy.argmax(results)]\n",
        "        # Append the true and predicted labels to the lists\n",
        "        y_true.append(intent[\"tag\"])\n",
        "        y_pred.append(predicted_intent)\n",
        "\n",
        "# Print the classification report, including precision and recall\n",
        "print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9L0lVD-k5pW",
        "outputId": "55b3e4bb-448e-4ef3-fa20-c397d8dc3aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0 12]]\n",
            "\n",
            "Precision:  1.0\n",
            "Recall:  1.0\n",
            "F1 score:  1.0\n"
          ]
        }
      ],
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_data = data[\"intents\"][:int(len(data[\"intents\"]) * 0.8)]\n",
        "test_data = data[\"intents\"][int(len(data[\"intents\"]) * 0.8):]\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for intent in test_data:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    x = bag_of_words(pattern, words)\n",
        "    y_true.append(labels.index(intent[\"tag\"]))\n",
        "    y_pred.append(numpy.argmax(model.predict([x])))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = sklearn.metrics.precision_score(y_true, y_pred, average='weighted')\n",
        "recall = sklearn.metrics.recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = sklearn.metrics.f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nPrecision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 score: \", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byfQYQAVkZ-w",
        "outputId": "52c8a10b-64bd-4740-bd49-6d9695be5a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['What', \"'s\", 'up']\n",
            "greeting\n"
          ]
        }
      ],
      "source": [
        "print(docs_x[5])\n",
        "print(docs_y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpzDQUVpZS7C",
        "outputId": "38d5801b-6eaf-49ba-c7da-f38dea781628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start talking with the bot (type quit to stop)!\n",
            "You: hi\n",
            "Good to see you again!\n",
            "You: What is the college fees\n",
            "For Fee details visit https://iiitranchi.ac.in/fees.aspx\n",
            "You: What is the syllabus\n",
            "Timetable is provided directly to the students through their respective Class Representatives.\n",
            "You: is there any canteen\n",
            "Our university has canteen with variety of food available\n",
            "You: and library\n",
            "There is one huge and spacious library. Timings are 8am to 6pm.\n",
            "You: What is college timings\n",
            "College is open 8am-5pm Monday-Friday!\n",
            "You: what about hostel facilities\n",
            "For hostel details visit https://iiitranchi.ac.in/facilities.aspx\n",
            "You: your name\n",
            "I'm Luna\n",
            "You: Thank you luna\n",
            "welcome, anything else i can assist you with?\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "# This chat function takes user input and returns a response from the chatbot.\n",
        "#For each input the function creates a bag of words representation, feeds it to the trained model, and returns\n",
        "# a response based on the predicted intent.\n",
        "\n",
        "def chat():\n",
        "  print(\"Start talking with the bot (type quit to stop)!\")\n",
        "  while True:\n",
        "    inp = input(\"You: \")\n",
        "    if inp.lower() == \"quit\":\n",
        "      break\n",
        "\n",
        "    results = model.predict([bag_of_words(inp, words)])\n",
        "    results_index = numpy.argmax(results)\n",
        "    tag = labels[results_index]\n",
        "\n",
        "    for tg in data[\"intents\"]:\n",
        "      if tg['tag'] == tag:\n",
        "        responses = tg['responses']\n",
        "\n",
        "    print(random.choice(responses))\n",
        "\n",
        "chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFsJpNvuEAAN3t6R3j63SA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}